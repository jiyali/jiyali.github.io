<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="/2022/08/05/mian-shi-bei-zhan-zhuan-ti-jian-zhi-offer-shua-ti-pian-chi-xu-geng-xin-zhong-1-wa-1-no-studying-makes-me-happy/"/>
      <url>/2022/08/05/mian-shi-bei-zhan-zhuan-ti-jian-zhi-offer-shua-ti-pian-chi-xu-geng-xin-zhong-1-wa-1-no-studying-makes-me-happy/</url>
      
        <content type="html"><![CDATA[<p>�=j�(“\��<em>�1��̻�i�b�^06!�2�^�&amp;��Yf���Ҁ��Q���:�U��J��h��]���ţ��@i�<del>8t����ѝO����3��˫�_������&gt;�J5�ڤ�0</del>F��x.sƀ���Ԡ�z����n��w�]�g���O򐸲J�[X��    ��ʸY�t��GÀ�V���ֆI���-��o �q^8s~�<code>����\a���U�8v\������ub��ܼޥd����1y��RέR|�9�b�J9-��OЦ�r�y���+fOsGŪ|�WT��2B��!��a�����hEb�����B�#;��%4\����+z�}/G�c����l�R\L���D�����O��}�&lt;b)�o��-I��S�6���v</code>�bz�Vh�6���</em>�Q��b������I3ӹ�B�T�LA�{���|�<br>g�l�жi���g�YY���}�<del>h.^���f61�%���?��o��#6���’�1�2��p�&lt;�Uy��U�7v����B<code>Z�&gt;fr�����7���#��.g/� ��.�����#�JPzX3#�    &amp;IO��&lt;��j��F:g��[6U�\o���#��j&gt;?</code>sB篊!B�<br>“-��6��n)�^!:]_�Vt�m<br>?</del>���Y��QQCF_��1�Z<code>���̒�*���x��R�~�yg!t�-�� �~e��x�6y��Zl=���s� ��-�ߔ�G�}</code>Nq|:�h#���㮍�!����Ȕ��g$����������r=y��}�O4����H�W�z41�P#���Zl�<del>vIT�˶�4���い��E���4�{�۠hK��_縛�3�����p9�U�V��X�y��ڇ��A��楰�p/T4�u��B��YY�k-:AP_���X�RO�@�I!]�M</del>M����Է?6�=�S��b�����ͷ��Ct��m;����<br>�l�4�����;@2�W��R볟���An��B�&lt;�U��k���NP���C����)�I��X�<em>��7�n�”��Eu9��<em>�G�A��WS�R�lt�I    ��hM9���J���-˓&amp;�H|��j��”7~�p�Z�r��w���5僎�Bbٍ�}N&gt;Q�T���+��߯���[���?��+�<br>�/J���ԟ/0��p��]�_L�k��ol�)�P��Jz.ceG&amp;xZ4�S’8��GΒ��<br>Άk[�r��@/�)�?F�m��z6�k��hq�{�+��m�B</em>������U���+� ��ͨt�Do��”�M��”l��z)� R���U�2�b��e�hӇ�Z�R���lK�R�)���E(�;�^,�|�.��s�!<br>�4ʾ�9�kyP3�!|<del>��P�i���⒝z”��’)|E5qRl�ڮ���<br>�mO�|E򔼖����$�����b��S�7KkO=N��G�z;T�/i8_p�V�1�m�@��+�<br>�E�/�\�l�k#�Xl�.o#S��U”Yu���وS���h&lt;Ѹ�<code>��kϞ1�p�f.�</code>nٍ��Uq�h���P��&amp;�h���x=���A��_���Yc��ı�f5�X�G<em>��[�z��ћ������</em>v�t�&lt;I?q���i��z��”�</del>�/ȅl�U��Lq���k�;�%S�4<code>��S���vkr��G�aɧ�&lt;���혉q�8$��R��P�k��U�� K��W��"�R�A��4��z;�g��_�7a��X(�7�mt-X��s�q���W��p�F'�'��$��f:ط���n�R$n��&amp;�!����fZRC:Ԗ���ɣ8f�x'�k1�?��'��\�RL&amp;=\ˠ��e~��ޑ�q����R�C�j,�+W��2�{�jҮT��z��������/�g:O7�'_&lt;� �����uH��{k�:��T܏��}�7~��ҝ������a��/{�h0�V hh[�</code>E��ӳ�”�^�’�L��.h��ɋP˴�<br>��    \cH�0@cN�&gt;!3J�u��6R�{ʼ����熠����‘J@��’xw�����=}\��O����I��/��!2��n�׈t��)�������T�U��$���k~u�</em>���M�s�Ń��t��a’�P�Ix<del>��{�<em>BM0�&amp;�\��������T�NĲ}M�    ���q���y{S�Г�־Vu&lt;��8���,����}#�%���G|8a</em>��?�_�’J�.[}�q��j</del>=��cC<br>v5��dw�s��{������&gt;H�1�n�M*<em>��&amp;G�{(�{:�&lt;+�b��8n3<br>����*�Ei��-&amp;�”gإ��<br>��B�    @�”�)Y�x<code>W� ���r�f��r    W�m����L��?�����c&gt;��/�zYS�����aEN�e��~�έGs9z�V�m�x���(Vԓ��moV��t��%:��H�ə:���(��Lx��2g�J?���NL�D��{)j�(    =�q�I_�X�:FE�o�����l�|��Z�i����^��Hl���PH1g�~o�,4�J��{���:e���������ݯ�|p�V������f��8=B�S�1�i��59N��h#{q�Q    GW��r��Ò3��I��^�PC,�)Ŕ#�f��W�    �V����H�ַi�p�o��_�F�J�zy,m�j�������l\�C���k)��e��� �k����Xm����$&gt;�B��0t5��֐F�#}3������IZ�f�\&gt;kA�D~K�hED���ѩ�&gt;M�L�k��B&lt;7���)&lt;n��4���յ^k��Ha�</code>Q�e����1P�&gt;^</em>,�h�n”�P0Oׂ}    ь���<del>��y�E�z��AH8m��Yj���o�l�{X0��4�B��h�8�O1����wҧ��w’��U�b�@�&lt;ކ?�:�<br>*�T�w�������5�m��4Iw{��hR��j��x6�ۍmǫzϢ�}Dt��p��.���B��A�{<br>��0�l!<br>p0V&gt;��:Ah��F�4���ߥRv�<br>�������a(k+�9˭�h�g��f��`�/���</del>�Z�䰈f��������(1ܤ7��i���n�xE��M�e�L�    U����o|���”�o�2Ց7�Ҡ��ί&gt;��|8D����a �����F\�9�h��; Q��Y�5    �    s����$1���2�=k�����6�i��Q_��j� t�;Y�����z�!�ˁ&lt;ɥy���p��y�hDH�wJd&lt;oScmi�ȕ�~8L�W���V�L�)7O\O�<em>�9�R<code>qGO�aO鵍Q&amp;�{�da7ԫ���U�9���v�8h�a4�}�|�Q�ZM��g䟓"���</code>s0rA1�Sx���R:��cu��p�λ����(nY<em>N�k��t�PI[��&gt;�:�a</em>�B�p��=*���F�� מ;g=SgR�Zr�:����ܵ.���X2%[C��oWsg��bfN.K{J#Eo��}����ܯդ˓w<br>�e��EB%?���e:C�lLZ��Sx&amp;k&amp;EW���&amp;F#c`��\E��c�r��n%Y�іE�m3</em>Λ�b�2сO����YX�kv��2��|���0F�̦����&amp;���-ܗ{ L���H�h��t����!�<br>�7)����/�����Y�5֍�{��    ����Cہ`)�G��fr����8��j��m�״�d�V�b.B�o�/�G��B��Z7�h|�̲��E}uy5hamC�ޯx(|�^”4�C;[<br>�&amp;��”%��5<br>��I<br>w�Ɋ�s�n�fL�F�I�焇I�����.���B�y�<br>-<br>Z5&lt;�/|W9���i�f�”�tK��&gt;Lԉ=�O+f�9</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>post天池Leetcode基础训练</title>
      <link href="/2022/02/15/post-tian-chi-leetcode-ji-chu-xun-lian/"/>
      <url>/2022/02/15/post-tian-chi-leetcode-ji-chu-xun-lian/</url>
      
        <content type="html"><![CDATA[<h2 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h2><p>数组</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>树模型与集成学习の学习笔记</title>
      <link href="/2021/10/10/shu-mo-xing-yu-ji-cheng-xue-xi-noxue-xi-bi-ji/"/>
      <url>/2021/10/10/shu-mo-xing-yu-ji-cheng-xue-xi-noxue-xi-bi-ji/</url>
      
        <content type="html"><![CDATA[<h2 id="Task01决策树"><a href="#Task01决策树" class="headerlink" title="Task01决策树"></a>Task01决策树</h2><p>先解决几个概念问题，比如信息量，信息熵等，这里借鉴【忆臻】大佬在知乎上的解释。</p><h3 id="信息量"><a href="#信息量" class="headerlink" title="信息量"></a>信息量</h3><p>信息的大小跟随机事件的概率有关。越小概率的事情发生了产生的信息量越大，如湖南产生的地震了；越大概率的事情发生了产生的信息量越小，如太阳从东边升起来了（肯定发生嘛，没什么信息量）。这很好理解！</p><ol><li>一个具体事件的信息量应该是随着其发生概率而递减的，且不能为负。</li><li>如果我们有俩个不相关的事件x和y，那么我们观察到的俩个事件同时发生时获得的信息应该等于观察到的事件各自发生时获得的信息之和，即： $h(x,y) = h(x)+h(y)$ ,由于x,y的是两个不相关的事件，那么也满足 $p(x,y)=p(x)*p(y)$</li></ol><p>  根据以上两个性质，我们容易看出h(x)一定与p(x)的对数有关（因为只有对数形式的真数相乘之后，能够对应对数的相加形式，可以试试）。因此我们有信息量公式如下：<br>\[h(x) = -{log_{ {2}}}p(x)\]<br>注；①其中，负号是为了确保信息一定是正数或者是0，总不能为负数吧！信息量取概率的负对数，其实是因为信息量的定义是概率的倒数的对数。而用概率的倒数，是为了使概率越大，信息量越小，同时因为概率的倒数大于1，其对数自然大于0了。<br>   ②为什么底数为2：这是因为，我们只需要信息量满足低概率事件x对应于高的信息量。那么对数的选择是任意的。我们只是遵循信息论的普遍传统，使用2作为对数的底！</p><h3 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h3><p>信息量度量的是一个具体事件发生了所带来的信息，而熵则是在结果出来之前对可能产生的信息量的期望——考虑该随机变量的所有可能取值，即所有可能发生事件所带来的信息量的期望。简单的讲，<strong>信息熵就是平均而言发生一个事件我们所得到的信息量的大小</strong><br>这里信息熵就表示为$H(x)=-\sum_{i=1}^{n} p(x_i)log_{ {2}}p(x_i)$，且当i=0时，定义$0log0=0$</p><h3 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h3><p>在决策树的分裂过程中，我们不但需要考察本节点的不确定性或纯度，而且还要考察子节点的平均不确定性或平均纯度来决定是否进行分裂。子节点的产生来源于决策树分支的条件，因此我们不但要研究随机变量的信息熵，还要研究在给定条件下随机变量的平均信息熵或条件熵<br>官方定义的条件熵如下：</p><blockquote><p>X给定条件下，Y的条件概率分布的熵对X的数学期望（已知随机变量X条件下随机变量Y的不确定性）<br>\[H(Y|X)=\sum_{i=1}^{n} H(Y|X=x_i)\]</p></blockquote><ul><li>那么条件熵到底是个啥嘞….<br>这一块比较复杂。。让我想想怎么填这个坑~（再补）<br>暂时来讲，我的理解是按一个新的变量的每个值对原变量进行分类，比如天气冷暖和我穿衣服多少是有联系的。假设X表示我穿衣服多少（假设只有2种情况，穿多或者穿少），Y表示天气冷暖，那么H（Y|X）表示的是在确定路人穿衣情况下天气冷暖的情况的不确定性的数学期望。这个例子把天气冷暖按照穿的多少分为两类。在每一个小类里面，都计算一个小熵，然后每一个小熵乘以各个类别的概率，最后求和，就是条件熵。</li></ul><h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><p>（特征选择的一个重要指标，他定义为一个特征能够为分类系统带来多少信息，带来的信息越多，说明该特征越重要，相应的信息增益也就越大）</p><h2 id="task01练习"><a href="#task01练习" class="headerlink" title="task01练习"></a>task01练习</h2><p>1.1定义X,Y的联合熵为 $H(Y,X)$ 为 ${E_{ {(Y,X)~p(x,y)}}}[−log_{ {2}}p(Y,X)]$<br>请证明如下关系： $G(Y,X)=H(X)−H(X|Y),G(Y,X)=H(X)+H(Y)−H(Y,X),G(Y,X)=H(Y,X)−H(X|Y)−H(Y|X)$<br>下图被分为了A、B和C三个区域。若AB区域代表X的不确定性，BC区域代表Y的不确定性，那么$H(X)、H(Y)、H(X|Y)、H(Y|X)、H(Y,X)和G(Y,X)$分别指代的是哪片区域？<br><img src="1.png"></p><p>2.假设当前我们需要处理一个分类问题，请问对输入特征进行归一化会对树模型的类别输出产生影响吗？请解释原因。<br>不会，树模型主要依赖的是数据的信息增益基尼系数等，算是一种特征分布的顺序，归一化只改变了数值，并不改变数据的排列</p><p>3.如果将系数替换为$1−\gamma^2$ ，请问对缺失值是加强了还是削弱了惩罚？<br>削弱，</p><p>4.如果将树的生长策略从深度优先生长改为广度优先生长，假设其他参数保持不变的情况下，两个模型对应的结果输出可能不同吗？<br>不可能，深度优先和广度优先只是搜索时的顺序不同，结果相同</p><p>5.在一般的机器学习问题中，我们总是通过一组参数来定义模型的损失函数，并且在训练集上以最小化该损失函数为目标进行优化。请问对于决策树而言，模型优化的目标是什么？<br>信息增益最大化吧</p><p>6.对信息熵中的log函数在p=1处进行一阶泰勒展开可以近似为基尼系数，那么如果在p=1处进行二阶泰勒展开我们可以获得什么近似指标？请写出对应指标的信息增益公式。</p><p>7.除了信息熵和基尼系数之外，我们还可以使用节点的$1−max_kp(Y=y_k)$和第m个子节点的$1−max_kp(Y=y_k|X=x_m)$来作为衡量纯度的指标。请解释其合理性并给出相应的信息增益公式。</p><p>8.为什么对没有重复特征值的数据，决策树能够做到损失为0？<br>按道理来讲，一棵决策树应该会拟合全部的数据，所以就不存在损失</p><p>9.如何理解min_samples_leaf参数能够控制回归树输出值的平滑程度？</p><h2 id="Task03-集成模式"><a href="#Task03-集成模式" class="headerlink" title="Task03 集成模式"></a>Task03 集成模式</h2><p>$y = f(x)$<br>\[y = {f_{ {g_1}}}(x)\]</p><p><a href="https://zhuanlan.zhihu.com/p/26486223">通俗理解信息熵</a><br><a href="https://zhuanlan.zhihu.com/p/26551798?tdsourcetag=s_pctim_aiomsg">通俗理解条件熵</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>情感分析学习笔记</title>
      <link href="/2021/09/12/qing-gan-fen-xi-xue-xi-bi-ji/"/>
      <url>/2021/09/12/qing-gan-fen-xi-xue-xi-bi-ji/</url>
      
        <content type="html"><![CDATA[<p>记录datawhale九月份的学习，地址：<a href="https://github.com/datawhalechina/team-learning-nlp/tree/master/Emotional_Analysis">情感分析</a></p><h2 id="Task0：自然语言处理之PyTorch情感分析简介"><a href="#Task0：自然语言处理之PyTorch情感分析简介" class="headerlink" title="Task0：自然语言处理之PyTorch情感分析简介"></a>Task0：自然语言处理之PyTorch情感分析简介</h2><p>   从来没有接触过情感分析，一切都要从零开始，这次大航海的教程好像都是实战为主的，有点怀疑自己能不能跟得上。先记录一下task0遇到的问题。<br><strong>问题</strong><br>环境配置按照教程的问题不大，进行到以下语句的时候出现了 <em>‘[WinError 10061] 由于目标计算机积极拒绝，无法连接’</em> 的问题</p><pre class="line-numbers language-none"><code class="language-none">python -m spacy download zh_core_web_smpython -m spacy download en_core_web_sm<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><strong>解决办法</strong><br>搜解决办法好多需要改防火墙设置什么的，而且不怎么奏效，索性直接选择了离线安装的办法。</p><ul><li>zh_core_web_sm下载地址：<br><a href="https://github.com/explosion/spacy-models/releases/tag/zh_core_web_sm-3.1.0">https://github.com/explosion/spacy-models/releases/tag/zh_core_web_sm-3.1.0</a></li><li>en_core_web_sm下载地址：<br><a href="https://github.com/explosion/spacy-models/releases/tag/en_core_web_sm-3.1.0">https://github.com/explosion/spacy-models/releases/tag/en_core_web_sm-3.1.0</a></li></ul><p>下载完成后运行即可</p><pre class="line-numbers language-none"><code class="language-none">pip install zh_core_web_sm-3.1.0.tar.gzpip install en_core_web_sm-3.1.0.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><strong>插曲</strong><br>记录一下不应该成为问题的问题：中间安装环境的时候，出现了一大片红色（真的好久没有见过这么大片的红），并提示<code>ValueError: check_hostname  requires server_hostname</code> 的错误，关闭科学上网工具后解决。fine~</p><h2 id="Task1-情感分析baseline"><a href="#Task1-情感分析baseline" class="headerlink" title="Task1:情感分析baseline"></a>Task1:情感分析baseline</h2><p>任务简介：使用pytorch和torchtext构造一个简单的模型预测情绪（正面或负面）<br>数据集：IMDb数据集</p><h3 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h3><h4 id="1-torchtext"><a href="#1-torchtext" class="headerlink" title="1.torchtext"></a>1.torchtext</h4><p>torchtext包含以下组件：</p><ul><li>Field :主要包含以下数据预处理的配置信息，比如指定分词方法，是否转成小写，起始字符，结束字符，补全字符以及词典等等。此次教程中需要指定一个 tokenizer_language 来告诉 torchtext 使用哪个 spaCy 模型。我们使用 en_core_web_sm 模型。<pre class="line-numbers language-none"><code class="language-none">en_core_web_sm模型的下载地址：https://github.com/explosion/spacy-models/releases/tag/en_core_web_sm-3.1.0安装时运行 pip install en_core_web_sm-3.1.0.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li>Dataset :继承自pytorch的Dataset，用于加载数据，提供了TabularDataset可以指点路径，格式，Field信息就可以方便的完成数据加载。同时torchtext还提供预先构建的常用数据集的Dataset对象，可以直接加载使用，splits方法可以同时加载训练集，验证集和测试集。</li><li>Iterator : 主要是数据输出的模型的迭代器，可以支持batch定制</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torchtext<span class="token punctuation">.</span>legacy <span class="token keyword">import</span> data<span class="token comment"># 设置随机种子数，该数可以保证随机数是可重复的</span>SEED <span class="token operator">=</span> <span class="token number">1234</span><span class="token comment"># 设置种子</span>torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span>torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>deterministic <span class="token operator">=</span> <span class="token boolean">True</span><span class="token comment"># 读取数据和标签</span>TEXT <span class="token operator">=</span> data<span class="token punctuation">.</span>Field<span class="token punctuation">(</span>tokenize <span class="token operator">=</span> <span class="token string">'spacy'</span><span class="token punctuation">,</span> tokenizer_language <span class="token operator">=</span> <span class="token string">'en_core_web_sm'</span><span class="token punctuation">)</span>LABEL <span class="token operator">=</span> data<span class="token punctuation">.</span>LabelField<span class="token punctuation">(</span>dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token comment"># 下载 IMDb 数据集</span><span class="token keyword">from</span> torchtext<span class="token punctuation">.</span>legacy <span class="token keyword">import</span> datasetstrain_data<span class="token punctuation">,</span> test_data <span class="token operator">=</span> datasets<span class="token punctuation">.</span>IMDB<span class="token punctuation">.</span>splits<span class="token punctuation">(</span>TEXT<span class="token punctuation">,</span> LABEL<span class="token punctuation">)</span><span class="token comment"># 查看训练集和测试集的大小</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Number of training examples: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>train_data<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Number of testing examples: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>test_data<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span><span class="token comment"># 查看其中一个示例数据</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">vars</span><span class="token punctuation">(</span>train_data<span class="token punctuation">.</span>examples<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-划分数据集"><a href="#2-划分数据集" class="headerlink" title="2.划分数据集"></a>2.划分数据集</h4><p>IMDb 数据集划分了训练集和测试集，这里我们还需要创建一个验证集。 可以使用 .split() 方法来做。（ <strong>注：将之前设置的随机种子SEED传递给 random_state 参数，确保我们每次获得相同的训练集和验证集。</strong> ）</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> randomtrain_data<span class="token punctuation">,</span> valid_data <span class="token operator">=</span> train_data<span class="token punctuation">.</span>split<span class="token punctuation">(</span>split_ratio<span class="token operator">=</span><span class="token number">0.8</span> <span class="token punctuation">,</span> random_state <span class="token operator">=</span> random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 查看训练集，验证集和测试集分别有多少数据</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Number of training examples: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>train_data<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Number of validation examples: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>valid_data<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Number of testing examples: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>test_data<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="3-构建词汇表"><a href="#3-构建词汇表" class="headerlink" title="3.构建词汇表"></a>3.构建词汇表</h4><ul><li>由于模型不能直接对字符串进行操作，只能对数字进行操作，所以需要建立一下查找表，每个单词都有唯一对应的index。</li><li>onehot编码在此不适用（训练集中不同的单词数量巨大，训练的代价大），需要优化，优化方案有：只取前n个出现次数最多的单词作为one-hot的基，另一种是忽略出现次数小于m个的单词。（这里使用前者）</li><li>优化后的onehot会出现很多单词虽然在数据集中，但是无法编码的情况，这里还要引进两个特殊的符号来编码，一个是<unk>用来处理前面所说的问题，另一个是<pad>，用于句子的填充。<pre class="line-numbers language-python" data-language="python"><code class="language-python">MAX_VOCAB_SIZE <span class="token operator">=</span> <span class="token number">25000</span> <span class="token comment"># 词汇表的最大长度</span>TEXT<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> max_size <span class="token operator">=</span> MAX_VOCAB_SIZE<span class="token punctuation">)</span>LABEL<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Unique tokens in TEXT vocabulary: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Unique tokens in LABEL vocabulary: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>LABEL<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></pad></unk></li></ul><h4 id="4-创建迭代器"><a href="#4-创建迭代器" class="headerlink" title="4.创建迭代器"></a>4.创建迭代器</h4><ul><li>准备数据的最后一步是创建迭代器. 需要创建验证集，测试集，以及训练集的迭代器, 每一次的迭代都会返回一个batch的数据。</li><li>本例中使用“BucketIterator”，它将返回一批示例，其中每个样本的长度差不多，从而最小化每个样本的padding数。</li><li>用torch.device，可以将张量放到gpu或者cpu上。<pre class="line-numbers language-python" data-language="python"><code class="language-python">BATCH_SIZE <span class="token operator">=</span> <span class="token number">64</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">'cpu'</span><span class="token punctuation">)</span>train_iterator<span class="token punctuation">,</span> valid_iterator<span class="token punctuation">,</span> test_iterator <span class="token operator">=</span> data<span class="token punctuation">.</span>BucketIterator<span class="token punctuation">.</span>splits<span class="token punctuation">(</span>    <span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> valid_data<span class="token punctuation">,</span> test_data<span class="token punctuation">)</span><span class="token punctuation">,</span>     batch_size <span class="token operator">=</span> BATCH_SIZE<span class="token punctuation">,</span>    device <span class="token operator">=</span> device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h3 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h3><ul><li>首先关于初始化的问题，在某些框架中，使用RNN需要初始化 ℎ0 ，但在pytorch中不用，默认为全0。</li><li>构建一个RNN模型(nn.module子类)<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">class</span> <span class="token class-name">RNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_dim<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span>                self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>RNN<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span>                self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span>            <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment">#text = [sent len, batch size]</span>                embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>text<span class="token punctuation">)</span>                <span class="token comment">#embedded = [sent len, batch size, emb dim]</span>                output<span class="token punctuation">,</span> hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>embedded<span class="token punctuation">)</span>                <span class="token comment">#output = [sent len, batch size, hid dim]</span>        <span class="token comment">#hidden = [1, batch size, hid dim]</span>                <span class="token keyword">assert</span> torch<span class="token punctuation">.</span>equal<span class="token punctuation">(</span>output<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> hidden<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                <span class="token keyword">return</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>hidden<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li>设置超参数<pre class="line-numbers language-python" data-language="python"><code class="language-python">INPUT_DIM <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span>EMBEDDING_DIM <span class="token operator">=</span> <span class="token number">100</span>HIDDEN_DIM <span class="token operator">=</span> <span class="token number">256</span>OUTPUT_DIM <span class="token operator">=</span> <span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><ul><li>设置优化器：SGD</li><li>定义损失函数：BCEWithLogitsLoss一般用来做二分类<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optimoptimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCEWithLogitsLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>model <span class="token operator">=</span> model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>criterion <span class="token operator">=</span> criterion<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li>设置训练函数和评估函数<br>训练函数和评估函数是相似的，评估时不需要再进行梯度计算，所以使用<code>with no_grad()</code>，去掉了<code>optimizer.zero_grad(), loss.backward() , optimizer.step()</code><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> iterator<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span><span class="token punctuation">:</span>        epoch_loss <span class="token operator">=</span> <span class="token number">0</span>    epoch_acc <span class="token operator">=</span> <span class="token number">0</span>        model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> batch <span class="token keyword">in</span> iterator<span class="token punctuation">:</span>                optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>                        predictions <span class="token operator">=</span> model<span class="token punctuation">(</span>batch<span class="token punctuation">.</span>text<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>                loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>                acc <span class="token operator">=</span> binary_accuracy<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>                loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>                optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>                epoch_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        epoch_acc <span class="token operator">+=</span> acc<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">return</span> epoch_loss <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span><span class="token punctuation">,</span> epoch_acc <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">evaluate</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> iterator<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span><span class="token punctuation">:</span>        epoch_loss <span class="token operator">=</span> <span class="token number">0</span>    epoch_acc <span class="token operator">=</span> <span class="token number">0</span>        model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">for</span> batch <span class="token keyword">in</span> iterator<span class="token punctuation">:</span>            predictions <span class="token operator">=</span> model<span class="token punctuation">(</span>batch<span class="token punctuation">.</span>text<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>                        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>                        acc <span class="token operator">=</span> binary_accuracy<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>            epoch_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>            epoch_acc <span class="token operator">+=</span> acc<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">return</span> epoch_loss <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span><span class="token punctuation">,</span> epoch_acc <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li>最后保留在验证集上的损失值最小的模型<pre class="line-numbers language-python" data-language="python"><code class="language-python">N_EPOCHS <span class="token operator">=</span> <span class="token number">5</span>best_valid_loss <span class="token operator">=</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token string">'inf'</span><span class="token punctuation">)</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>N_EPOCHS<span class="token punctuation">)</span><span class="token punctuation">:</span>    start_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>        train_loss<span class="token punctuation">,</span> train_acc <span class="token operator">=</span> train<span class="token punctuation">(</span>model<span class="token punctuation">,</span> train_iterator<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span>    valid_loss<span class="token punctuation">,</span> valid_acc <span class="token operator">=</span> evaluate<span class="token punctuation">(</span>model<span class="token punctuation">,</span> valid_iterator<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span>        end_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>    epoch_mins<span class="token punctuation">,</span> epoch_secs <span class="token operator">=</span> epoch_time<span class="token punctuation">(</span>start_time<span class="token punctuation">,</span> end_time<span class="token punctuation">)</span>        <span class="token keyword">if</span> valid_loss <span class="token operator">&lt;</span> best_valid_loss<span class="token punctuation">:</span>        best_valid_loss <span class="token operator">=</span> valid_loss        torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'tut1-model.pt'</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Epoch: </span><span class="token interpolation"><span class="token punctuation">{</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token format-spec">02</span><span class="token punctuation">}</span></span><span class="token string"> | Epoch Time: </span><span class="token interpolation"><span class="token punctuation">{</span>epoch_mins<span class="token punctuation">}</span></span><span class="token string">m </span><span class="token interpolation"><span class="token punctuation">{</span>epoch_secs<span class="token punctuation">}</span></span><span class="token string">s'</span></span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'\tTrain Loss: </span><span class="token interpolation"><span class="token punctuation">{</span>train_loss<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string"> | Train Acc: </span><span class="token interpolation"><span class="token punctuation">{</span>train_acc<span class="token operator">*</span><span class="token number">100</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string">%'</span></span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'\t Val. Loss: </span><span class="token interpolation"><span class="token punctuation">{</span>valid_loss<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string"> |  Val. Acc: </span><span class="token interpolation"><span class="token punctuation">{</span>valid_acc<span class="token operator">*</span><span class="token number">100</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string">%'</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>贴一张训练结果图</p><p>实验结果的准确率很低，有可能是本身RNN是一个很简单的模型，做不了太复杂的任务，另外word_embedding这里使用的是onehot，也会影响模型准确率~</p><h2 id="Task2：Updated情感分析"><a href="#Task2：Updated情感分析" class="headerlink" title="Task2：Updated情感分析"></a>Task2：Updated情感分析</h2><p>此次任务使用的是双向LSTM网络，并修改了基础模型的词向量，改用Glove</p><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>准备数据阶段与上次任务相同，这里不再赘述</p><ul><li>词向量（Glove）<br>这次任务使用的是 “glove.6B.100d” ，其中，6B表示词向量是在60亿规模的tokens上训练得到的，100d表示词向量是100维的(注意,这个词向量有800多兆)<pre class="line-numbers language-python" data-language="python"><code class="language-python">MAX_VOCAB_SIZE <span class="token operator">=</span> 25_000TEXT<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">,</span>                  max_size <span class="token operator">=</span> MAX_VOCAB_SIZE<span class="token punctuation">,</span>                  vectors <span class="token operator">=</span> <span class="token string">"glove.6B.100d"</span><span class="token punctuation">,</span>                  unk_init <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">.</span>normal_<span class="token punctuation">)</span>LABEL<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li>迭代器与GPU设置也不再赘述</li></ul><h3 id="构建模型-1"><a href="#构建模型-1" class="headerlink" title="构建模型"></a>构建模型</h3><h4 id="有关LSTM"><a href="#有关LSTM" class="headerlink" title="有关LSTM"></a>有关LSTM</h4><p>RNN作为一种处理序列数据的神经网络，序列数据通常会有上下文之间的影响，RNN通过让上一时刻的隐藏层影响当前时刻的隐藏层来解决序列间关系的问题。但是RNN还存在一些漏洞，比如在长序列数据处理中存在梯度消失的问题，长序列处理中，梯度被近距离梯度主导，导致模型难以学到远距离的依赖关系。而LSTM则解决了此问题<br>LSTM的关键是细胞状态（直译：cell state），表示为$C_t$ ，用来保存当前LSTM的状态信息并传递到下一时刻的LSTM中，也就是RNN中那根“自循环”的箭头。当前的LSTM接收来自上一个时刻的细胞状态 $C_{t-1}$ ，并与当前LSTM接收的信号输入 $x_t$ 共同作用产生当前LSTM的细胞状态 $C_t$，具体的作用方式下面将详细介绍。<br><img src="1.png"><br>LSTM主要包括三个不同的门结构：遗忘门、记忆门和输出门。这三个门用来控制LSTM的信息保留和传递，最终反映到细胞状态 $C_t$和输出信$h_t$ 。<br>*遗忘门由一个sigmod神经网络层和一个按位乘操作构成；<br>*记忆门由输入门（input gate）与tanh神经网络层和一个按位乘操作构成；<br>*输出门（output gate）与 tanh函数（注意：这里不是tanh神经网络层）以及按位乘操作共同作用将细胞状态和输入信号传递到输出端。<br>遗忘门<br><img src="2.png"><br>记忆门<br><img src="3.png"><br>更新细胞状态<br><img src="4.png"><br>输出门:<br><img src="5.png"></p><h4 id="模型搭建"><a href="#模型搭建" class="headerlink" title="模型搭建"></a>模型搭建</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">class</span> <span class="token class-name">RNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">,</span> n_layers<span class="token punctuation">,</span>                  bidirectional<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span> pad_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># 词嵌入</span>        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> padding_idx <span class="token operator">=</span> pad_idx<span class="token punctuation">)</span>                <span class="token comment"># 双向LSTM</span>        self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">,</span>  <span class="token comment"># input_size</span>                           hidden_dim<span class="token punctuation">,</span>  <span class="token comment">#output_size</span>                           num_layers<span class="token operator">=</span>n_layers<span class="token punctuation">,</span>  <span class="token comment"># 层数</span>                           bidirectional<span class="token operator">=</span>bidirectional<span class="token punctuation">,</span> <span class="token comment">#是否双向</span>                           dropout<span class="token operator">=</span>dropout<span class="token punctuation">)</span> <span class="token comment">#随机去除神经元</span>        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span> <span class="token comment"># 因为前向传播+后向传播有两个hidden sate,且合并在一起,所以乘以2</span>        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>            <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">,</span> text_lengths<span class="token punctuation">)</span><span class="token punctuation">:</span>        embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">)</span>        packed_embedded <span class="token operator">=</span> nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>pack_padded_sequence<span class="token punctuation">(</span>embedded<span class="token punctuation">,</span> text_lengths<span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">'cpu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        packed_output<span class="token punctuation">,</span> <span class="token punctuation">(</span>hidden<span class="token punctuation">,</span> cell<span class="token punctuation">)</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>packed_embedded<span class="token punctuation">)</span>        hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>hidden<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> hidden<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>hidden<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="实例化模型"><a href="#实例化模型" class="headerlink" title="实例化模型"></a>实例化模型</h4><p>（由于我是GTX1660Ti的显卡，在训练过程中出现问题，所以将教程中的超参数HIDDEN_DIM 改为了128）</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">INPUT_DIM <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span> <span class="token comment"># 250002: 之前设置的只取25000个最频繁的词,加上pad_token和unknown token</span>EMBEDDING_DIM <span class="token operator">=</span> <span class="token number">100</span>HIDDEN_DIM <span class="token operator">=</span> <span class="token number">128</span> <span class="token comment">#原教程为256</span>OUTPUT_DIM <span class="token operator">=</span> <span class="token number">1</span>N_LAYERS <span class="token operator">=</span> <span class="token number">2</span>BIDIRECTIONAL <span class="token operator">=</span> <span class="token boolean">True</span>DROPOUT <span class="token operator">=</span> <span class="token number">0.5</span>PAD_IDX <span class="token operator">=</span> TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>stoi<span class="token punctuation">[</span>TEXT<span class="token punctuation">.</span>pad_token<span class="token punctuation">]</span> <span class="token comment">#指定参数,定义pad_token的index索引值,让模型不管pad token</span>model <span class="token operator">=</span> RNN<span class="token punctuation">(</span>INPUT_DIM<span class="token punctuation">,</span>             EMBEDDING_DIM<span class="token punctuation">,</span>             HIDDEN_DIM<span class="token punctuation">,</span>             OUTPUT_DIM<span class="token punctuation">,</span>             N_LAYERS<span class="token punctuation">,</span>             BIDIRECTIONAL<span class="token punctuation">,</span>             DROPOUT<span class="token punctuation">,</span>             PAD_IDX<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h4><p>这里的优化器更改为Adam，损失函数还是BCEWithLogitsLoss()</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optimoptimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCEWithLogitsLoss<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 损失函数. criterion 在本task中时损失函数的意思</span>model <span class="token operator">=</span> model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>criterion <span class="token operator">=</span> criterion<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><em>准确率</em></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">binary_accuracy</span><span class="token punctuation">(</span>preds<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>    rounded_preds <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">round</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>preds<span class="token punctuation">)</span><span class="token punctuation">)</span>    correct <span class="token operator">=</span> <span class="token punctuation">(</span>rounded_preds <span class="token operator">==</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#convert into float for division </span>    acc <span class="token operator">=</span> correct<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>correct<span class="token punctuation">)</span>    <span class="token keyword">return</span> acc<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><em>训练函数与评估函数</em></li></ul><p>这里的训练函数和评估函数也与之前类似，不再贴代码</p><ul><li><em>验证</em></li></ul><blockquote><p>“predict_sentiment”函数的作用如下：<br>将模型切换为evaluate模式<br>对句子进行分词操作<br>将分词后的每个词，对应着词汇表，转换成对应的index索引，<br>获取句子的长度<br>将indexes，从list转化成tensor<br>通过unsqueezing 添加一个batch维度<br>将length转化成张量tensor<br>用sigmoid函数将预测值压缩到0-1之间<br>用item（）方法，将只有一个值的张量tensor转化成整数</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> spacynlp <span class="token operator">=</span> spacy<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'en_core_web_sm'</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">predict_sentiment</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>    model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    tokenized <span class="token operator">=</span> <span class="token punctuation">[</span>tok<span class="token punctuation">.</span>text <span class="token keyword">for</span> tok <span class="token keyword">in</span> nlp<span class="token punctuation">.</span>tokenizer<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span><span class="token punctuation">]</span>    indexed <span class="token operator">=</span> <span class="token punctuation">[</span>TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>stoi<span class="token punctuation">[</span>t<span class="token punctuation">]</span> <span class="token keyword">for</span> t <span class="token keyword">in</span> tokenized<span class="token punctuation">]</span>    length <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>indexed<span class="token punctuation">)</span><span class="token punctuation">]</span>    tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>indexed<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>    tensor <span class="token operator">=</span> tensor<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    length_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>length<span class="token punctuation">)</span>    prediction <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>model<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> length_tensor<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> prediction<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><p>贴一张训练图片<br><img src="RNN%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C.jpg"></p><h2 id="task3：Faster-情感分析"><a href="#task3：Faster-情感分析" class="headerlink" title="task3：Faster 情感分析"></a>task3：Faster 情感分析</h2><h3 id="有关fastText"><a href="#有关fastText" class="headerlink" title="有关fastText"></a>有关fastText</h3><ul><li>fastText为每个n字符的gram训练一个向量表示，其中包括词、拼错的词、词片段甚至是单个字符。</li><li>fastText 模型架构和 Word2Vec 的 CBOW 模型类似 。和CBOW一样，fastText模型也只有三层：输入层、隐含层、输出层（Hierarchical Softmax），输入都是多个经向量表示的单词，输出都是一个特定的target，隐含层都是对多个词向量的叠加平均。不同的是，CBOW的输入是目标单词的上下文，fastText的输入是多个单词及其n-gram特征，这些特征用来表示单个文档；CBOW的输入单词被onehot编码过，fastText的输入特征是被embedding过；CBOW的输出是目标词汇，fastText的输出是文档对应的类标。fastText在输入时，将单词的字符级别的n-gram向量作为额外的特征；在输出时，fastText采用了分层Softmax，大大降低了模型训练时间。</li></ul><h3 id="数据预处理-1"><a href="#数据预处理-1" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>FastText分类模型与其他文本分类模型最大的不同之处在于其计算了输入句子的n-gram，并将n-gram作为一种附加特征来获取局部词序特征信息添加至标记化列表的末尾。<br>本次task使用bi-grams。即字节大小为2</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">generate_bigrams</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    n_grams <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">[</span>x<span class="token punctuation">[</span>i<span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> n_gram <span class="token keyword">in</span> n_grams<span class="token punctuation">:</span>        x<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>n_gram<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其他像导入包和加载数据构建迭代器等的步骤同上，不再贴代码~</p><h3 id="构建模型-2"><a href="#构建模型-2" class="headerlink" title="构建模型"></a>构建模型</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">class</span> <span class="token class-name">FastText</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">,</span> pad_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> padding_idx<span class="token operator">=</span>pad_idx<span class="token punctuation">)</span>                self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span>            <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>                      embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>text<span class="token punctuation">)</span>               embedded <span class="token operator">=</span> embedded<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        pooled <span class="token operator">=</span> F<span class="token punctuation">.</span>avg_pool2d<span class="token punctuation">(</span>embedded<span class="token punctuation">,</span> <span class="token punctuation">(</span>embedded<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>         <span class="token keyword">return</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>pooled<span class="token punctuation">)</span>INPUT_DIM <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span>EMBEDDING_DIM <span class="token operator">=</span> <span class="token number">100</span>OUTPUT_DIM <span class="token operator">=</span> <span class="token number">1</span>PAD_IDX <span class="token operator">=</span> TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>stoi<span class="token punctuation">[</span>TEXT<span class="token punctuation">.</span>pad_token<span class="token punctuation">]</span>model <span class="token operator">=</span> FastText<span class="token punctuation">(</span>INPUT_DIM<span class="token punctuation">,</span> EMBEDDING_DIM<span class="token punctuation">,</span> OUTPUT_DIM<span class="token punctuation">,</span> PAD_IDX<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><p>基本也是与之前相同，不赘述</p><h3 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h3><p>贴一个结果图,<br><img src="fastText.jpg"></p><p>fastText相对上一次任务的双向LSTM来说，速度快了不少，由于使用了一些特有的技巧，训练效果也得到了提升。</p><h2 id="task4：卷积情感分析"><a href="#task4：卷积情感分析" class="headerlink" title="task4：卷积情感分析"></a>task4：卷积情感分析</h2><p>呕吼~一直以为卷积只是用来处理图片的，没想到还能用来处理文本<br>卷积神经网络能够从局部输入图像块中提取特征，并能将表示模块化，同时可以高效第利用数据;可以用于处理时序数据，时间可以被看作一个空间维度，就像二维图像的高度和宽度<br>至于为什么可以在文本上使用卷积神经网络，原因如下：</p><ul><li>与3x3 filter可以查看图像块的方式相同，1x2 filter 可以查看一段文本中的两个连续单词，即双字符</li><li>本模型将使用多个不同大小的filter，这些filter将查看文本中的bi-grams（a 1x2 filter）、tri-grams（a 1x3 filter）and/or n-grams（a 1xn filter）。</li></ul><h3 id="数据预处理-2"><a href="#数据预处理-2" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>这里与fastText不同，不需要再创建bi-gram</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torchtext<span class="token punctuation">.</span>legacy <span class="token keyword">import</span> data<span class="token keyword">from</span> torchtext<span class="token punctuation">.</span>legacy <span class="token keyword">import</span> datasets<span class="token keyword">import</span> random<span class="token keyword">import</span> numpy <span class="token keyword">as</span> npSEED <span class="token operator">=</span> <span class="token number">1234</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span>torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span>torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>deterministic <span class="token operator">=</span> <span class="token boolean">True</span>TEXT <span class="token operator">=</span> data<span class="token punctuation">.</span>Field<span class="token punctuation">(</span>tokenize <span class="token operator">=</span> <span class="token string">'spacy'</span><span class="token punctuation">,</span>                   tokenizer_language <span class="token operator">=</span> <span class="token string">'en_core_web_sm'</span><span class="token punctuation">,</span>                  batch_first <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>LABEL <span class="token operator">=</span> data<span class="token punctuation">.</span>LabelField<span class="token punctuation">(</span>dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>train_data<span class="token punctuation">,</span> test_data <span class="token operator">=</span> datasets<span class="token punctuation">.</span>IMDB<span class="token punctuation">.</span>splits<span class="token punctuation">(</span>TEXT<span class="token punctuation">,</span> LABEL<span class="token punctuation">)</span>train_data<span class="token punctuation">,</span> valid_data <span class="token operator">=</span> train_data<span class="token punctuation">.</span>split<span class="token punctuation">(</span>random_state <span class="token operator">=</span> random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span><span class="token punctuation">)</span>MAX_VOCAB_SIZE <span class="token operator">=</span> 25_000TEXT<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">,</span>                  max_size <span class="token operator">=</span> MAX_VOCAB_SIZE<span class="token punctuation">,</span>                  vectors <span class="token operator">=</span> <span class="token string">"glove.6B.100d"</span><span class="token punctuation">,</span>                  unk_init <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">.</span>normal_<span class="token punctuation">)</span>LABEL<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="构建模型-3"><a href="#构建模型-3" class="headerlink" title="构建模型"></a>构建模型</h3><h4 id="有关TextCNN"><a href="#有关TextCNN" class="headerlink" title="有关TextCNN"></a>有关TextCNN</h4><blockquote><p>卷积神经网络的核心思想是捕捉局部特征，对于文本来说，局部特征就是由若干单词组成的滑动窗口，类似于N-gram。卷积神经网络的优势在于能够自动地对N-gram特征进行组合和筛选，获得不同抽象层次的语义信息。</p></blockquote><p>具体的笔记再补~参考文献中的那个 <a href="https://zhuanlan.zhihu.com/p/77634533">深入TextCNN（一）详述CNN及TextCNN原理</a> 感觉讲的很好呀</p><h4 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">class</span> <span class="token class-name">CNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> n_filters<span class="token punctuation">,</span> filter_sizes<span class="token punctuation">,</span> output_dim<span class="token punctuation">,</span>                  dropout<span class="token punctuation">,</span> pad_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> padding_idx <span class="token operator">=</span> pad_idx<span class="token punctuation">)</span>                self<span class="token punctuation">.</span>conv_0 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>                                 out_channels <span class="token operator">=</span> n_filters<span class="token punctuation">,</span>                                 kernel_size <span class="token operator">=</span> <span class="token punctuation">(</span>filter_sizes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>conv_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>                                 out_channels <span class="token operator">=</span> n_filters<span class="token punctuation">,</span>                                 kernel_size <span class="token operator">=</span> <span class="token punctuation">(</span>filter_sizes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>conv_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>                                 out_channels <span class="token operator">=</span> n_filters<span class="token punctuation">,</span>                                 kernel_size <span class="token operator">=</span> <span class="token punctuation">(</span>filter_sizes<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>filter_sizes<span class="token punctuation">)</span> <span class="token operator">*</span> n_filters<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span>                self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>            <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>                        <span class="token comment">#text = [batch size, sent len]</span>                embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>text<span class="token punctuation">)</span>                        <span class="token comment">#embedded = [batch size, sent len, emb dim]</span>                embedded <span class="token operator">=</span> embedded<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>                <span class="token comment">#embedded = [batch size, 1, sent len, emb dim]</span>                conved_0 <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv_0<span class="token punctuation">(</span>embedded<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        conved_1 <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv_1<span class="token punctuation">(</span>embedded<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        conved_2 <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv_2<span class="token punctuation">(</span>embedded<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                    <span class="token comment">#conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]</span>                pooled_0 <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool1d<span class="token punctuation">(</span>conved_0<span class="token punctuation">,</span> conved_0<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>        pooled_1 <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool1d<span class="token punctuation">(</span>conved_1<span class="token punctuation">,</span> conved_1<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>        pooled_2 <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool1d<span class="token punctuation">(</span>conved_2<span class="token punctuation">,</span> conved_2<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>                <span class="token comment">#pooled_n = [batch size, n_filters]</span>                cat <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>pooled_0<span class="token punctuation">,</span> pooled_1<span class="token punctuation">,</span> pooled_2<span class="token punctuation">)</span><span class="token punctuation">,</span> dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment">#cat = [batch size, n_filters * len(filter_sizes)]</span>                    <span class="token keyword">return</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>cat<span class="token punctuation">)</span>INPUT_DIM <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span>EMBEDDING_DIM <span class="token operator">=</span> <span class="token number">100</span>N_FILTERS <span class="token operator">=</span> <span class="token number">100</span>FILTER_SIZES <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">]</span>OUTPUT_DIM <span class="token operator">=</span> <span class="token number">1</span>DROPOUT <span class="token operator">=</span> <span class="token number">0.5</span>PAD_IDX <span class="token operator">=</span> TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>stoi<span class="token punctuation">[</span>TEXT<span class="token punctuation">.</span>pad_token<span class="token punctuation">]</span>model <span class="token operator">=</span> CNN<span class="token punctuation">(</span>INPUT_DIM<span class="token punctuation">,</span> EMBEDDING_DIM<span class="token punctuation">,</span> N_FILTERS<span class="token punctuation">,</span> FILTER_SIZES<span class="token punctuation">,</span> OUTPUT_DIM<span class="token punctuation">,</span> DROPOUT<span class="token punctuation">,</span> PAD_IDX<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>后续的任务还是与之前相同，不再赘述。</p><h3 id="小结-3"><a href="#小结-3" class="headerlink" title="小结"></a>小结</h3><h2 id="task5：多类型情感分析"><a href="#task5：多类型情感分析" class="headerlink" title="task5：多类型情感分析"></a>task5：多类型情感分析</h2><p>进行多分类的情感分析任务时，输出不再是正面负面两个标量了，而是一个C维向量。</p><h3 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h3><p>这次换了一个新的数据集，使用的是TREC数据集而不是IMDB数据集：训练集：5452，测试集：500。</p><ul><li>加载数据：</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torchtext <span class="token keyword">import</span> data<span class="token keyword">from</span> torchtext <span class="token keyword">import</span> datasets<span class="token keyword">import</span> randomSEED <span class="token operator">=</span> <span class="token number">1234</span>torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span>torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>deterministic <span class="token operator">=</span> <span class="token boolean">True</span>TEXT <span class="token operator">=</span> data<span class="token punctuation">.</span>Field<span class="token punctuation">(</span>tokenize <span class="token operator">=</span> <span class="token string">'spacy'</span><span class="token punctuation">)</span>LABEL <span class="token operator">=</span> data<span class="token punctuation">.</span>LabelField<span class="token punctuation">(</span><span class="token punctuation">)</span>train_data<span class="token punctuation">,</span> test_data <span class="token operator">=</span> datasets<span class="token punctuation">.</span>TREC<span class="token punctuation">.</span>splits<span class="token punctuation">(</span>TEXT<span class="token punctuation">,</span> LABEL<span class="token punctuation">,</span> fine_grained<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>train_data<span class="token punctuation">,</span> valid_data <span class="token operator">=</span> train_data<span class="token punctuation">.</span>split<span class="token punctuation">(</span>random_state <span class="token operator">=</span> random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">vars</span><span class="token punctuation">(</span>train_data<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>接下来，我们将构建词汇表。 由于这个数据集很小（只有约 3800 个训练样本），它的词汇量也非常小（约 7500 个不同单词，即one-hot向量为7500维），这意味着我们不需要像以前一样在词汇表上设置“max_size”。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">MAX_VOCAB_SIZE <span class="token operator">=</span> 25_000TEXT<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">,</span>                  max_size <span class="token operator">=</span> MAX_VOCAB_SIZE<span class="token punctuation">,</span>                  vectors <span class="token operator">=</span> <span class="token string">"glove.6B.100d"</span><span class="token punctuation">,</span>                  unk_init <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">.</span>normal_<span class="token punctuation">)</span>LABEL<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>检查标签</li></ul><blockquote><p>6 个标签（对于非细粒度情况）对应于数据集中的 6 类问题：<br>HUM：关于人类的问题<br>ENTY：关于实体的问题的<br>DESC：关于要求提供描述的问题<br>NUM：关于答案为数字的问题<br>LOC：关于答案是位置的问题<br>ABBR：关于询问缩写的问题</p></blockquote><h3 id="构建模型-4"><a href="#构建模型-4" class="headerlink" title="构建模型"></a>构建模型</h3><p>这次使用Task04中的CNN模型，将之前模型中的 output_dim 改为 𝐶 维而不是 2 维即可，同时确保将输出维度： OUTPUT_DIM 设置为  𝐶 </p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">class</span> <span class="token class-name">CNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> n_filters<span class="token punctuation">,</span> filter_sizes<span class="token punctuation">,</span> output_dim<span class="token punctuation">,</span>                  dropout<span class="token punctuation">,</span> pad_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span>                self<span class="token punctuation">.</span>convs <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>                                    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>                                               out_channels <span class="token operator">=</span> n_filters<span class="token punctuation">,</span>                                               kernel_size <span class="token operator">=</span> <span class="token punctuation">(</span>fs<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>                                     <span class="token keyword">for</span> fs <span class="token keyword">in</span> filter_sizes                                    <span class="token punctuation">]</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>filter_sizes<span class="token punctuation">)</span> <span class="token operator">*</span> n_filters<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span>                self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>            <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>        text <span class="token operator">=</span> text<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>        embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>text<span class="token punctuation">)</span>        embedded <span class="token operator">=</span> embedded<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>        conved <span class="token operator">=</span> <span class="token punctuation">[</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>conv<span class="token punctuation">(</span>embedded<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span> <span class="token keyword">for</span> conv <span class="token keyword">in</span> self<span class="token punctuation">.</span>convs<span class="token punctuation">]</span>        pooled <span class="token operator">=</span> <span class="token punctuation">[</span>F<span class="token punctuation">.</span>max_pool1d<span class="token punctuation">(</span>conv<span class="token punctuation">,</span> conv<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token keyword">for</span> conv <span class="token keyword">in</span> conved<span class="token punctuation">]</span>        cat <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>pooled<span class="token punctuation">,</span> dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                    <span class="token keyword">return</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>cat<span class="token punctuation">)</span>INPUT_DIM <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span>EMBEDDING_DIM <span class="token operator">=</span> <span class="token number">100</span>N_FILTERS <span class="token operator">=</span> <span class="token number">100</span>FILTER_SIZES <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span>OUTPUT_DIM <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>LABEL<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span>DROPOUT <span class="token operator">=</span> <span class="token number">0.5</span>PAD_IDX <span class="token operator">=</span> TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>stoi<span class="token punctuation">[</span>TEXT<span class="token punctuation">.</span>pad_token<span class="token punctuation">]</span>model <span class="token operator">=</span> CNN<span class="token punctuation">(</span>INPUT_DIM<span class="token punctuation">,</span> EMBEDDING_DIM<span class="token punctuation">,</span> N_FILTERS<span class="token punctuation">,</span> FILTER_SIZES<span class="token punctuation">,</span> OUTPUT_DIM<span class="token punctuation">,</span> DROPOUT<span class="token punctuation">,</span> PAD_IDX<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>与前几个notebook比,损失函数(也就是criterion)是不同的.之前用的是BCEWithLogisLoss,现在使用的是CrossEntropyLoss,它使用的是softrmax函数,来计算cross entropy</p><p>一般来说:</p><ul><li>CrossEntropyLoss :用于多分类问题</li><li>BCEWithLogitsLoss :用于2分类问题,(0,1),也用于多标签分类(multilabel classification) 1vs rest</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optimoptimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>model <span class="token operator">=</span> model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>criterion <span class="token operator">=</span> criterion<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="小结-4"><a href="#小结-4" class="headerlink" title="小结"></a>小结</h3><p>贴一个结果图<br><img src="%E5%A4%9A%E5%88%86%E7%B1%BB.jpg"></p><h2 id="task6：使用Transformer进行情感分析"><a href="#task6：使用Transformer进行情感分析" class="headerlink" title="task6：使用Transformer进行情感分析"></a>task6：使用Transformer进行情感分析</h2><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><p>本次任务使用的是Bert模型，由于模型很大，参数很多，所以将固定（而不训练）transformer，只训练从transformer产生的表示中学习的模型的其余部分。<br>在这种情况下，我们将使用双向GRU继续提取从Bert embedding后的特征。最后在fc层上输出最终的结果。</p><ul><li><em>导入相关库，并设定随机种子</em></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> random<span class="token keyword">import</span> numpy <span class="token keyword">as</span> npSEED <span class="token operator">=</span> <span class="token number">1234</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span>torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span>torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>deterministic <span class="token operator">=</span> <span class="token boolean">True</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>transformers 库为每个提供的transformer 模型都有分词器。 在这种情况下，我们使用忽略大小写的 BERT 模型（即每个单词都会小写）。 我们通过加载预训练的“bert-base-uncased”标记器来实现这一点。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> BertTokenizertokenizer <span class="token operator">=</span> BertTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'bert-base-uncased'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>使用tokenizer.tokenize方法对字符串进行分词，并统一大小写。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">tokens <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span><span class="token string">'Hello WORLD how ARE yoU?'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li><em>定义一个函数处理所有的标记化。</em></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">tokenize_and_cut</span><span class="token punctuation">(</span>sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>    tokens <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span>     tokens <span class="token operator">=</span> tokens<span class="token punctuation">[</span><span class="token punctuation">:</span>max_input_length<span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span>    <span class="token keyword">return</span> tokens<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ul><li><em>定义标签字段</em></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> torchtext<span class="token punctuation">.</span>legacy <span class="token keyword">import</span> dataTEXT <span class="token operator">=</span> data<span class="token punctuation">.</span>Field<span class="token punctuation">(</span>batch_first <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>                  use_vocab <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>                  tokenize <span class="token operator">=</span> tokenize_and_cut<span class="token punctuation">,</span>                  preprocessing <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>convert_tokens_to_ids<span class="token punctuation">,</span>                  init_token <span class="token operator">=</span> init_token_idx<span class="token punctuation">,</span>                  eos_token <span class="token operator">=</span> eos_token_idx<span class="token punctuation">,</span>                  pad_token <span class="token operator">=</span> pad_token_idx<span class="token punctuation">,</span>                  unk_token <span class="token operator">=</span> unk_token_idx<span class="token punctuation">)</span>LABEL <span class="token operator">=</span> data<span class="token punctuation">.</span>LabelField<span class="token punctuation">(</span>dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><em>拆分出训练集和验证集</em></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> torchtext<span class="token punctuation">.</span>legacy <span class="token keyword">import</span> datasetstrain_data<span class="token punctuation">,</span> test_data <span class="token operator">=</span> datasets<span class="token punctuation">.</span>IMDB<span class="token punctuation">.</span>splits<span class="token punctuation">(</span>TEXT<span class="token punctuation">,</span> LABEL<span class="token punctuation">)</span>train_data<span class="token punctuation">,</span> valid_data <span class="token operator">=</span> train_data<span class="token punctuation">.</span>split<span class="token punctuation">(</span>random_state <span class="token operator">=</span> random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><em>创建迭代器</em></li></ul><p>这里依旧是不能用教程里面的参数128，自己训练的时候改为了64</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">BATCH_SIZE <span class="token operator">=</span> <span class="token number">128</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">'cpu'</span><span class="token punctuation">)</span>train_iterator<span class="token punctuation">,</span> valid_iterator<span class="token punctuation">,</span> test_iterator <span class="token operator">=</span> data<span class="token punctuation">.</span>BucketIterator<span class="token punctuation">.</span>splits<span class="token punctuation">(</span>    <span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> valid_data<span class="token punctuation">,</span> test_data<span class="token punctuation">)</span><span class="token punctuation">,</span>     batch_size <span class="token operator">=</span> BATCH_SIZE<span class="token punctuation">,</span>     device <span class="token operator">=</span> device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="构建模型-5"><a href="#构建模型-5" class="headerlink" title="构建模型"></a>构建模型</h3><ul><li><em>导入预训练的bert</em></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> BertTokenizer<span class="token punctuation">,</span> BertModelbert <span class="token operator">=</span> BertModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'bert-base-uncased'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li><em>定义模型</em></li></ul><blockquote><p>*我们将使用预训练的Transformer模型，而不是使用embedding层来获取文本的embedding。然后将这些embedding输入GRU以生成对句子情绪的预测。<br>*通过config属性从transformer中获取嵌入维度大小（称为hidden_size）,其余初始化是标准的<br>*在前向传递的过程中，我们将transformer包装在一个no_grad中，以确保不会再模型的这部分计算梯度。<br>*transformer实际上返回整个序列的embedding以及pooled输入。<br>*前向传递的其余部分是循环模型的标准实现，我们在最后的时间步长中获取隐藏状态，并将其传递给一个线性层以获得我们的预测。</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">class</span> <span class="token class-name">BERTGRUSentiment</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>                 bert<span class="token punctuation">,</span>                 hidden_dim<span class="token punctuation">,</span>                 output_dim<span class="token punctuation">,</span>                 n_layers<span class="token punctuation">,</span>                 bidirectional<span class="token punctuation">,</span>                 dropout<span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>bert <span class="token operator">=</span> bert                embedding_dim <span class="token operator">=</span> bert<span class="token punctuation">.</span>config<span class="token punctuation">.</span>to_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'hidden_size'</span><span class="token punctuation">]</span>                self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">,</span>                          hidden_dim<span class="token punctuation">,</span>                          num_layers <span class="token operator">=</span> n_layers<span class="token punctuation">,</span>                          bidirectional <span class="token operator">=</span> bidirectional<span class="token punctuation">,</span>                          batch_first <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>                          dropout <span class="token operator">=</span> <span class="token number">0</span> <span class="token keyword">if</span> n_layers <span class="token operator">&lt;</span> <span class="token number">2</span> <span class="token keyword">else</span> dropout<span class="token punctuation">)</span>                self<span class="token punctuation">.</span>out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim <span class="token operator">*</span> <span class="token number">2</span> <span class="token keyword">if</span> bidirectional <span class="token keyword">else</span> hidden_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span>                self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>            <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>                       <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>bert<span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>                        _<span class="token punctuation">,</span> hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>embedded<span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>bidirectional<span class="token punctuation">:</span>            hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>hidden<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> hidden<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>hidden<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>               output <span class="token operator">=</span> self<span class="token punctuation">.</span>out<span class="token punctuation">(</span>hidden<span class="token punctuation">)</span>        <span class="token keyword">return</span> output<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><em>创建实例</em></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">HIDDEN_DIM <span class="token operator">=</span> <span class="token number">256</span>OUTPUT_DIM <span class="token operator">=</span> <span class="token number">1</span>N_LAYERS <span class="token operator">=</span> <span class="token number">2</span>BIDIRECTIONAL <span class="token operator">=</span> <span class="token boolean">True</span>DROPOUT <span class="token operator">=</span> <span class="token number">0.25</span>model <span class="token operator">=</span> BERTGRUSentiment<span class="token punctuation">(</span>bert<span class="token punctuation">,</span>                         HIDDEN_DIM<span class="token punctuation">,</span>                         OUTPUT_DIM<span class="token punctuation">,</span>                         N_LAYERS<span class="token punctuation">,</span>                         BIDIRECTIONAL<span class="token punctuation">,</span>                         DROPOUT<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="训练和评估"><a href="#训练和评估" class="headerlink" title="训练和评估"></a>训练和评估</h3><ul><li><em>定义损失函数</em></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optimoptimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCEWithLogitsLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>model <span class="token operator">=</span> model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>criterion <span class="token operator">=</span> criterion<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><em>构建训练函数</em></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> iterator<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span><span class="token punctuation">:</span>        epoch_loss <span class="token operator">=</span> <span class="token number">0</span>    epoch_acc <span class="token operator">=</span> <span class="token number">0</span>        model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> batch <span class="token keyword">in</span> iterator<span class="token punctuation">:</span>                optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>                predictions <span class="token operator">=</span> model<span class="token punctuation">(</span>batch<span class="token punctuation">.</span>text<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>                loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>                acc <span class="token operator">=</span> binary_accuracy<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>                loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>                optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>                epoch_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        epoch_acc <span class="token operator">+=</span> acc<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">return</span> epoch_loss <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span><span class="token punctuation">,</span> epoch_acc <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><em>评估函数</em></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">evaluate</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> iterator<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span><span class="token punctuation">:</span>        epoch_loss <span class="token operator">=</span> <span class="token number">0</span>    epoch_acc <span class="token operator">=</span> <span class="token number">0</span>        model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">for</span> batch <span class="token keyword">in</span> iterator<span class="token punctuation">:</span>            predictions <span class="token operator">=</span> model<span class="token punctuation">(</span>batch<span class="token punctuation">.</span>text<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>                        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>                        acc <span class="token operator">=</span> binary_accuracy<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>            epoch_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>            epoch_acc <span class="token operator">+=</span> acc<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">return</span> epoch_loss <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span><span class="token punctuation">,</span> epoch_acc <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><em>训练</em></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">N_EPOCHS <span class="token operator">=</span> <span class="token number">5</span>best_valid_loss <span class="token operator">=</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token string">'inf'</span><span class="token punctuation">)</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>N_EPOCHS<span class="token punctuation">)</span><span class="token punctuation">:</span>        start_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>        train_loss<span class="token punctuation">,</span> train_acc <span class="token operator">=</span> train<span class="token punctuation">(</span>model<span class="token punctuation">,</span> train_iterator<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span>    valid_loss<span class="token punctuation">,</span> valid_acc <span class="token operator">=</span> evaluate<span class="token punctuation">(</span>model<span class="token punctuation">,</span> valid_iterator<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span>            end_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>            epoch_mins<span class="token punctuation">,</span> epoch_secs <span class="token operator">=</span> epoch_time<span class="token punctuation">(</span>start_time<span class="token punctuation">,</span> end_time<span class="token punctuation">)</span>            <span class="token keyword">if</span> valid_loss <span class="token operator">&lt;</span> best_valid_loss<span class="token punctuation">:</span>        best_valid_loss <span class="token operator">=</span> valid_loss        torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'tut6-model.pt'</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Epoch: </span><span class="token interpolation"><span class="token punctuation">{</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token format-spec">02</span><span class="token punctuation">}</span></span><span class="token string"> | Epoch Time: </span><span class="token interpolation"><span class="token punctuation">{</span>epoch_mins<span class="token punctuation">}</span></span><span class="token string">m </span><span class="token interpolation"><span class="token punctuation">{</span>epoch_secs<span class="token punctuation">}</span></span><span class="token string">s'</span></span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'\tTrain Loss: </span><span class="token interpolation"><span class="token punctuation">{</span>train_loss<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string"> | Train Acc: </span><span class="token interpolation"><span class="token punctuation">{</span>train_acc<span class="token operator">*</span><span class="token number">100</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string">%'</span></span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'\t Val. Loss: </span><span class="token interpolation"><span class="token punctuation">{</span>valid_loss<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string"> |  Val. Acc: </span><span class="token interpolation"><span class="token punctuation">{</span>valid_acc<span class="token operator">*</span><span class="token number">100</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string">%'</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="小结-5"><a href="#小结-5" class="headerlink" title="小结"></a>小结</h3><p>训练速度很慢，BATCH_SIZE依旧是不能用教程里面的128，改为64之后训练结束~<br>贴一张训练图<br><img src="9.png"><br>\[y = {f_{ {g_1}}}(x)\]<br>参考链接：<a href="https://zhuanlan.zhihu.com/p/31139113">torchtext入门教程，轻松玩转文本数据处理</a><br>        <a href="https://zhuanlan.zhihu.com/p/104475016">[干货]深入浅出LSTM及其Python代码实现</a><br>        <a href="https://zhuanlan.zhihu.com/p/32965521">fastText原理及实践</a><br>        <a href="https://blog.csdn.net/feilong_csdn/article/details/88655927">fastText原理和文本分类实战，看这一篇就够了</a><br>        <a href="https://zhuanlan.zhihu.com/p/77634533">深入TextCNN（一）详述CNN及TextCNN原理</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>ubuntu16.04+google+shadowsocks 科学上网配置教程 </title>
      <link href="/2019/05/30/ubuntu16-04-google-shadowsocks-ke-xue-shang-wang-pei-zhi-jiao-cheng/"/>
      <url>/2019/05/30/ubuntu16-04-google-shadowsocks-ke-xue-shang-wang-pei-zhi-jiao-cheng/</url>
      
        <content type="html"><![CDATA[<p>最近使用ubuntu系统调试bug的时候，百度问题的时候超级费劲，一怒之下还是安装了google翻墙去国外搜索教程，下面是本次安装的具体过程，尽情食用~</p><h2 id="安装-google-chrome"><a href="#安装-google-chrome" class="headerlink" title="安装 google chrome"></a>安装 google chrome</h2><ul><li>进入 Ubuntu 16.04 桌面，按下 Ctrl + Alt + t 键盘组合键，启动终端。</li><li>在终端中，输入<br><code>sudo wget http://www.linuxidc.com/files/repo/google-chrome.list -P /etc/apt/sources.list.d/</code></li><li>在终端中，输入 <code>wget -q -O - https://dl.google.com/linux/linux_signing_key.pub  | sudo apt-key add -</code> ,导入谷歌软件的公钥，用于下面步骤中对下载软件进行验证。如果顺利的话，命令将返回“OK”。</li><li>在终端中，输入 <code>sudo apt-get update</code>用于对当前系统的可用更新列表进行更新。</li><li>在终端中，输入 <code>sudo apt-get install google-chrome-stable</code> ,执行对谷歌 Chrome 浏览器（稳定版）的安装。</li><li>最后，如果一切顺利，在终端中执行 <code>/usr/bin/google-chrome-stable</code> ,将会启动谷歌 Chrome 浏览器，它的图标将会出现在屏幕左侧的 <code>Launcher</code> 上，在图标上右键 -&gt; <code>锁定到启动器</code>，以后就可以简单地单击启动了。</li></ul><h2 id="shadowsocks-教程"><a href="#shadowsocks-教程" class="headerlink" title="shadowsocks 教程"></a>shadowsocks 教程</h2><h3 id="加入PPA源和更新、安装命令"><a href="#加入PPA源和更新、安装命令" class="headerlink" title="加入PPA源和更新、安装命令"></a>加入PPA源和更新、安装命令</h3><ul><li>命令行输入以下指令：</li></ul> <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">sudo</span> add-apt-repository ppa:hzwhuang/ss-qt5<span class="token function">sudo</span> <span class="token function">apt-get</span> update<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> shadowsocks-qt5<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ul><li>成功后打开 <code>Dash</code> ，搜索 <code>Shadowscoks</code></li></ul><p><img src="%E6%90%9C%E7%B4%A2.png" alt="搜索"></p><ul><li>打开后 -&gt; 选择 <code>连接</code> -&gt; <code>添加</code> -&gt; <code>手动</code> -&gt; 添加shandowsocks的服务器地址、服务器端口、密钥、加密方式</li></ul><p><img src="%E9%85%8D%E7%BD%AE%E7%BC%96%E8%BE%91%E5%99%A8.png" alt="配置编辑器"></p><ul><li>设置完成后，选择 <code>连接</code></li></ul><h3 id="SwitchyOmega"><a href="#SwitchyOmega" class="headerlink" title="SwitchyOmega"></a>SwitchyOmega</h3><ul><li>下载插件，附一下载地址：<a href="https://github.com/FelisCatus/SwitchyOmega/releases">SwitchyOmega for Chromium &amp; Firefox</a>，选择 <code>SwitchyOmega_Chromium.crx</code> ,点击下载</li><li>安装插件：打开chrom浏览器，单机右上角的三个点，选择 <code>更多工具</code> -&gt; <code>扩展程序</code> -&gt; 打开右上角的 <code>开发者模式</code> 开关 -&gt; 将下载的 <code>SwitchyOmega_Chromium.crx</code> 拖拽进来，安装成功</li></ul><p><img src="%E6%B7%BB%E5%8A%A0%E6%8F%92%E4%BB%B6.png" alt="添加插件"><br><em>注：安装这个插件的时候我这里出现不能安装甚至按照网上的教程重命名成zip文件解压各种方法都无法安装的问题，这里我的解决办法就是试一下下载低版本的插件哈哈哈哈没想到成功了</em></p><ul><li><p>配置</p><ul><li>SwitchyOmega插件安装成功后，chrom浏览器右上角会出现一个黑圈圈的标志，点击这个黑圈圈，进入 <code>SwitchyOmega选项</code></li><li>选择 <code>新建情景模式</code> ，随意起一个情景模式名称，选择 <code>代理服务器</code></li></ul><p><img src="%E6%83%85%E6%99%AF%E6%A8%A1%E5%BC%8F.png" alt="情景模式"></p><ul><li>修改代理服务器：将 <code>代理协议</code> 修改为 <code>SOCKS5</code> ,将 <code>代理服务器</code> 修改为 <code>127.0.0.1</code> ,在下面不代理的地址列表中可以添加不想代理的地址，比如说百度。</li></ul><p><img src="%E4%BB%A3%E7%90%86%E6%9C%8D%E5%8A%A1%E5%99%A8.png" alt="代理服务器"></p><ul><li>点击左侧的 <code>应用选项</code> 保存</li></ul></li></ul><p>这样就可以尽情的在Ubuntu系统下尽情的科学上网啦，另外推荐一个google插件，叫 <code>油猴</code> 巨巨巨巨好用！详细安装教程见此链接 <a href="https://sspai.com/post/40485">用 Chrome 的人都需要知道的「神器」扩展：「油猴」使用详解</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> ubuntu16.04 </tag>
            
            <tag> google </tag>
            
            <tag> shadowsocks </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用nohup后台运行python脚本</title>
      <link href="/2019/05/26/shi-yong-nolup-hou-tai-yun-xing-python-jiao-ben/"/>
      <url>/2019/05/26/shi-yong-nolup-hou-tai-yun-xing-python-jiao-ben/</url>
      
        <content type="html"><![CDATA[<p>本地连接服务器训练一个机器学习或者深度学习模型的时候，需要花很长时间才能训练出一个结果，但是在这个过程中本地的ssh连接很容易就断开了，这时在服务器上跑的程序也就跟着断掉了，这个时候就非常非常需要后台运行训练程序。</p><p>先记录一下使用nohup解决这个问题的过程</p><ul><li>因为服务器多人在使用，所以在进入正题之前还是需要先激活环境：source 环境名/bin/activate</li><li>打开程序所在文件夹：cd 某文件夹</li></ul><p>接下来进入正题，使用nohup：</p><ul><li>在上述步骤结束之后，继续输入：<code>nohup python -u 程序名.py &gt; /日志路径 日志名.log 2&gt;&amp;1 &amp;</code></li></ul><p>验证程序是否已经在后台运行</p><ul><li>成功之后，关闭当前终端</li><li>打开新的终端，ssh命令连接服务器</li><li>输入 <code>ps -aux|grep 程序名.py</code> ，就能看到正在运行的程序啦</li></ul><p>训练完毕后杀死进程</p><ul><li>在执行 <code>ps -aux|grep 程序名.py</code> 的时候，我们可以看到第二列是程序运行的ID，记下ID号码</li><li>命令行输入 <code>kill -9 ID号</code> ，就可以杀死进程啦</li></ul><p>训练完成后想看程序中关于 <code>print</code> 的输出，只要打开自定义的目录文件下的 <code>目录.log</code> 就可以查看啦~</p>]]></content>
      
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 服务器 </tag>
            
            <tag> nohup </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ubuntu常用指令集合(持续更新中✧(≖ ◡ ≖✿)</title>
      <link href="/2019/05/26/ubuntu-chang-yong-zhi-ling-ji-he/"/>
      <url>/2019/05/26/ubuntu-chang-yong-zhi-ling-ji-he/</url>
      
        <content type="html"><![CDATA[<h2 id="文件操作"><a href="#文件操作" class="headerlink" title="文件操作"></a>文件操作</h2><h3 id="打开及查看文件夹"><a href="#打开及查看文件夹" class="headerlink" title="打开及查看文件夹"></a>打开及查看文件夹</h3><ul><li>打开文件夹： cd 文件路径</li><li>返回上一目录：cd ..</li><li>列出文件夹下的所有文件：ls</li><li>查看文件夹下所有文件的具体信息：ls -al</li></ul><h3 id="打开及修改文档"><a href="#打开及修改文档" class="headerlink" title="打开及修改文档"></a>打开及修改文档</h3><ul><li>vim</li><li>输入 <code>i</code>切换到输入模式，以输入字符</li><li>跳到文本的最后一行：按“G”,即“shift+g”</li><li>跳到最后一行的最后一个字符：先重复1的操作即按“G”，之后按“$”键，即“shift+4”。</li><li>跳到第一行的第一个字符：先按两次“g”</li><li>跳转到当前行的第一个字符：在当前行按“0”。</li><li>保存退出：按左上角的 <code>Esc</code> 按键 -&gt; 输入 <code>:wq</code></li></ul><h3 id="删除文件夹"><a href="#删除文件夹" class="headerlink" title="删除文件夹"></a>删除文件夹</h3><ul><li>删除某一个文件 <strong><code>rm 文件名</code></strong></li><li>删除整个文件夹 <strong><code>rm -rf /文件夹路径</code></strong> (据说这个操作在某些efi机器上还会删除主板固件，造成主板固件丢失从而无法开机(比操作系统无法启动还严重，瑟瑟发抖~)</li></ul><h3 id="解压文件"><a href="#解压文件" class="headerlink" title="解压文件"></a>解压文件</h3><ul><li>zip文件：**<code>unzip 文件名.zip</code>**</li><li>tar文件：**<code>tar -xvf 文件名.tar</code>**</li><li>gz文件： <strong><code>gunzip 文件名.gz</code></strong></li><li>rar文件：**<code>rar x 文件名.rar</code>**<br>注：如果要解压到指定目录下，在上述操作之后加上 <code>-C 路径</code> 就可以啦，比如说 <code>unzip 文件名.zip -C /home/jyl</code></li></ul><h3 id="复制文件"><a href="#复制文件" class="headerlink" title="复制文件"></a>复制文件</h3><p> <strong><code>cp -参数 源文件或文件夹路径 目标文件夹或文件夹路径</code></strong><br>  这里的参数有：</p><ul><li>-a: 该选项通常在复制文件夹时使用。它保留链接、文件属性，并递归地复制文件夹，其作用等于dpR选项的组合。</li><li>-d:拷贝时保留链接。</li><li>-f:删除已经存在的目标文件而不提示。</li><li>-i:和f选项相反，在覆盖目标文件之前将给出提示要求用户确认。回答y时目标文件将被覆盖，是交互式拷贝。</li><li>-p:此时cp除复制源文件的内容外，还将把其改动时间和訪问权限也拷贝到新文件里。</li><li>-r:若给出的源文件是一文件夹文件，此时cp将递归复制该文件夹下全部的子文件夹和文件。此时目标文件必须为一个文件夹名。</li><li>-l:不作拷贝，仅仅是链接文件。<br> <strong>注：为防止用户在不经意的情况下用cp命令破坏还有一个文件，如用户指定的目标文件名称已存在，用cp命令复制文件后，这个文件就会被新源文件覆盖，因此，建议用户在使用cp命令复制文件时，最好使用i选项。</strong><br> 比如从 <code>/home/download</code> 文件夹下复制 <code>123.text</code> 到 目录 <code>/home/jyl/test</code> 下的指令为 <code>cp -i /home/download/123.text /home/jyl/test</code></li></ul><h3 id="移动与重命名"><a href="#移动与重命名" class="headerlink" title="移动与重命名"></a>移动与重命名</h3><p> <strong><code>mv 源文件目录 目标文件目录</code></strong><br> 以上命令可以将源文件目录下的文件移动到目标文件目录下<br> <strong><code>mv 修改前的文件名 想要修改成的文件名</code></strong><br> 以上命令可以给想要修改名字的文件更名</p><h3 id="给带括号的文件更名（否则会出现错误）"><a href="#给带括号的文件更名（否则会出现错误）" class="headerlink" title="给带括号的文件更名（否则会出现错误）"></a>给带括号的文件更名（否则会出现错误）</h3><p><strong><code>mv 文件名\(带括号的部分\).后缀 去掉括号的文件名（要更新的文件名）.后缀</code></strong><br>比如：我有一个文件名为 <code>hhh(a)hh.py</code><br>更改方式：<code>mv hhh\(a\)hh.py hhhahh.py</code></p><h3 id="本地复制文件到远程服务器上"><a href="#本地复制文件到远程服务器上" class="headerlink" title="本地复制文件到远程服务器上"></a>本地复制文件到远程服务器上</h3><p><strong><code>scp -参数 本地文件或文件夹路径 服务器文件或文件夹路径</code></strong><br> 这里的参数有：</p><ul><li><p>-1： 强制scp命令使用协议ssh1</p></li><li><p>-2： 强制scp命令使用协议ssh2</p></li><li><p>-4： 强制scp命令只使用IPv4寻址</p></li><li><p>-6： 强制scp命令只使用IPv6寻址</p></li><li><p>-B： 使用批处理模式（传输过程中不询问传输口令或短语）</p></li><li><p>-C： 允许压缩。（将-C标志传递给ssh，从而打开压缩功能）</p></li><li><p>-p： 保留原文件的修改时间，访问时间和访问权限。</p></li><li><p>-q： 不显示传输进度条。</p></li><li><p>-r： 递归复制整个目录。</p></li><li><p>-v： 详细方式显示输出。scp和ssh(1)会显示出整个过程的调试信息。这些信息用于调试连接，验证和配置问题。</p></li><li><p>-c cipher： 以cipher将数据传输进行加密，这个选项将直接传递给ssh。</p></li><li><p>-F ssh_config： 指定一个替代的ssh配置文件，此参数直接传递给ssh。</p></li><li><p>-i identity_file： 从指定文件中读取传输时使用的密钥文件，此参数直接传递给ssh。</p></li><li><p>-l limit： 限定用户所能使用的带宽，以Kbit/s为单位。</p></li><li><p>-o ssh_option： 如果习惯于使用ssh_config(5)中的参数传递方式。</p></li><li><p>-P port：注意是大写的P, port是指定数据传输用到的端口号。</p></li><li><p>-S program： 指定加密传输时所使用的程序。此程序必须能够理解ssh(1)的选项。</p><p>以上这些参数其实我也没全用过，满足我正常传文件的需求就使用过 <code>-r</code> ，比如讲本地目录 <code>C:\Users\Administrator\Desktop</code> 下的 <code>test</code> 整个文件夹上传到服务器目录 <code>/home/jyl</code> 下的指令为: <code>scp -r C:\Users\Administrator\Desktop\test 服务器用户名@服务器地址:/home/jyl</code></p></li></ul><p>注：用 <code>scp</code> 传输大文件不知道为什么在我的操作过程中巨慢!!(这个问题如果后期我整理出了问题所在或者解决办法再来补充)，目前最成功的的解决方法是直接用 <code>BaiduPCS-Goo</code> ，将大文件上传到百度云之后，用此工具直接在ubuntu系统中下载下来，教程也在维的博客中出现过，贴一个链接：<a href="https://jiyali.github.io/2019/05/22/linux%E7%BB%88%E7%AB%AF%E4%BD%BF%E7%94%A8%E7%99%BE%E5%BA%A6%E4%BA%91%E7%9A%84%E6%96%B9%E6%B3%95-BaiduPCS-Goo/">linux终端使用百度云的方法–BaiduPCS-Goo</a></p><h2 id="查看进程状态"><a href="#查看进程状态" class="headerlink" title="查看进程状态"></a>查看进程状态</h2><h3 id="ps"><a href="#ps" class="headerlink" title="ps"></a><strong><code>ps</code></strong></h3><ul><li>查询指定进程的PID：<code>ps -ef|grep 进程名</code></li><li>查询指定进程名或者PID的占用情况：<code>ps -aux|grep 进程名/PID</code></li></ul><h3 id="top"><a href="#top" class="headerlink" title="top"></a><strong><code>top</code></strong></h3><p>  <strong><code>top [-参数]</code></strong><br>  这里的可选参数为:</p><ul><li>d：指定每两次屏幕信息刷新之间的时间间隔。当然用户可以使用s交互命令来改变之。</li><li>p:通过指定监控进程ID来仅仅监控某个进程的状态。</li><li>q:该选项将使top没有任何延迟的进行刷新。如果调用程序有超级用户权限，那么top将以尽可能高的优先级运行。</li><li>S：指定累计模式</li><li>s：使top命令在安全模式中运行。这将去除交互命令所带来的潜在危险。</li><li>i：使top不显示任何闲置或者僵死进程。</li><li>c:显示整个命令行而不只是显示命令名。</li></ul><p>退出：<code>ctrl+c</code> 或者 <code>q</code><br>其实说实话，直接 <code>top -n 2</code> 这里的 <code>2</code> 代表刷新次数，就可以了，防止top命令卡住最好用的方式哈哈哈</p><h3 id="查看占用cpu最高的进程"><a href="#查看占用cpu最高的进程" class="headerlink" title="查看占用cpu最高的进程"></a>查看占用cpu最高的进程</h3><p>  <strong><code>ps aux|head -1;ps aux|grep -v PID|sort -rn -k +3|head</code></strong></p><h3 id="查看占用内存最高的进程"><a href="#查看占用内存最高的进程" class="headerlink" title="查看占用内存最高的进程"></a>查看占用内存最高的进程</h3><p>  <strong><code>ps aux|head -1;ps aux|grep -v PID|sort -rn -k +4|head</code></strong></p><h2 id="查看服务器带宽"><a href="#查看服务器带宽" class="headerlink" title="查看服务器带宽"></a>查看服务器带宽</h2><ul><li><code>ifconfig</code></li><li><code>sudo ethtool xxx</code></li></ul><p>暂时能想起来的就这么多啦，后续只要用过的指令会持续更新的！✧(≖ ◡ ≖✿</p><p>参考:<br>  <a href="https://www.runoob.com/linux/linux-command-manual.html">linux命令大全</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> ubuntu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux终端使用百度云的方法--BaiduPCS-Goo</title>
      <link href="/2019/05/22/linux-zhong-duan-shi-yong-bai-du-yun-de-fang-fa-baidupcs-goo/"/>
      <url>/2019/05/22/linux-zhong-duan-shi-yong-bai-du-yun-de-fang-fa-baidupcs-goo/</url>
      
        <content type="html"><![CDATA[<p>今天干了件大事哈哈哈哈哈哈哈哈哈哈！！！<br>连接远程服务器的时候传输大文件确实是一件很麻烦很无聊的事情，关键是浪费嗣干！机智的我用了百度云哈哈哈哈哈哈！！！<br>废话不多说！夸我就是！教程如下：</p><p>0.先附一官方文档<a href="https://github.com/iikira/BaiduPCS-Go">BaiduPCS-Go 百度网盘客户端</a></p><p>1.运行如下代码下载</p><pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">wget https://github.com/iikira/BaiduPCS-Go/releases/download/v3.5.6/BaiduPCS-Go-v3.5.6-linux-amd64.zip<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2.下载完成后解压</p><pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">unzip BaiduPCS-Go-v3.5.6-linux-amd64.zip<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3.将解压后的文件改名为BaiduPCS-Go:</p><pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">mv BaiduPCS-Go-v3.5.6-linux-amd64 BaiduPCS-Go<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4.打开文件</p><pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">cd BaiduPCS-Go/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>5.运行</p><pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">sudo ./BaiduPCS-Go<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>6.登录</p><pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">login然后按照各种登录提示操作即可<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><img src="%E7%99%BB%E5%BD%95.png" alt="登录"></p><p>在这里附上用cookie方式登录的教程，如下：</p><ul><li><p>打开百度云，登录账号，登陆成功后，打开地址栏前面的锁头：<br><img src="%E5%B0%8F%E9%94%81%E5%A4%B4.png" alt="小锁头"></p></li><li><p>跳出窗口，选择 <code>Cookie</code><br><img src="cookie.png" alt="cookie"></p></li><li><p>选择 <code>baidu.com</code> -&gt; <code>BDUSS</code> -&gt; <code>内容</code> ，全选复制<br><img src="%E5%86%85%E5%AE%B9.png" alt="内容"></p></li><li><p>登录时，输入 <code>login -bduss=刚才复制的内容</code> 就可以登录啦！</p></li></ul><p>7.下载</p><pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">d 百度云上要下载的文件路径<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>比如说 我要下载我的大文件 <code>feats.npy</code> ，他在我百度云下的路径为<br><img src="%E8%B7%AF%E5%BE%84.png" alt="路径"><br>那么我要下载这个文件的操作就是 <code>d feats.npy</code></p><p>8.解决问题：<code>获取路径信息错误, 获取文件/目录的元信息: 遇到错误, 远端服务器返回错误, 代码: 31300, 消息: stream type is not authorized</code><br>执行 <code>BaiduPCS-Go config set -appid 266719</code> 即可</p><p>9.解决问题 <code>下载文件错误, 403 Forbidden, 重试 2/3</code></p><ul><li><p>创建baidu_shurufa 目录<br> 首先使用浏览器登陆百度网盘，登陆成功后访问以下网址：<code>http://pcs.baidu.com/rest/2.0/pcs/file?app_id=265486&amp;method=list&amp;path=%2F</code><br> 网页会显示如下类似内容<br> <code>{"error_code":31064,"error_msg":"file is not authorized","request_id":***************}</code><br> 然后进入百度网盘，进入<code>我的应用数据</code>目录，里面会出现一个名为 baidu_shurufa 的目录，该目录就是我们需要的目录，把需要下载的文件或文件夹移动到该目录</p></li><li><p>设置 appid<br>依次执行<code>cd /apps/baidu_shurufa</code> -&gt; <code>config set -appid=265486</code></p></li></ul><p>9.其他操作见 <code>0</code> 中的官方文档喔！</p>]]></content>
      
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> BaiduPCS-Goo </tag>
            
            <tag> 百度云 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>远程服务器使用教程</title>
      <link href="/2019/05/17/yuan-cheng-fu-wu-qi-shi-yong-jiao-cheng/"/>
      <url>/2019/05/17/yuan-cheng-fu-wu-qi-shi-yong-jiao-cheng/</url>
      
        <content type="html"><![CDATA[<p>服务器上面的配置因为我没有机会亲自配置过，所以这里只对连接和使用服务器的各种步骤进行学习和记录。<br>首先在服务器上搭建自己的环境，避免安装各种程序需要的包时发生冲突</p><h2 id="设置虚拟环境"><a href="#设置虚拟环境" class="headerlink" title="设置虚拟环境"></a>设置虚拟环境</h2><p>这里可以选择python的版本等，比如说我需要的是python3.5版本的环境，设置步骤如下：</p><ul><li><p>首先去确定服务器上面存在的python版本，在终端运行 <code>cd /usr/bin</code> -&gt; 输入 <code>ls</code> -&gt; 查看是否存在需要的pyhon版本，如果存在，进行下一步，如果不存在，自行百度啦~</p></li><li><p>创建 <code>virtualenv -p python35 py35</code></p></li><li><p>激活 <code>source py35/bin/activate</code></p></li><li><p>安装包 比如 <code>pip3 install -i https://pypi.mirrors.ustc.edu.cn/simple tensorflow==1.10.0</code>,<br>直接 <code>pip install + 包名</code> 也是可以的 但是会很慢，这里的 <code>https://pypi.mirrors.ustc.edu.cn/simple</code>是使用了国内的镜像，这里列一下国内的一些源：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">清华：https://pypi.tuna.tsinghua.edu.cn/simple阿里云：http://mirrors.aliyun.com/pypi/simple中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple华中理工大学：http://pypi.hustunique.com山东理工大学：http://pypi.sdutlinux.org豆瓣：http://pypi.douban.com/simplenote：新版ubuntu要求使用https源，要注意。使用时直接在命令行输入 <span class="token string">'pip -i 源地址'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>退出 <code>deactivate</code></p></li></ul><h2 id="Pycharm-2018连接服务器远程调试"><a href="#Pycharm-2018连接服务器远程调试" class="headerlink" title="Pycharm 2018连接服务器远程调试"></a>Pycharm 2018连接服务器远程调试</h2><h3 id="连接服务器"><a href="#连接服务器" class="headerlink" title="连接服务器"></a>连接服务器</h3><ul><li><p>打开pycharm2018</p></li><li><p>工具栏找到 <code>Tools--&gt;Deplcyment--&gt;Configuration</code><br><img src="%E5%B7%A5%E5%85%B7%E6%A0%8F.png" alt="工具栏"></p></li><li><p>点击左上角 <code>+</code> 号添加一个新的配置，选择 <code>SFTP</code><br><img src="sftp.png" alt="SFTP"></p></li><li><p>给新的配置命名<br><img src="%E5%91%BD%E5%90%8D.png" alt="命名"></p></li><li><p>设置 <code>Connection</code> 标签<br><img src="%E8%AE%BE%E7%BD%AE.png" alt="设置"><br>下拉 <code>Advanced</code>，将 <code>Encoding for client-server communication</code> 设置为 <code>UTF-8</code><br><img src="%E8%A1%A5%E5%85%85.png" alt="补充"></p><p> <strong>问题： 这里的 <code>Private key path</code>定位到私钥所在的位置，但我在配置中出现了问题，在这贴一下，如下图，这里我们需要为私钥转换一个格式</strong><br> <img src="%E7%A7%81%E9%92%A5%E9%97%AE%E9%A2%98.png" alt="私钥问题"><br><strong>解决方案：</strong></p><ul><li>贴一软件的<a href="https://pan.baidu.com/s/1MHNAJYqpKteGtbyHZMvIEA">百度云地址</a></li><li>打开软件之后，如下图，选择 <code>Load</code><br><img src="%E7%94%9F%E6%88%90.png" alt="生成"></li><li>选择私钥所在的文件夹，比如说我的是<code>C:\Users\Administrator\.ssh</code> -&gt; 选择 `All File -&gt; 选定 ‘id_rsa’<br><img src="%E9%80%89%E6%8B%A9%E6%96%87%E4%BB%B6%E5%A4%B9.png" alt="选择文件夹"></li><li>输入私钥的密码之后，选择 <code>Load</code>下面的<code>Save private key</code>为新的私钥命名保存后，重新选定就可以啦</li></ul></li><li><p>设置 <code>Mappings</code> 标签<br><img src="mapping.png" alt="mapping"></p></li><li><p>设置完成后，点下面的 <code>ok</code> 即可。</p></li></ul><h3 id="配置远程解释器"><a href="#配置远程解释器" class="headerlink" title="配置远程解释器"></a>配置远程解释器</h3><ul><li>打开 <code>File -&gt; Settings</code><br><img src="setting.png" alt="setting"></li><li>选择 <code>Project Interpreter</code> 下拉弹窗选择 <code>Show all</code><br><img src="showall.png" alt="showall"></li><li>创建新的环境，选择 <code>+</code> 号<br><img src="%E5%8A%A0.png" alt="加"></li><li>选择 <code>SSH Interpreter</code> -&gt; 右侧选择 <code>Existing server configuration</code> -&gt; 选择刚才创建的服务器<br><img src="ssh.png" alt="ssh"></li><li>选择 <code>Move</code><br><img src="move.png" alt="move"></li><li>点击 <code>Next</code></li><li>按照自己配置的路径修改如下内容<br><img src="aa.png" alt="aa"><br>其中 <code>Sync folder</code> 需要配置两个路径，如下<br><img src="%E8%B7%AF%E5%BE%84.png" alt="路径"></li><li>配置完毕后，选择 <code>Finish</code>，等待完成。</li></ul><h2 id="使用服务器跑本地代码"><a href="#使用服务器跑本地代码" class="headerlink" title="使用服务器跑本地代码"></a>使用服务器跑本地代码</h2><ul><li>连接服务器<br><img src="%E8%BF%9E%E6%8E%A5.png" alt="连接"></li><li>连接成功后，会在下面的窗口弹出下图窗口，首先我们要激活环境<br><img src="%E6%BF%80%E6%B4%BB%E7%8E%AF%E5%A2%83.png" alt="激活环境"></li><li>打开要运行的代码的文件，比如说 <code>cd/home/jyl/show_attention_tell</code></li><li>输入 <code>python+要运行的代码</code>，比如 <code>python imagecaption.py</code>，就可以在服务器上跑代码啦~</li></ul><h2 id="另外"><a href="#另外" class="headerlink" title="另外"></a>另外</h2><ul><li>正常情况下，按照上述教程，服务器会自动同步本地文件夹下面所有的文件到服务器上面指定的路径下的，但是万一出现什么意外我们也可以用 <code>scp</code> 传输文件。具体指令如下 <code>scp -r 本地文件路径 服务器登录名@服务器地址:服务器文件路径</code><br>比如说：<code>scp -r C:\Users\Administrator\Desktop\show_attention_tell jyl@xidianqa.com:/home/jyl</code></li></ul><p>这是目前我所有到的所有配置的相关教程，后期如果有什么更牛皮的操作再来补充~<br>参考：<br><a href="https://blog.csdn.net/Aerry_ale/article/details/81566832">Pycharm 2018连接服务器远程调试</a><br><a href="https://blog.csdn.net/qingche456/article/details/65465760">在ubuntu中使用virtualenv创建python2和python3的虚拟环境</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 服务器 </tag>
            
            <tag> 虚拟环境 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ubuntu16.04+python+cpu caffe编译全过程</title>
      <link href="/2019/04/22/ubuntu16-4-python-cpu-caffe-bian-yi-quan-guo-cheng/"/>
      <url>/2019/04/22/ubuntu16-4-python-cpu-caffe-bian-yi-quan-guo-cheng/</url>
      
        <content type="html"><![CDATA[<p>系统配置</p><pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">ubuntu16.04python v3.5.2<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>继上次在win10下安装了caffe后，因为没有服务器的原因又重新尝试在ubuntu下进行编译，结果发现caffe不只是在win10系统下不友好，在ubuntu环境下也不是非常有友好，在编译通过之后，含泪写下此文，引以为戒.<br>但是网上很多人说每个linux的环境都会不同，所以不一定有效…不一定..有效…hhh</p><p>因为第一次详细接触ubuntu，很多概念或描述并不是很准确，还望包涵指正</p><p>开始~</p><h2 id="注意！！系统最好先不要安装anaconda！！"><a href="#注意！！系统最好先不要安装anaconda！！" class="headerlink" title="注意！！系统最好先不要安装anaconda！！"></a>注意！！系统最好先不要安装anaconda！！</h2><p> 至于是不是因为这个原因我也不太清楚，但是确实很多资料显示在安装anaconda之后会出现各种麻烦的情况，在我第一天编译的时候也是因为安装了anaconda导致各种形形色色的bug。所以如果是新手还是选择先将anaconda卸载之后在进行下一步。<br> 卸载完全完全不会影响之后的安装的！</p><ul><li><p>首先打开anaconda的安装路径<br> 比如我的就是 <code>cd /home/jiyali</code></p></li><li><p>执行指令 <code>sudo rm -rf anaconda3</code></p></li><li><p>到根目录下，打开终端并输入 <code>sudo gedit ~/.bashrc</code></p></li><li><p>拉倒最底下，将类似于如下代码全部删掉(网上很多教程是注释掉<code>\export PATH="/home/vision/Softwares/Anaconda3/bin:$PATH"</code>，但是打开命令行总会提醒<code>行130:未预期的符号'fi'附近有语法错误</code>，鉴于强迫症就给全删掉了~)</p> <pre class="line-numbers language-dash" data-language="dash"><code class="language-dash"># added by Anaconda3 5.3.1 installer# &gt;&gt;&gt; conda init &gt;&gt;&gt;# !! Contents within this block are managed by 'conda init' !!__conda_setup="$(CONDA_REPORT_ERRORS=false '/home/vision/Softwares/Anaconda3/bin/conda' shell.bash hook 2&gt; /dev/null)"if [ $? -eq 0 ]; then    \eval "$__conda_setup" else     if [ -f "/home/vision/Softwares/Anaconda3/etc/profile.d/conda.sh" ]; then         . "/home/vision/Softwares/Anaconda3/etc/profile.d/conda.sh"         CONDA_CHANGEPS1=false conda activate base     else         \export PATH="/home/vision/Softwares/Anaconda3/bin:$PATH"     fi fi unset __conda_setup # &lt;&lt;&lt; conda init &lt;&lt;&lt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p> 删掉后保存并关闭文件</p></li><li><p>在终端运行<code>source ~/.bashrc</code></p></li></ul><h2 id="python-版本"><a href="#python-版本" class="headerlink" title="python 版本"></a>python 版本</h2><ul><li><p>首先确定系统中是否存在python3.5版本，在终端运行 <code>cd /usr/bin</code> -&gt; <code>ls</code>，去查看是否存在python3.5</p></li><li><p>如果存在，尝试建立软连接操作如下</p>  <pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">sudo rm pythonsudo ln -s python3 python<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li><p>查看python版本是否发生改变，命令行输入 <code>python</code></p></li></ul><h2 id="下载caffe"><a href="#下载caffe" class="headerlink" title="下载caffe"></a>下载caffe</h2><p> 命令行执行如下命令</p> <pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">sudo apt-get install gitgit clone http://github.com/BVLC/caffe.git<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="关于libboost1-55"><a href="#关于libboost1-55" class="headerlink" title="关于libboost1.55"></a>关于libboost1.55</h2><p> caffe想用python3.5，有libboost&gt;=1.5的要求，依次执行如下命令</p> <pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">sudo apt-get install libboost1.55-dev #一定要安装的sudo apt-get install libboost1.55-all-dev #不一定要成功，但是一定要执行<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="各种python的依赖包"><a href="#各种python的依赖包" class="headerlink" title="各种python的依赖包"></a>各种python的依赖包</h2><p> 在网上大部分的教程中，这一不会提到执行<code>for req in $(cat requirement.txt);do sudo pip3 install $req;done</code>命令，但是<strong>这是针对python2.7的，按照这些教程编译一定会出现各种问题</strong>，所以我们要用 <code>pip3</code> 来执行，步骤如下：</p><ul><li>首先安装 <code>pip3</code>，执行 <code>sudo apt-get install python3-pip</code>，安装成功后，再输入 <code>pip3 --version</code> 确认安装成功</li><li>进入 <code>caffe/python</code> (比如我的是在命令端输入 <code>cd /home/jiyali/caffe/python</code>)</li><li>执行 <code>for req in $(cat requirements.txt);do sudo pip3 install $req;done</code></li><li>安装结束后退到caffe根目录，查看是否安装成功，执行 <code>sudo pip3 install -r python/requirements.txt</code></li><li>** 如果其中发生任何错误，自行百度解决，我在安装过程中就出现了错误，后来好像是版本问题，因为过程之中没来得及记录，所以在这不再提啦~但是必须保证所有的包安装成功 ***</li></ul><h2 id="numpy安装"><a href="#numpy安装" class="headerlink" title="numpy安装"></a>numpy安装</h2><p> 执行 <code>sudo apt-get install python-numpy</code></p><h2 id="protobuf安装"><a href="#protobuf安装" class="headerlink" title="protobuf安装"></a>protobuf安装</h2><ul><li><p>首先查看当前版本，执行 <code>protoc --version</code> ，如果显示是 <code>3.x</code> 版本应该是没问题，这里不建议版本太高</p></li><li><p>如果不存在，执行以下指令</p> <pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">mkdir protobuf &amp;&amp; cd protobufmkdir cpp &amp;&amp; cd cppwget https://github.com/google/protobuf/releases/download/v3.0.0/protobuf-cpp-3.0.0.tar.gztar xvf protobuf-cpp-3.0.0.tar.gz &amp;&amp; cd protobuf-3.0.0./configure &amp;&amp; makemake checksudo make installsudo ldconfigcd ../..mkdir python &amp;&amp; cd pythonwget https://github.com/google/protobuf/releases/download/v3.0.0/protobuf-python-3.0.0.tar.gztar xvf protobuf-python-3.0.0.tar.gz &amp;&amp; cd protobuf-3.0.0/python/python setup.py buildpython setup.py testpython setup.py install<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>检查当前版本 <code>protoc --version</code> -&gt; <code>sudo protoc --version</code> ，普通用户和root账户最好版本相同</p></li></ul><h2 id="修改-Makefile-config"><a href="#修改-Makefile-config" class="headerlink" title="修改 Makefile.config"></a>修改 Makefile.config</h2><ul><li><p>首先复制一个副本，执行 <code>cp Makefile.config.example Makefile.config</code></p></li><li><p>执行 <code>sudo gedit Makefile.config</code></p></li><li><p>查找numpy的安装目录，执行 <code>pip show numpy</code><br> <img src="%E6%9F%A5%E6%89%BEnumpy.png" alt="查找numpy"></p></li><li><p>将8行改为 <code>CPU_ONLY := 1</code></p></li><li><p>查找系统中的 <code>boost_python</code> ，在命令行输入 <code>local boost python</code><br><img src="%E6%9F%A5%E6%89%BEboost.png" alt="查找"></p></li><li><p>将81.82行修改为</p> <pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">PYTHON_LIBRARIES := boost_python-py35 python3.5mPYTHON_INCLUDE := /usr/include/python3.5m \                  /home/jiyali/.local/lib/python3.5/site-packages/numpy/core/include #前半部分是numpy的安装路径<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>*** 这些文件目录最好自己亲自检查是不是在这个文件夹下后再修改 ***</p></li><li><p>将94行改为 <code>WITH_PYTHON_LAYER := 1</code></p></li><li><p>将97.98行改为</p><pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial/LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib/x86_64-linux-gnu/hdf5/serial<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ul><p><img src="%E9%85%8D%E7%BD%AEmakefileconfig.png" alt="配置makefileconfig"></p><h2 id="编译caffe"><a href="#编译caffe" class="headerlink" title="编译caffe"></a>编译caffe</h2><p> *** 如果是第一次编译caffe，没问题，按照下面步骤来就可以了 ***</p><ul><li><p>依次输入以下指令</p>  <pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">sudo make cleansudo make allsudo make testsudo make runtestsudo make pycaffe<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>上面都编译通过后，再执行 <code>sudo make pytest</code></p></li><li><p>中间出现的问题，因为我修改了超多所以我把能记得的有效的办法贴一下~</p></li></ul><h2 id="pycaffe添加环境变量"><a href="#pycaffe添加环境变量" class="headerlink" title="pycaffe添加环境变量"></a>pycaffe添加环境变量</h2><ul><li>运行 <code>vim ~/.bashrc</code></li><li>将 <code>export PYTHONPATH=/home/wanghh/caffe/python:$PYTHONPATH</code> 加到文件中，保存退出</li><li>运行 <code>source ~/.bashrc</code> ，使修改生效</li></ul><h2 id="编译中出现的问题解决"><a href="#编译中出现的问题解决" class="headerlink" title="编译中出现的问题解决"></a>编译中出现的问题解决</h2><ul><li>之间大部分错误是因为我没有使用 <code>sudo</code> ，这个具体怎么个情况我也不太清楚</li><li>错误(make:Nothing to be done for ‘pycaffe’)<br> 解决:执行  <code>sudo make clean</code></li><li>中间编译的过程中出现了很多错误，一般都是 <code>No Module Named:xxx</code>，要么是模块没有安装，要么是安装了不支持Python3。<br> 解决：缺啥安啥，执行  <code>sudo pip3 install XXX</code></li><li>错误： Makefile:554:recipe for target’.build_release/lib/libcaffe.so.1.0.0-rc3’failed<br> 解决： 修改 <code>Makefile</code> 文件 ，将181行改为 <code>LIBRARIES += glog gflags protobuf boost_system boost_filesystem m hdf5_serial_hl hdf5_serial</code></li><li>错误:</li></ul><pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">Makefile:546:recipe for target 'pytest' failedmake:***[pytest] Error 1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>  解决：运行 <code>sudo pip3 install matplotlib --upgrade</code></p><p>参考：<br><a href="https://blog.csdn.net/qwrqwrqwer/article/details/79224402">（ubuntu16.04）关于caffe+python3.5（非anaconda安装）编译的一些问题</a><br><a href="https://github.com/BVLC/caffe/issues">官方github中的issues</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> ubuntu16.04 </tag>
            
            <tag> caffe </tag>
            
            <tag> cpu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>win10+cpu编译cafffe血泪全过程</title>
      <link href="/2019/04/16/win10-an-zhuang-cafffe-quan-guo-cheng/"/>
      <url>/2019/04/16/win10-an-zhuang-cafffe-quan-guo-cheng/</url>
      
        <content type="html"><![CDATA[<p><img src="%E5%93%AD.jpg" alt="哭"></p><span id="more"></span><p>系统配置</p><pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">win10+64位Visual Studio2015python v3.5.2Anaconda3-2019.03-Windows-x86_64cmake version 3.14.2<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="软件准备"><a href="#软件准备" class="headerlink" title="软件准备"></a>软件准备</h2><h3 id="Visual-Studio2015"><a href="#Visual-Studio2015" class="headerlink" title="Visual Studio2015"></a>Visual Studio2015</h3><h4 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h4><p>  贴一个<a href="https://visualstudio.microsoft.com/zh-hans/vs/older-downloads/">Visual Studio官方下载地址</a>，或者直接点击<a href="https://pan.baidu.com/s/1foPolGTY4VonKg9gAyRx0g">百度网盘下载</a>，提取码：z4u6</p><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><ul><li>VS2015 下载完成后会得到一个镜像文件(.iso 文件)，双击打开，运行 <code>vs_professional.exe</code> 进入安装程序。</li><li>选择安装位置以及安装方式<br> 安装位置可以自己选择，但是安装方式需要选择 <code>自定义</code><br> <img src="%E8%87%AA%E5%AE%9A%E4%B9%89.png" alt="自定义"><br> 点击 <code>下一步</code></li><li>选择要安装的组件<br> 配置caffe的时候需要用到 <code>vcvarsall.bat</code> 文件，所以我们需要选择安装 <code>Visual C++</code><br> <strong>(后期我debug的时候又补充安装了下面的 <code>针对Visual Studio 的 Python 工具(2017年1月)</code> 不知道是否有用，大家还是勾选吧)</strong><br> <img src="%E9%80%89%E6%8B%A9C.png" alt="选择组件"></li><li>剩下的就一路默认就可以啦，等待安装完成</li><li>安装完成后点击 <code>启动</code></li><li>点击 <code>以后再说</code><br> <img src="%E4%BB%A5%E5%90%8E%E5%86%8D%E8%AF%B4.png" alt="以后再说"></li><li>选择主题后，点击 <code>启动Visual Studio</code></li></ul><h4 id="激活"><a href="#激活" class="headerlink" title="激活"></a>激活</h4><ul><li><p>点击菜单栏的 <code>帮助</code> ，点击 <code>注册产品</code><br> <img src="%E6%B3%A8%E5%86%8C%E4%BA%A7%E5%93%81.png" alt="注册产品"></p></li><li><p>点击 <code>使用产品密钥解锁</code><br> <img src="%E4%BD%BF%E7%94%A8%E4%BA%A7%E5%93%81%E5%AF%86%E9%92%A5.png" alt="使用产品密钥"></p></li><li><p>复制下列激活码：(网上找的)后，点击 <code>应用</code></p> <pre class="line-numbers language-dsah" data-language="dsah"><code class="language-dsah">KEY：HMGNV-WCYXV-X7G9W-YCX63-B98R2<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p> <img src="%E6%B3%A8%E5%86%8C%E7%A0%81.png" alt="注册码"></p></li><li><p>注册成功<br> <img src="%E6%BF%80%E6%B4%BB%E6%88%90%E5%8A%9F.png" alt="激活成功"></p></li></ul><h3 id="python3-5"><a href="#python3-5" class="headerlink" title="python3.5"></a>python3.5</h3><h4 id="python3-5下载"><a href="#python3-5下载" class="headerlink" title="python3.5下载"></a>python3.5下载</h4><ul><li>附一<a href="https://www.python.org/">python官网(https://www.python.org/)</a>，打开后选择 <code>Downloads</code> ，选择合适系统，比如 <code>Windows</code><br> <img src="python%E5%AE%98%E7%BD%91.png" alt="python官网"></li><li>选择要下载的python版本，点击进入下拉找到 <code>Files</code></li><li>选择合适系统的安装包，点击下载<br> <img src="%E5%AE%89%E8%A3%85%E5%8C%85%E4%B8%8B%E8%BD%BD.png" alt="安装包下载"></li></ul><h4 id="python3-5安装"><a href="#python3-5安装" class="headerlink" title="python3.5安装"></a>python3.5安装</h4><ul><li>双击打开下载好的安装包 <code>python-3.5.2-amd64.exe</code> ，选择 <code>Customize installation</code> ，勾选 <code>Add Python 3.5 to PATH</code> ，下一步<br><img src="python%E5%AE%89%E8%A3%85.png" alt="python安装"></li><li>默认不作更改，下一步<br><img src="2.png" alt="2"></li><li>在 <code>Customize install location</code> 自定义选择安装路径后，点 <code>install</code><br><img src="3.png" alt="3"></li><li>等待安装完成<br><img src="4.png" alt="4"></li></ul><h3 id="anaconda"><a href="#anaconda" class="headerlink" title="anaconda"></a>anaconda</h3><h4 id="anaconda下载"><a href="#anaconda下载" class="headerlink" title="anaconda下载"></a>anaconda下载</h4><ul><li>贴<a href="https://www.anaconda.com/">anaconda官网(https://www.anaconda.com/)</a>，点击右上角的 <code>Download</code></li><li>下拉选择 <code>Windows</code> ，继续选择 <code>Python3.7 version</code> ，选择核实系统的安装包进行下载，比如我的电脑是64位系统，选择 <code>64-Bit Graphical Installer (662 MB)</code><br><img src="anaconda%E4%B8%8B%E8%BD%BD.png" alt="anaconda下载"></li></ul><h4 id="anaconda安装"><a href="#anaconda安装" class="headerlink" title="anaconda安装"></a>anaconda安装</h4><ul><li>双击下载好的安装包 <code>Anaconda3-2019.03-Windows-x86_64.exe</code> ，点击 <code>next</code><br><img src="a%E5%AE%89%E8%A3%85.png" alt="anaconda安装"></li><li>点击 <code>I Agree</code> ，下一步<br><img src="agree.png" alt="agree"></li><li>选择 <code>All Users</code> ，下一步<br><img src="%E7%94%A8%E6%88%B7.png" alt="用户"></li><li>自定义安装位置<br><img src="a%E4%BD%8D%E7%BD%AE.png" alt="位置"></li><li>勾选环境变量<br><img src="a%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F.png" alt="环境变量"></li><li>等待安装完成即可</li></ul><h4 id="配置pyhton3-5环境"><a href="#配置pyhton3-5环境" class="headerlink" title="配置pyhton3.5环境"></a>配置pyhton3.5环境</h4><p> 我们下载好的anaconda默认是python3.7环境的，现在我们需要更改一下python3.5的环境</p><ul><li>打开 <code>开始</code> 菜单 -&gt; 找到 <code>Anaconda Powershell Prompt</code> -&gt; 输入命令 <code>conda create --name py35 python=3.5</code></li><li>安装中间会出现 <code>Proceed([y]/n)?</code> -&gt; 输入 <code>y</code> -&gt; 回车</li><li>完成安装</li></ul><h3 id="cmake"><a href="#cmake" class="headerlink" title="cmake"></a>cmake</h3><h4 id="cmake下载"><a href="#cmake下载" class="headerlink" title="cmake下载"></a>cmake下载</h4><p>   贴<a href="https://cmake.org/download/">cmake官网(https://cmake.org/download/)</a><br>   <img src="cmake%E5%AE%98%E7%BD%91.png" alt="cmake官网"><br>   选择合适版本的安装包进行下载，比如说我下载的是 <code>cmake-3.14.2-win64-x64.zip</code></p><h4 id="cmake安装"><a href="#cmake安装" class="headerlink" title="cmake安装"></a>cmake安装</h4><ul><li>找一合适的文件夹解压 <code>cmake-3.14.2-win64-x64.zip</code></li><li>打开解压后的文件夹 <code>cmake-3.14.2-win64-x64</code> -&gt; <code>bin</code></li><li>复制文件地址，比如我的是 <code>F:\Program Files\cmake-3.14.2-win64-x64\bin</code></li><li>将上述地址添加到环境变量<ul><li>同时按 <code>WIN</code> + <code>R</code> 键，打开 <code>运行</code> 对话框，输入 <code>sysdm.cpl</code> ，按回车键打开 <code>系统属性</code><br> <img src="%E7%B3%BB%E7%BB%9F%E8%AE%BE%E7%BD%AE.png" alt="系统设置"></li><li>在系统属性对话框中选择 <code>高级</code> 选项卡<br> <img src="%E9%AB%98%E7%BA%A7%E9%80%89%E9%A1%B9%E5%8D%A1.png" alt="高级选项卡"></li><li>选择 <code>Path</code> ，点击 <code>编辑</code><br> <img src="%E7%BC%96%E8%BE%91.png" alt="编辑"></li><li>选择 <code>新建</code> ，将刚才复制过的地址复制过来就可以啦<br> <img src="%E6%96%B0%E5%BB%BA.png" alt="新建"></li><li>记得全部点保存后再退出</li></ul></li><li>打开cmd，输入 <code>cmake --version</code> ，得到 <code>cmake version 3.14.2</code> 后，说明安装成功~</li></ul><h2 id="caffe的部分依赖包"><a href="#caffe的部分依赖包" class="headerlink" title="caffe的部分依赖包"></a>caffe的部分依赖包</h2><h3 id="依赖包下载"><a href="#依赖包下载" class="headerlink" title="依赖包下载"></a>依赖包下载</h3><p>   暂时我就只能贴一<a href="https://pan.baidu.com/s/11d0osj5CPWbbw07Vg5XKBA">百度云下载链接啦</a>，提取码：43mq</p><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><ul><li>在C盘目录下打开 <code>用户</code> -&gt; 打开 <code>Administrator</code> -&gt; 打开<code>cmd</code>，输入<code>mkdir .caffe</code> 创建一个名为 <code>.caffe</code> 的文件 -&gt; 打开 <code>.caffe</code> 后，新建文件夹 <code>dependencies</code> -&gt; 打开后，新建文件夹 <code>download</code></li><li>将上面下载的 <code>libraries_v140_x64_py35_1.1.0.tar</code> 文件拷贝到刚才新建的文件夹 <code>C:\Users\Administrator\.caffe\dependencies\download</code> 下面</li></ul><h2 id="caffe-windows"><a href="#caffe-windows" class="headerlink" title="caffe-windows"></a>caffe-windows</h2><h3 id="caffe下载"><a href="#caffe下载" class="headerlink" title="caffe下载"></a>caffe下载</h3><ul><li><p>贴上<a href="https://github.com/BVLC/caffe/tree/windows">caffe官方github(https://github.com/BVLC/caffe/tree/windows)</a>，可以直接下载</p></li><li><p>或者在想要安装的文件夹下，右键 <code>Git Bash Here</code> -&gt; 依次输入：</p> <pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">git clone https://github.com/BVLC/caffe.gitcd caffegit checkout windows<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ul><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><ul><li><p>修改 <code>build_win.cmd</code></p><pre><code>* 打开 `caffe` -&gt; `scripts` -&gt; 用`notepad++` 打开 `build_win.cmd`* 修改如下内容</code></pre> <pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">MSVC_VERSION=14    //选用VS2015编译WITH_NINJA=0       //不用NinjaCPU_ONLY=1         //用cuda加速就设置为0，不用就设置为1PYTHON_VERSION=3  //我用的是python3.5，所以设置为3<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>   <strong>注：以上修改内容最好使用 <code>搜索</code> ，将文件中全部上述内容的进行修改，我之前安装失败就是因为修改时只改了一部分，没有修改全部(不出问题是有两处)</strong><br>   <img src="%E4%BF%AE%E6%94%B9%E8%AE%BE%E7%BD%AE.png" alt="修改设置"><br>  <strong>( 另外我还改了另一个地方，搜索 <code>set CONDA_ROOT</code> ，将 <code>=</code> 后面改为 <code>Anaconda3</code> 的地址，比如说我的是 <code>F:\ProgramData\Anaconda3</code> ，这个部分我不确定是不是必须要改，网上说法不一，反正我改掉啦~ )</strong><br>  <img src="%E5%9C%B0%E5%9D%80.png" alt="地址"></p></li><li><p>修改 <code>WindowsDownloadPrebuiltDependencies.cmake</code></p><ul><li>打开 <code>caffe</code> -&gt; <code>cmake</code> -&gt; 用<code>notepad++</code> 打开 <code>WindowsDownloadPrebuiltDependencies.cmake</code></li><li>搜索 <code>if(NOT DEFINED DEPENDENCIES_URL_${MSVC_VERSION}_${_pyver})</code></li><li>在 <code>if(NOT DEFINED DEPENDENCIES_URL_${MSVC_VERSION}_${_pyver})</code> 上面加一行代码：</li></ul> <pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">set(MSVC_VERSION 1900)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>![加一行代码](加一行代码.png)</code></pre><ul><li>保存退出</li></ul></li></ul><h3 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h3><ul><li><p>切换环境<br> <strong>切记切记！！一定要在编译前切换python的环境！！！</strong><br> 打开 <code>cmd</code> -&gt; 输入 <code>activate py35</code> -&gt; 输入 <code>f</code> -&gt; 输入 <code>cd F:\caffe\scripts</code><br> <img src="%E6%89%93%E5%BC%80.png" alt="打开"></p></li><li><p>进入正题<br> 输入 <code>build_win.cmd</code><br> <img src="%E7%BC%96%E8%AF%91.png" alt="编译"></p></li><li><p>长篇大论之后，大概会出现下面的界面(这一部分我最终的结果和网上不一样，不知道是否编译成功)<br><img src="%E6%9C%80%E7%BB%88.png" alt="最终"></p></li></ul><h3 id="曾出现的Bug记录"><a href="#曾出现的Bug记录" class="headerlink" title="曾出现的Bug记录"></a>曾出现的Bug记录</h3><p> 应该是我太菜鸡，安装过程中Bug层出不穷，前面基本上已经把我之前失误的地方都改过了，我就再说一个比较顽固的bug吧</p><ul><li>之前出现<code>The dependency target "pycaffe" of target "pytest" does not exist</code>时，搜索了很多类似的问题都没啥解决办法。后来发现可能是环境有问题，环境中没有安装 <code>numpy</code>, 所以为了避免这个问题出现，我们可以在编译之前，<code>cmd</code> 中输入 <code>python</code> ，加载成功之后 输入 <code>import numpy</code> ，出现下面的界面时，说明我们的环境中有 <code>numpy</code> ，不会出现上述问题</li></ul><p> <img src="numpy.png" alt="numpy"></p><ul><li><strong>切记，不论出现任何问题，都要把 <code>:\caffe\scripts</code> 目录下的 <code>build</code> 文件彻底删掉才能进行下一轮编译</strong></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> win10 </tag>
            
            <tag> caffe </tag>
            
            <tag> cpu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title> hexo+next 部署各种炫酷博客特效</title>
      <link href="/2019/04/15/hexo-next-bu-shu-ge-chong-xuan-ku-bo-ke-te-xiao/"/>
      <url>/2019/04/15/hexo-next-bu-shu-ge-chong-xuan-ku-bo-ke-te-xiao/</url>
      
        <content type="html"><![CDATA[<h2 id="在右上角或者左上角实现fork-me-on-github"><a href="#在右上角或者左上角实现fork-me-on-github" class="headerlink" title="在右上角或者左上角实现fork me on github"></a>在右上角或者左上角实现fork me on github</h2><p>效果图：<br><img src="forkme.png" alt="forkme效果图"></p><p>实现方法：<br>戳<a href="https://github.blog/2008-12-19-github-ribbons/">这个链接</a>或<a href="http://tholman.com/github-corners/">这个链接</a>，选择喜欢的图标样式，并复制代码，下面图片红框中右侧的代码：<br><img src="code.png" alt="code"></p><p>打开博客主目录文件 -&gt; <code>themes</code> -&gt; <code>next</code> -&gt;  <code>layout</code> -&gt; <code>_layout.swig</code> -&gt; 搜索 <code>class="headband"</code> -&gt; 将刚才复制的代码粘贴在下面 -&gt; 并把 <code>href</code> 改为你的github地址<br><img src="%E4%BF%AE%E6%94%B9%E5%9C%B0%E5%9D%80.png" alt="修改地址"></p><h2 id="添加RSS"><a href="#添加RSS" class="headerlink" title="添加RSS"></a>添加RSS</h2><p>效果图：<br><img src="RSS.png" alt="RSS"></p><p>实现方法：<br>找到博客主目录文件 -&gt; 右键 <code>Git Bash Here</code> -&gt; 执行 <code>npm install --save hexo-generator-feed</code> -&gt; 安装结束后，打开博客主目录文件 -&gt; 打开 <code>_config.yml</code> -&gt; 在末尾添加代码：</p><pre class="line-numbers language-dash" data-language="dash"><code class="language-dash"># Extensions## Plugins: http://hexo.io/plugins/plugins: hexo-generate-feed<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>再打开 <code>themes</code> -&gt; <code>next</code> -&gt;  <code>_config.yml</code> -&gt; 搜索 <code>rss</code> -&gt; 在 <code>rss:</code> 后面添加 <code>/atom.xml</code>，,注意在冒号后面要加一个空格<br><img src="%E6%B7%BB%E5%8A%A0rss.png" alt="添加rss"></p><h2 id="侧边栏社交小图标设置"><a href="#侧边栏社交小图标设置" class="headerlink" title="侧边栏社交小图标设置"></a>侧边栏社交小图标设置</h2><p>效果图：<br><img src="%E7%A4%BE%E4%BA%A4.png" alt="社交"></p><p>实现：<br>打开博客主目录文件 -&gt; <code>themes</code> -&gt; <code>next</code>  -&gt; <code>_config.yml</code> -&gt; 搜索 <code>social</code> -&gt; 删掉想要展示的社交账号前面的 <code>#</code> 号，将地址改为自己的地址 -&gt; 找到下面的 <code>social_icons:</code> -&gt; 将 <code>enable</code> 设置为 <code>true</code> -&gt; 在<a href="https://fontawesome.com/v4.7.0/icons/">图标库</a>找自己喜欢的小图标，并将名字复制在如下位置，保存即可<br><img src="%E9%85%8D%E7%BD%AE.png" alt="配置"></p><h2 id="主页文章添加阴影效果"><a href="#主页文章添加阴影效果" class="headerlink" title="主页文章添加阴影效果"></a>主页文章添加阴影效果</h2><p>效果图：<br><img src="%E9%98%B4%E5%BD%B1.png" alt="阴影"></p><p>实现：<br>打开博客主目录文件 -&gt; <code>themes</code> -&gt; <code>next</code>  -&gt; <code>source</code> -&gt; <code>css</code> -&gt; <code>_custom</code> -&gt; <code>custom.styl</code> -&gt; 在里面粘贴如下代码：</p><pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">.post {   margin-top: 60px;   margin-bottom: 60px;   padding: 25px;   -webkit-box-shadow: 0 0 5px rgba(202, 203, 203, .5);   -moz-box-shadow: 0 0 5px rgba(202, 203, 204, .5);  }<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="页面底部跳动的爱心"><a href="#页面底部跳动的爱心" class="headerlink" title="页面底部跳动的爱心"></a>页面底部跳动的爱心</h2><p>效果图：<br><img src="%E7%88%B1%E5%BF%83.png" alt="爱心"></p><p>实现：</p><p>首先找到<a href="https://fontawesome.com/v4.7.0/icons/">图标库</a>，如我现在选择下图中的爱心，那么我要复制的内容就是下图红框中的 <code>fa-heartbeat</code>。<br>打开博客主目录文件 -&gt; <code>themes</code> -&gt; <code>next</code>  -&gt; <code>_config.yml</code> -&gt; 搜索 <code>footer</code> -&gt; 将 <code>name</code> 后面的 <code>user</code> 替换成 <code>fa-heartbeat</code> -&gt; 将 <code>animated</code> 设置成 <code>true</code> -&gt; 将 <code>color</code> 修改为 <code>#ff0000</code><br><img src="%E8%AE%BE%E7%BD%AE.png" alt="设置"></p><h2 id="页面访问量统计"><a href="#页面访问量统计" class="headerlink" title="页面访问量统计"></a>页面访问量统计</h2><p>实现：<br>打开博客主目录文件 -&gt; <code>themes</code> -&gt; <code>next</code>  -&gt; <code>_config.yml</code> -&gt; 搜索 <code>busuanzi_count</code> -&gt; 将 <code>enable</code> 设置为 <code>true</code></p><h2 id="添加顶部加载条"><a href="#添加顶部加载条" class="headerlink" title="添加顶部加载条"></a>添加顶部加载条</h2><p>实现：<br>打开博客主目录文件 -&gt; <code>themes</code> -&gt; <code>next</code> -&gt;  <code>layout</code> -&gt; <code>_partials</code> -&gt; <code>head</code> -&gt; <code>head.swig</code> -&gt; 在第四行后面添加如下代码：</p><pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">&lt;script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"&gt;&lt;/script&gt;&lt;link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet"&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>打开博客主目录文件 -&gt; <code>themes</code> -&gt; <code>next</code>  -&gt; <code>_config.yml</code> -&gt; 搜索 <code>pace</code> -&gt; 将 <code>pace</code> 设置为 <code>true</code><br>默认的颜色是粉红色的。</p><p><img src="pace.png" alt="pace"></p><h2 id="隐藏网页底部powered-By-Hexo-强力驱动"><a href="#隐藏网页底部powered-By-Hexo-强力驱动" class="headerlink" title="隐藏网页底部powered By Hexo / 强力驱动"></a>隐藏网页底部powered By Hexo / 强力驱动</h2><p>实现：<br>打开博客主目录文件 -&gt; <code>themes</code> -&gt; <code>next</code> -&gt;  <code>layout</code> -&gt; <code>_partials</code> -&gt; <code>footer.swig</code> -&gt; 搜索 <code>class="powered-by"</code> -&gt; 使用 <code>&lt;!--   --&gt;</code> 隐藏下图所示代码<br><img src="%E9%9A%90%E8%97%8F.png" alt="隐藏"></p><h2 id="为博客添加萌宠"><a href="#为博客添加萌宠" class="headerlink" title="为博客添加萌宠"></a>为博客添加萌宠</h2><p>效果图：<br><img src="koharu.gif" alt="萌宠"></p><p>实现：<br>先给出hexo-helper-live2d 的 Github 链接：<a href="https://github.com/EYHN/hexo-helper-live2d">https://github.com/EYHN/hexo-helper-live2d</a></p><h3 id="安装并配置-hexo-helper-live2d"><a href="#安装并配置-hexo-helper-live2d" class="headerlink" title="安装并配置 hexo-helper-live2d"></a>安装并配置 hexo-helper-live2d</h3><h4 id="安装-hexo-helper-live2d"><a href="#安装-hexo-helper-live2d" class="headerlink" title="安装 hexo-helper-live2d"></a>安装 hexo-helper-live2d</h4><p>  找到博客主目录文件 -&gt; 右键 <code>Git Bash Here</code> -&gt; 执行 <code>npm install hexo-helper-live2d --save</code></p><h4 id="配置-hexo-helper-live2d"><a href="#配置-hexo-helper-live2d" class="headerlink" title="配置 hexo-helper-live2d"></a>配置 hexo-helper-live2d</h4><h5 id="挑选萌物"><a href="#挑选萌物" class="headerlink" title="挑选萌物"></a>挑选萌物</h5><p>   到<a href="https://huaji8.top/post/live2d-plugin-2.0/">插件作者博客</a>挑选你喜欢的萌物，记录下她的名字~</p><h5 id="安装萌物"><a href="#安装萌物" class="headerlink" title="安装萌物"></a>安装萌物</h5><p>   比如说我喜欢 <code>koharu</code>，安装 <code>koharu</code>的步骤：<br>   找到博客主目录文件 -&gt; 右键 <code>Git Bash Here</code> -&gt; 执行 <code>npm install live2d-widget-model-koharu --save</code></p><h5 id="配置萌物"><a href="#配置萌物" class="headerlink" title="配置萌物"></a>配置萌物</h5><p>   打开博客主目录文件 -&gt; <code>_config.yml</code> -&gt; 添加如下代码：</p>   <pre class="line-numbers language-dash" data-language="dash"><code class="language-dash"># Live2D## https://github.com/EYHN/hexo-helper-live2dlive2d:  enable: true  scriptFrom: local  pluginRootPath: live2dw/  pluginJsPath: lib/  pluginModelPath: assets/  tagMode: false  debug: false  model:     use: live2d-widget-model-koharu  display:     position: right     width: 150     height: 300  mobile:     show: true<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>执行 <code>hexo clean</code> -&gt;  <code>hexo g</code> -&gt;  <code>hexo s</code> ，就能看见我们的萌宠啦~</p><h2 id="浏览页面的时候显示当前浏览进度"><a href="#浏览页面的时候显示当前浏览进度" class="headerlink" title="浏览页面的时候显示当前浏览进度"></a>浏览页面的时候显示当前浏览进度</h2><p>效果：<br><img src="%E8%BF%9B%E5%BA%A6.png" alt="效果"></p><p>实现：<br>打开博客主目录文件 -&gt; <code>themes</code> -&gt; <code>next</code>  -&gt; <code>_config.yml</code> -&gt; 搜索 <code>scrollpercent</code> -&gt; 将 <code>scrollpercent</code> 设置为 <code>true</code><br><img src="%E5%BD%93%E5%89%8D%E8%BF%9B%E5%BA%A6.png" alt="当前进度"></p><p>参考：<br><a href="https://11.tt/posts/2018/how-to-play-with-live2d-on-hexo/">用Live2D让看板喵入住你的Hexo博客吧(^o^)/~</a><br><a href="http://shenzekun.cn/hexo%E7%9A%84next%E4%B8%BB%E9%A2%98%E4%B8%AA%E6%80%A7%E5%8C%96%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B.html">hexo的next主题个性化教程:打造炫酷网站</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> next </tag>
            
            <tag> 博客美化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用VSCode写博客ヾ(◍°∇°◍)ﾉﾞ</title>
      <link href="/2019/04/14/shi-yong-vscode-xie-bo-ke-deg-deg-no/"/>
      <url>/2019/04/14/shi-yong-vscode-xie-bo-ke-deg-deg-no/</url>
      
        <content type="html"><![CDATA[<h2 id="安装VSCode和markdownlint"><a href="#安装VSCode和markdownlint" class="headerlink" title="安装VSCode和markdownlint"></a>安装VSCode和markdownlint</h2><h3 id="安装VSCode"><a href="#安装VSCode" class="headerlink" title="安装VSCode"></a>安装VSCode</h3><p>   先贴一<a href="https://code.visualstudio.com/">Visual Studio Code官网(https://code.visualstudio.com/)</a><br>   打开是如下界面<br>   <img src="VS%E5%AE%98%E7%BD%91.png" alt="VSCode官网"><br>   点击红框下面的下拉箭头，选择适合系统的安装文件，点击下载，下载完成后运行 <code>VSCodeUserSetup-x64-1.33.1.exe</code><br>   如果没有什么特殊要求就一路默认安装即可。</p><h3 id="安装markdownlint"><a href="#安装markdownlint" class="headerlink" title="安装markdownlint"></a>安装markdownlint</h3><p>   打开VSCode，在下图所示位置输入 <code>markdownlint</code><br>   <img src="mark.png" alt="mark"><br>   点击进入，出现下述界面<br>   <img src="markdown.png" alt="markdown"><br>   点击红框中的 <code>install</code></p><p>安装结束~</p><h2 id="撰写新的文章"><a href="#撰写新的文章" class="headerlink" title="撰写新的文章"></a>撰写新的文章</h2><h3 id="新建一篇文章"><a href="#新建一篇文章" class="headerlink" title="新建一篇文章"></a>新建一篇文章</h3><p>找到博客主目录文件，右键选择 <code>Git Bash Here</code>，输入如下命令<code>hexo new post "文章的名字"</code></p><h3 id="找到文章路径"><a href="#找到文章路径" class="headerlink" title="找到文章路径"></a>找到文章路径</h3><p>打开 博客主目录文件 -&gt; <code>source</code> -&gt; <code>_posts</code> -&gt; <code>你文章的名字</code><br>右键选择 <code>打开方式</code> ，选择 <code>Visual Studio Code</code> ，打开页面如下：<br><img src="vscode.png" alt="vscode界面"></p><h3 id="markdown-的基本语法"><a href="#markdown-的基本语法" class="headerlink" title="markdown 的基本语法"></a>markdown 的基本语法</h3><p>markdown 的语法大部分都非常简单，一般来说，稍微看一看都能迅速写出文章。吹爆！至于绘制公式等复杂的用法，以后用到会来更新~</p><h4 id="标题语法"><a href="#标题语法" class="headerlink" title="标题语法"></a>标题语法</h4><p>标题语法是这样的：<br>   第一级标题，开头是一个<code>#</code><br>   第二级标题，开头是两个<code>##</code><br>   第三级标题，开头是两个<code>###</code><br>   ……<br>   请注意，标题需要独自占据一行，并且在标题的文字和 # 之间需要有一个空格，不然可能会不被识别。同时，markdown只支持六级标题，六级以上的标题不存在鸭~</p><pre class="line-numbers language-dash" data-language="dash"><code class="language-dash"># 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题###### 六级标题####### h7      // 错误代码######## h8     // 错误代码######### h9    // 错误代码########## h10  // 错误代码<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="字体语法"><a href="#字体语法" class="headerlink" title="字体语法"></a>字体语法</h4><h5 id="加粗"><a href="#加粗" class="headerlink" title="加粗"></a>加粗</h5><p>  要加粗的文字左右分别用两个*号包起来</p><p>  比如：<code>**加粗以后我就是最靓的仔**</code></p><p>  效果：<br>  <strong>加粗以后我就是最靓的仔</strong></p><h5 id="斜体"><a href="#斜体" class="headerlink" title="斜体"></a>斜体</h5><p>   要倾斜的文字左右分别用一个*号包起来</p><p>   比如：<code>*整条街就我最横*</code></p><p>   效果：<br>    <em>整条街就我最横</em></p><h5 id="斜体加粗"><a href="#斜体加粗" class="headerlink" title="斜体加粗"></a>斜体加粗</h5><p>  要倾斜和加粗的文字左右分别用三个*号包起来</p><p>  比如：<code>***我又靓又横我最牛批***</code></p><p>  效果：<br>   <em><strong>我又靓又横我最牛批</strong></em></p><h5 id="删除线"><a href="#删除线" class="headerlink" title="删除线"></a>删除线</h5><p>  要加删除线的文字左右分别用两个~~号包起来</p><p>  比如：<code>~~WTF!当我没说这句话~~</code></p><p>  效果：<br>   <del>WTF!当我没说这句话</del></p><h4 id="引用语法"><a href="#引用语法" class="headerlink" title="引用语法"></a>引用语法</h4><p>引用需要的一个特殊符号是 <code>&gt;</code>，使用方法依旧很简单，只需要在引用的行首加上 <code>&gt;</code> 就可以啦！多行引用就在连续的每一行之前都加上 <code>&gt;</code></p><p>比如说</p><pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">&gt; 爱的魔力转圈圈，想你想到心花怒放黑夜白天&gt; 可是我害怕爱情只是一瞬间，转眼会不见&gt; 我要慢慢冒险<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>上面的markdown效果就是:</p><blockquote><p>爱的魔力转圈圈，想你想到心花怒放黑夜白天<br>可是我害怕爱情只是一瞬间，转眼会不见<br>我要慢慢冒险</p></blockquote><h4 id="代码标记语法"><a href="#代码标记语法" class="headerlink" title="代码标记语法"></a>代码标记语法</h4><p>感觉会来捣腾这样一个博客的，基本都会和程序员搭上点边？？？<br>那么，在自己的文章中把代码高亮出来的这种操作一定也是非常常用啦！<br>在这里，和大家分享三种标记代码的方式：行内代码标记高亮代码块标记。</p><ul><li><p>行内代码标记<br>行内的标记语法 需要通过两个 ` 把需要被标记的内容包起来，注意，这个符号就是键盘上在数字 1 左边的那个符号。<br>比如：</p> <pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">爱的魔力转圈圈 `爱的魔力转圈圈` 爱的魔力转圈圈<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>效果：<br> 爱的魔力转圈圈 <code>爱的魔力转圈圈</code> 爱的魔力转圈圈</p></li><li><p>高亮代码块<br> 这种代码标记的方式可以显示行号，并且可以根据你指定的代码语言对代码进行高亮。为了使用这种标记方式，我们需要在代码块的上方和下方各添加一个以三个反引号开头的行，这个符号也是键盘上数字 1 左侧的符号。<br> 在开头行的三个反引号后写上代码所使用的语言即可对代码块进行语法高亮(彩色)。不加的话显示为黑白效果。</p><p> 比如</p> <pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">这里有三个反引号Javavar executeSync = function(){  var args = Array.prototype.slice.call(arguments);    if (typeof args[0] === 'function'){      args[0].apply(null, args.splice(1));    }};这里有三个反引号<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p> 效果：</p> <pre class="line-numbers language-Java" data-language="Java"><code class="language-Java">var executeSync = function(){  var args = Array.prototype.slice.call(arguments);    if (typeof args[0] === 'function'){      args[0].apply(null, args.splice(1));    }};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h4 id="列表语法"><a href="#列表语法" class="headerlink" title="列表语法"></a>列表语法</h4><p>在我们需要按条列举一些内容时，我们可以使用两种方式来展示。</p><ul><li>第一种是直接使用 <code>[数字].[空格][内容]</code> 的方式实现</li></ul><p>比如：</p> <pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">1. 爱的魔力转圈圈2. 想你想到心花怒放黑夜白天3. 可是我害怕爱情只是一瞬间4. 转眼会不见，我要慢慢冒险<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>效果：</p><ol><li>爱的魔力转圈圈</li><li>想你想到心花怒放黑夜白天</li><li>可是我害怕爱情只是一瞬间</li><li>转眼会不见，我要慢慢冒险</li></ol><ul><li>第二种是用 <code>*</code> 代替数字，使用 <code>[*][空格][内容]</code>的方式实现</li></ul><p> 比如：</p> <pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">* 爱的魔力转圈圈* 想你想到心花怒放黑夜白天* 可是我害怕爱情只是一瞬间* 转眼会不见，我要慢慢冒险<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p> 效果：</p><ul><li>爱的魔力转圈圈</li><li>想你想到心花怒放黑夜白天</li><li>可是我害怕爱情只是一瞬间</li><li>转眼会不见，我要慢慢冒险</li></ul><h4 id="链接语法"><a href="#链接语法" class="headerlink" title="链接语法"></a>链接语法</h4><p>生成链接的markdown语法有两种，一种是内联式，一种是引用式。</p><ul><li>内联式<br>内联式的链接由连续的一对中括号和一对小括号组成，中括号里的内容是链接显示出来的文字，小括号里的内容是链接的地址，写法为 <code>[你想要展示给大家的链接的名称](链接地址)</code></li></ul><p> 比如:</p> <pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">[这是一个小仙女的碎碎念博客](https://jiyali.github.io/)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p> 效果：<br>   <a href="https://jiyali.github.io/">这是一个小仙女的碎碎念博客</a></p><p> 此外，如果想要跳转到同一页面的某个标题处，只需要将链接的位置的内容修改为井号并加上标题名称即可，写法为：<code>[你想要的展示给大家的链接的名称](#本页面的标题名称)</code></p><p> 比如：</p> <pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">[我来教大家怎么写标题~](#标题语法)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p> 效果：<br>  <a href="#%E6%A0%87%E9%A2%98%E8%AF%AD%E6%B3%95">我来教大家怎么写标题~</a><br>  点击上面的链接就调到了标题语法的章节，有木有很方便~</p><ul><li>引用式<br>写论文的时候会用到各种引用，在markdown中也有非常方便的引用式的链接，写法为：<code>[空格][你想要展示给大家的链接名称][链接的标号][空格]</code>，然后在任意位置写引用标签，写法为：<code>[刚才的链接的标号]：链接地址</code>。</li></ul><p> 比如：</p> <pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">在这里我们将要引用一篇论文，论文题目为 [《Show and Tell: A Neural Image Caption Generator》][1][1]:https://arxiv.org/pdf/1411.4555.pdf<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p> 效果：<br> 在这里我们将要引用一篇论文，论文题目为 [《Show and Tell: A Neural Image Caption Generator》][1]<br> [1]:<a href="https://arxiv.org/pdf/1411.4555.pdf">https://arxiv.org/pdf/1411.4555.pdf</a></p><h4 id="插入图片语法"><a href="#插入图片语法" class="headerlink" title="插入图片语法"></a>插入图片语法</h4><h5 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h5><p>打开博客主目录文件 -&gt;  <code>_config.yml</code> -&gt; 搜索<code>post_asset_folder</code> 选项 -&gt; 设置为 <code>true</code></p><h5 id="安装图片插件"><a href="#安装图片插件" class="headerlink" title="安装图片插件"></a>安装图片插件</h5><p>找到博客主目录文件 -&gt; 右键 <code>Git Bash Here</code> -&gt; 执行 <code>npm install hexo-asset-image --save</code> -&gt; 等待安装完成</p><h5 id="生成新的博文"><a href="#生成新的博文" class="headerlink" title="生成新的博文"></a>生成新的博文</h5><p>找到博客主目录文件 -&gt; 右键 <code>Git Bash Here</code> -&gt; 执行 <code>hexo new post '文章题目'</code> ，就可以看到/source/_posts文件夹内除了xxxx.md文件还有一个同名的文件夹，而我们文章中需要引用的图片会放在这个文件夹下面~</p><h5 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h5><p>向文中插入图片的方式和链接是十分类似的，只需要在链接语法前加上一个 <code>!</code> 即可，其中，中括号中的内容是鼠标移到图片上时显示的描述。写法： <code>![你想要展示给大家的图片的名称](保存时图片的名称比如'hhh.png')</code></p><p>比如：</p><pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">![老哥这波稳](timg.jpg)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>效果：<br> <img src="timg.jpg" alt="老哥这波稳"></p><p>酱<del>然后就是hexo三联，提交就能看见我们的博客啦:<br><code>hexo clean</code><br><code>hexo g</code><br><code>hexo s</code><br>是不是不那么乱七八糟啦</del>基本上实现小学生水平的图文并茂~</p><p>参考：<br><a href="https://11.tt/posts/2018/how-to-play-with-hexo/">学会写作和你的Hexo博客一起愉快地玩耍吧</a><br><a href="https://blog.csdn.net/qq_37497322/article/details/80628713">hexo生成博文插入图片</a><br><a href="https://daringfireball.net/projects/markdown/syntax">markdown作者的英文文档</a><br><a href="https://markdown-zh.readthedocs.io/en/latest/">附带markdown作者的中文文档</a><br>学会看技术文档是个好习惯！(ง •_•)ง</p>]]></content>
      
      
      
        <tags>
            
            <tag> markdown语法 </tag>
            
            <tag> vscode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用Valine.js添加留言板功能</title>
      <link href="/2019/04/14/shi-yong-valine-js-tian-jia-liu-yan-ban-gong-neng/"/>
      <url>/2019/04/14/shi-yong-valine-js-tian-jia-liu-yan-ban-gong-neng/</url>
      
        <content type="html"><![CDATA[<p>本文配置Valine 是基于Next主题的，相对来说更简单一些，如果不是使用Next主题，这里提供<a href="https://valine.js.org/">Valine</a>官方文档给大家参阅。</p><h2 id="LeanCloud-基本应用创建"><a href="#LeanCloud-基本应用创建" class="headerlink" title="LeanCloud 基本应用创建"></a>LeanCloud 基本应用创建</h2><p>先来一条官网链接<a href="https://leancloud.cn/">LeanCloud(https://leancloud.cn/)</a></p><h3 id="注册LeanCloud"><a href="#注册LeanCloud" class="headerlink" title="注册LeanCloud"></a>注册LeanCloud</h3><p><a href="https://leancloud.cn/dashboard/login.html#/signup">LeanCloud注册链接</a><br><img src="%E6%B3%A8%E5%86%8C.jpg" alt="注册"><br>注册结束，登录</p><h3 id="创建应用"><a href="#创建应用" class="headerlink" title="创建应用"></a>创建应用</h3><p>登陆之后会出现下图所示的一个界面，点击红框中的创建应用<br><img src="hhh.png" alt="创建应用"><br>然后在弹出框中随意填写一个名字，使用免费的开发版即可~<br><img src="%E5%88%9B%E5%BB%BA%E5%BA%94%E7%94%A8.png" alt="创建应用"><br>创建结束后出现如下所示界面<br><img src="%E8%AF%84%E8%AE%BA.png" alt="创建结束"></p><h3 id="获取应用Key"><a href="#获取应用Key" class="headerlink" title="获取应用Key"></a>获取应用Key</h3><p>点击上图的这个窗口进入应用界面。<br>点击左侧的”设置”和中间的”应用Key”<br><img src="key.png" alt="Key"><br>保存最右侧的App ID和App Key，将其复制下来以供接下来的配置使用。</p><h3 id="配置安全域名"><a href="#配置安全域名" class="headerlink" title="配置安全域名"></a>配置安全域名</h3><p>现在这个状态，任何人都可以访问我们的资源，这样就有可能会导致资源的泄露，所以我们需要配置安全域名，只有在白名单中的域名才可以使用我们的资源。<br>仍然是在刚才的设置页面，点击当中红框的“安全中心”，在最右侧红框“Web 安全域名”中根据提示填写我们站点的域名后点击保存即可。<br><img src="%E5%AE%89%E5%85%A8%E5%9F%9F%E5%90%8D.png" alt="安全域名"></p><h2 id="配置-Next-主题中的-Valine-评论功能"><a href="#配置-Next-主题中的-Valine-评论功能" class="headerlink" title="配置 Next 主题中的 Valine 评论功能"></a>配置 Next 主题中的 Valine 评论功能</h2><p>将评论功能添加到博客中<br>打开博客主目录文件-&gt; <code>themes</code> 文件夹-&gt; <code>next</code> -&gt; <code>_config.yml</code><br>打开 <code>_config.yml</code> 文件后搜索 <code># Valine</code> 字段，我们可以看到如下内容：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># Valine.</span><span class="token comment"># You can get your appid and appkey from https://leancloud.cn</span><span class="token comment"># more info please open https://valine.js.org</span>valine:  enable: <span class="token boolean">false</span> <span class="token comment"># When enable is set to be true, leancloud_visitors is recommended to be closed for the re-initialization problem within different leancloud adk version.</span>  appid:  <span class="token comment"># your leancloud application appid</span>  appkey:  <span class="token comment"># your leancloud application appkey</span>  notify: flase <span class="token comment"># mail notifier, See: https://github.com/xCss/Valine/wiki</span>  verify: flase <span class="token comment"># Verification code</span>  placeholder: Just go go <span class="token comment"># comment box placeholder</span>  avatar: mm <span class="token comment"># gravatar style</span>  guest_info: nick,mail,link <span class="token comment"># custom comment header</span>  pageSize: <span class="token number">10</span> <span class="token comment"># pagination size</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我们将刚刚获得的 <code>App ID</code> 和 <code>App Key</code> 分别填写到 <code>appid</code> 和 <code>appkey</code> 这两个字段的冒号后面。并且将 <code>enable</code> 修改为 <code>true</code> 。</p><p>刷新博客的文章页面，评论功能就出现啦~</p><p>参考链接：<br><a href="https://11.tt/posts/2018/add-valine-to-your-blog/">使用Valine.js评论系统让游客们到你的Hexo博客留个脚印吧</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Valine.js </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用Github+hexo+next搭建个人博客的全过程</title>
      <link href="/2019/04/14/shi-yong-github-hexo-next-da-jian-ge-ren-bo-ke-de-quan-guo-cheng/"/>
      <url>/2019/04/14/shi-yong-github-hexo-next-da-jian-ge-ren-bo-ke-de-quan-guo-cheng/</url>
      
        <content type="html"><![CDATA[<h2 id="搭建过程"><a href="#搭建过程" class="headerlink" title="搭建过程"></a>搭建过程</h2><p>本次搭建Hexo使用的本地环境如下：</p><blockquote><ul><li>Windows 10</li><li>node v10.15.3 -x64</li><li>git version2.20.1.windows.1</li><li>hexo v3.8.0</li><li>next v6.0.0</li></ul></blockquote><h3 id="准备软件的安装"><a href="#准备软件的安装" class="headerlink" title="准备软件的安装"></a>准备软件的安装</h3><h4 id="Node-js"><a href="#Node-js" class="headerlink" title="Node.js"></a>Node.js</h4><p>  打开<a href="http://nodejs.org/">Node.js官网(http://nodejs.org)</a>，我们就能看见如下的安装界面：<br>  <img src="node%E5%AE%89%E8%A3%85%E7%95%8C%E9%9D%A2.png" alt="Node.js安装界面"><br>  点击左面的按钮(10.15.3LTS)，下载 <code>node-v10.15.3-x64.msi</code> 安装文件。<br>  下载完成后，打开 <code>node-v10.15.3-x64.msi</code> 进行安装。<br>  除非你要修改安装路径或者有什么特殊的需求，否则一路默认安装即可。<br>  安装完成后，打开cmd，输入 <code>node -v</code> ，若为以下类似的输出，则安装成功。</p>  <pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">C:\Users\Administrator&gt;node -vv10.15.3<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h4 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h4><p>  打开<a href="https://git-scm.com/downloads">Git官网(https://git-scm.com/downloads)</a>，我们就能看见如下安装界面：<br>    <img src="git%E5%AE%89%E8%A3%85%E7%95%8C%E9%9D%A2.png" alt="Git安装界面"><br>    选择适用系统(比如windows)，点击进入后<br>    <img src="gitwindows.png" alt="Git安装界面"><br>    选择图中红框内的链接，下载 <code>Git-2.21.0-64-bit.exe</code> 安装文件。<br>    下载完成后，打开 <code>Git-2.21.0-64-bit.exe</code> 进行安装。<br>    安装完成后，打开cmd，输入 <code>git --version</code> ，若为以下类似的输出，则安装成功。</p><pre><code><pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">C:\Users\Administrator&gt;git --versiongit version 2.20.1.windows.1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></code></pre><h4 id="github"><a href="#github" class="headerlink" title="github"></a>github</h4><h5 id="注册"><a href="#注册" class="headerlink" title="注册"></a>注册</h5><p>   点击<a href="https://github.com/">github官网(https://github.com/)</a><br>  <img src="%E6%B3%A8%E5%86%8C%E7%95%8C%E9%9D%A2.png" alt="github注册界面"><br>  这里的 <code>Username</code> 慎重考虑喔！因为个人网站的固定格式就是：<code>username.github.io</code></p><h5 id="创建Repository"><a href="#创建Repository" class="headerlink" title="创建Repository"></a>创建Repository</h5><p>   登录github之后，点击右上角的 + 号，选择New repository 创建一个与你的博客相关的Repository项目进行管理，之后所有你博客的动态都会在这Repository更新。Repository的名字是username.github.io，比如我的JiYali.github.io已经创建。其余可以先不填，点击Create repository<br>   <img src="%E5%88%9B%E5%BB%BARepository.png" alt="创建Repository界面"></p><h5 id="配置和使用github"><a href="#配置和使用github" class="headerlink" title="配置和使用github"></a>配置和使用github</h5><p>   开始—所有应用—找到git bash<br>   <img src="%E5%BC%80%E5%A7%8B.png" alt="开始界面"></p><h5 id="配置SSH-Keys"><a href="#配置SSH-Keys" class="headerlink" title="配置SSH Keys"></a>配置SSH Keys</h5><p>   SSH Keys用来使本地git项目与GitHub联系，这样能在GitHub上的博客项目是最新更新的。</p><ul><li><p>检查SSH Key的设置<br>  首先检查自己电脑上现有的SSH Key:</p>  <pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">cd ~/.ssh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>  如果显示下图信息，则说明这是你第一次使用git<br> <img src="ssh.png" alt="ssh"></p></li><li><p>生成新的SSH Key:</p> <pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">ssh-keygen -t rsa -C "邮件地址@youremail.com"Generating public/private rsa key pair.Enter file in which to save the key (/Users/your_user_directory/.ssh/id_rsa):&lt;回车就好&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p> 这里的邮箱地址，输入注册github的邮箱地址<br> 然后系统会要你输入密码</p> <pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">Enter passphrase (empty for no passphrase):&lt;设置密码&gt;Enter same passphrase again:&lt;再次输入密码&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p> 再回车，这里会提示你输入一个密码，作为你提交项目时使用。<br> 这个密码的作用就是在个人网站里所有的改动只能经过你的手，也可以不设置密码，直接为空。<br> 注意：输入密码的时候没有输入痕迹的，不要以为什么也没有输入。<br> 最后看到类似于这样的界面，就成功设置ssh key了：<br> <img src="web_08.png" alt="密钥界面"></p></li><li><p>添加SSH Key到github上<br> 在本地文件夹找到id_rsa.pub文件。看上面图片中的第四行的位置就告诉你保存在哪里了。默认为<code>C:\Users\Administrator\.ssh</code>。没找到的话勾选一下文 件扩展名和隐藏的项目。<br> <img src="%E5%8B%BE%E9%80%89.png" alt="勾选"><br> .ssh文件夹里记事本打开这个文件复制全部内容到github相应位置<br> 回到你的GitHub主页，右上角点击头像选中Setting<br> <img src="setting.png" alt="setting"><br> 继续选中左边菜单栏的SSH and GPG keys<br> <img src="add.png" alt="add"><br> Title最好写，随便写。网上有说不写title也有可能后期出现乱七八糟的错误<br> Key部分就是放刚才复制的内容了，点击Add SSH key</p></li></ul><h5 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h5><p>回到git bash 框里<br>输入以下代码，不要改任何一个字！！！</p><p><code>ssh -T git@github.com</code></p><p>  回车，看到如下：</p><pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">The authenticity of host 'github.com (52.74.223.119)' can't be established.RSA key fingerprint is SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8.Are you sure you want to continue connecting (yes/no)?  ```dash  输入yes回车<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Enter passphrase for key ‘/c/Users/Administrator/.ssh/id_rsa’:</p><pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">输入刚才设置的密码回车，看到“You’ve successfully authenticated…”成功！下一步！#### 输入用户信息现在已经成功通过SSH链接到github啦，我们需要完善一下个人信息：```dashgit config --global user.name "username" //输入注册时的usernamegit config --global user.email  "注册的邮箱地址" //填写注册邮箱<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>   到此，SSH Key配置结束啦！你的电脑已成功连接到 github。</p><h4 id="Hexo"><a href="#Hexo" class="headerlink" title="Hexo"></a>Hexo</h4><p>开始菜单中打开git bash，利用npm命令安装hexo</p><pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">npm install -g hexo<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>请耐心稍等一会儿，如果在安装过程中头部出现 WARN ，可能是因为某些内容不支持 Windows，请不要担心，并不影响实际使用。</p><p>(在我写教程的时候安装hexo死活就是出错，尝试了<br><code>npm config set registry https://registry.npm.taobao.org</code><br>再输入<code>npm install -g hexo</code>后才安装正常，可以做个小参考，如果没用，请自行百度解决~)</p><p>在安装完成后，输入<code>hexo -v</code>，若出现类似以下内容，则 Hexo 已经安装成功。</p><pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">hexo-cli: 1.1.0os: Windows_NT 10.0.17763 win32 x64http_parser: 2.8.0node: 10.15.3v8: 6.8.275.32-node.51uv: 1.23.2zlib: 1.2.11ares: 1.15.0modules: 64nghttp2: 1.34.0napi: 3openssl: 1.1.0jicu: 62.1unicode: 11.0cldr: 33.1tz: 2018e<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="搭建hexo博客"><a href="#搭建hexo博客" class="headerlink" title="搭建hexo博客"></a>搭建hexo博客</h3><h4 id="创建博客主目录"><a href="#创建博客主目录" class="headerlink" title="创建博客主目录"></a>创建博客主目录</h4><p>在本地创建一个与 Repository中博客项目同名的文件夹username.github.io(如D:/JiYali.github.io)</p><h4 id="初始化hexo主目录"><a href="#初始化hexo主目录" class="headerlink" title="初始化hexo主目录"></a>初始化hexo主目录</h4><p>在文件夹上点击鼠标右键，选择 Git bash here，输入如下命令<code>hexo init</code></p><ul><li>安装依赖包</li></ul><p> <code>npm install</code></p><ul><li><p>确保git部署<br><code>npm install hexo-deployer-git --save</code></p></li><li><p>本地查看</p></li></ul> <pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">hexo ghexo s<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>打开浏览器，在地址输入<a href="http://localhost:4000/">http://localhost:4000</a> ，我们就可以看到激动人心的界面啦！<br> <img src="%E5%88%9D%E5%A7%8B%E7%95%8C%E9%9D%A2.png" alt="初始界面"><br>至此hexo博客就真的已经简！单！的！搭！建！完！成！了！✿✿ヽ(°▽°)ノ✿</p><h3 id="使用next主题"><a href="#使用next主题" class="headerlink" title="使用next主题"></a>使用next主题</h3><p>我想说next这个主题真的是超简单方面，之前尝试了几个主题配置方面都是贼麻烦，所以最终还是随大流选择了next，emmmm..完全是发现了新！大！陆！(✪ω✪)</p><h4 id="克隆主题"><a href="#克隆主题" class="headerlink" title="克隆主题"></a>克隆主题</h4><p>在博客主目录文件上点击鼠标右键，选择 Git bash here，输入如下命令：</p><pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">git clone https://github.com/theme-next/hexo-theme-next themes/next<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="启用next主题"><a href="#启用next主题" class="headerlink" title="启用next主题"></a>启用next主题</h4><p>我们在博客主目录文件夹下可以看到有一个名为 <code>_config.yml</code> 的配置文件，我们用编辑器打开它，搜索 <code>theme</code>，我们可以找到以下内容：</p><pre class="line-numbers language-dash" data-language="dash"><code class="language-dash"># Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: landscape<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>我们要将其中的 <code>landscape</code> 修改为我们要是用的 <code>next</code>，修改完如下所示：</p><pre class="line-numbers language-dash" data-language="dash"><code class="language-dash"># Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h4 id="查看已启用的主题"><a href="#查看已启用的主题" class="headerlink" title="查看已启用的主题"></a>查看已启用的主题</h4><p>我们在 Git Bash 依次输入下方三条命令：</p><pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">hexo cleanhexo ghexo s<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>就可以看到如下界面啦<br><img src="next%E5%88%9D%E5%A7%8B%E5%8C%96.png" alt="next初始化"></p><h2 id="将博客部署到uesrname-github-io"><a href="#将博客部署到uesrname-github-io" class="headerlink" title="将博客部署到uesrname.github.io"></a>将博客部署到uesrname.github.io</h2><h3 id="复制SSH码"><a href="#复制SSH码" class="headerlink" title="复制SSH码"></a>复制SSH码</h3><p>进入 Github 个人主页中的 Repository，复制新建的独立博客项目username.github.io的 SSH码<br><img src="githubSSH.png" alt="gethubSSH"></p><h3 id="编辑整站配置文件"><a href="#编辑整站配置文件" class="headerlink" title="编辑整站配置文件"></a>编辑整站配置文件</h3><p>打开博客主目录文件夹下的 <code>_config.yml</code> ，用编辑器打开它，搜索 <code>deploy</code>，把刚刚复制的 SSH码粘贴到repository：后面，别忘了冒号后要空一格。</p><pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">deploy:  type: git  repository: git@github.com:jiyali/JiYali.github.io.git  branch: master<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="执行下列指令即可完成部署"><a href="#执行下列指令即可完成部署" class="headerlink" title="执行下列指令即可完成部署"></a>执行下列指令即可完成部署</h3><p>Git Bash 依次输入下方命令：</p><pre class="line-numbers language-dash" data-language="dash"><code class="language-dash">hexo cleanhexo ghexo shexo d<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>【提示】如果在配置 SSH key 时设置了密码，执行 hexo d 命令上传文件时需要输入密码进行确认<br>输入密码之后在浏览器输入：username.github.io<br>Surprise🎉！恭喜你~<br>现在你已经拥有自己的一个小空间啦~</p><p>参考链接:<br><a href="http://codewithzhangyi.com/2018/04/19/%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99%EF%BC%88%E4%B8%8A%EF%BC%89/">如何搭建自己的个人网站（上）</a><br><a href="https://11.tt/posts/2018/set-up-hexo-with-coding-and-github/">基于Hexo+Coding+Github搭建个人博客的全过程</a></p><p>我的文章将持续更新在我的 <a href="https://jiyali.github.io/">https://jiyali.github.io/</a> 里<br>任何疑问请在下方留言，也将在下一期教如何制作留言板和写博客等内容~<br>敬请期待~❤</p>]]></content>
      
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> next </tag>
            
            <tag> github </tag>
            
            <tag> 博客 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
