<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>树模型与集成学习の学习笔记</title>
      <link href="/2021/10/10/shu-mo-xing-yu-ji-cheng-xue-xi-noxue-xi-bi-ji/"/>
      <url>/2021/10/10/shu-mo-xing-yu-ji-cheng-xue-xi-noxue-xi-bi-ji/</url>
      
        <content type="html"><![CDATA[<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><p>先解决几个概念问题，比如信息量，信息熵等，这里借鉴【忆臻】大佬在知乎上的解释。</p><h3 id="信息量"><a href="#信息量" class="headerlink" title="信息量"></a>信息量</h3><p>信息的大小跟随机事件的概率有关。越小概率的事情发生了产生的信息量越大，如湖南产生的地震了；越大概率的事情发生了产生的信息量越小，如太阳从东边升起来了（肯定发生嘛，没什么信息量）。这很好理解！</p><ol><li>一个具体事件的信息量应该是随着其发生概率而递减的，且不能为负。</li><li>如果我们有俩个不相关的事件x和y，那么我们观察到的俩个事件同时发生时获得的信息应该等于观察到的事件各自发生时获得的信息之和，即： $h(x,y) = h(x)+h(y)$ ,由于x,y的是两个不相关的事件，那么也满足 $p(x,y)=p(x)*p(y)$</li></ol><p>  根据以上两个性质，我们容易看出h(x)一定与p(x)的对数有关（因为只有对数形式的真数相乘之后，能够对应对数的相加形式，可以试试）。因此我们有信息量公式如下：<br>\[h(x) = -{log_{ {2}}}p(x)\]<br>注；①其中，负号是为了确保信息一定是正数或者是0，总不能为负数吧！信息量取概率的负对数，其实是因为信息量的定义是概率的倒数的对数。而用概率的倒数，是为了使概率越大，信息量越小，同时因为概率的倒数大于1，其对数自然大于0了。<br>   ②为什么底数为2：这是因为，我们只需要信息量满足低概率事件x对应于高的信息量。那么对数的选择是任意的。我们只是遵循信息论的普遍传统，使用2作为对数的底！</p><h3 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h3><p>信息量度量的是一个具体事件发生了所带来的信息，而熵则是在结果出来之前对可能产生的信息量的期望——考虑该随机变量的所有可能取值，即所有可能发生事件所带来的信息量的期望。简单的讲，<strong>信息熵就是平均而言发生一个事件我们所得到的信息量的大小</strong><br>这里信息熵就表示为$H(x)=-\sum_{i=1}^{n} p(x_i)log_{ {2}}p(x_i)$，且当i=0时，定义$0log0=0$</p><h3 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h3><p>在决策树的分裂过程中，我们不但需要考察本节点的不确定性或纯度，而且还要考察子节点的平均不确定性或平均纯度来决定是否进行分裂。子节点的产生来源于决策树分支的条件，因此我们不但要研究随机变量的信息熵，还要研究在给定条件下随机变量的平均信息熵或条件熵<br>官方定义的条件熵如下：</p><blockquote><p>X给定条件下，Y的条件概率分布的熵对X的数学期望（已知随机变量X条件下随机变量Y的不确定性）<br>\[H(Y|X)=\sum_{i=1}^{n} H(Y|X=x_i)\]</p></blockquote><ul><li>那么条件熵到底是个啥嘞….</li></ul><p>$y = f(x)$<br>\[y = {f_{ {g_1}}}(x)\]</p><p><a href="https://zhuanlan.zhihu.com/p/26486223">通俗理解信息熵</a><br><a href="https://zhuanlan.zhihu.com/p/26551798?tdsourcetag=s_pctim_aiomsg">通俗理解条件熵</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>情感分析学习笔记</title>
      <link href="/2021/09/12/qing-gan-fen-xi-xue-xi-bi-ji/"/>
      <url>/2021/09/12/qing-gan-fen-xi-xue-xi-bi-ji/</url>
      
        <content type="html"><![CDATA[<p>记录datawhale九月份的学习，地址：<a href="https://github.com/datawhalechina/team-learning-nlp/tree/master/Emotional_Analysis">情感分析</a></p><h2 id="Task0：自然语言处理之PyTorch情感分析简介"><a href="#Task0：自然语言处理之PyTorch情感分析简介" class="headerlink" title="Task0：自然语言处理之PyTorch情感分析简介"></a>Task0：自然语言处理之PyTorch情感分析简介</h2><p>   从来没有接触过情感分析，一切都要从零开始，这次大航海的教程好像都是实战为主的，有点怀疑自己能不能跟得上。先记录一下task0遇到的问题。<br><strong>问题</strong><br>环境配置按照教程的问题不大，进行到以下语句的时候出现了 <em>‘[WinError 10061] 由于目标计算机积极拒绝，无法连接’</em> 的问题</p><pre class="line-numbers language-none"><code class="language-none">python -m spacy download zh_core_web_smpython -m spacy download en_core_web_sm<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><strong>解决办法</strong><br>搜解决办法好多需要改防火墙设置什么的，而且不怎么奏效，索性直接选择了离线安装的办法。</p><ul><li>zh_core_web_sm下载地址：<br><a href="https://github.com/explosion/spacy-models/releases/tag/zh_core_web_sm-3.1.0">https://github.com/explosion/spacy-models/releases/tag/zh_core_web_sm-3.1.0</a></li><li>en_core_web_sm下载地址：<br><a href="https://github.com/explosion/spacy-models/releases/tag/en_core_web_sm-3.1.0">https://github.com/explosion/spacy-models/releases/tag/en_core_web_sm-3.1.0</a></li></ul><p>下载完成后运行即可</p><pre class="line-numbers language-none"><code class="language-none">pip install zh_core_web_sm-3.1.0.tar.gzpip install en_core_web_sm-3.1.0.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><strong>插曲</strong><br>记录一下不应该成为问题的问题：中间安装环境的时候，出现了一大片红色（真的好久没有见过这么大片的红），并提示<code>ValueError: check_hostname  requires server_hostname</code> 的错误，关闭科学上网工具后解决。fine~</p><h2 id="Task1-情感分析baseline"><a href="#Task1-情感分析baseline" class="headerlink" title="Task1:情感分析baseline"></a>Task1:情感分析baseline</h2><p>任务简介：使用pytorch和torchtext构造一个简单的模型预测情绪（正面或负面）<br>数据集：IMDb数据集</p><h3 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h3><h4 id="1-torchtext"><a href="#1-torchtext" class="headerlink" title="1.torchtext"></a>1.torchtext</h4><p>torchtext包含以下组件：</p><ul><li>Field :主要包含以下数据预处理的配置信息，比如指定分词方法，是否转成小写，起始字符，结束字符，补全字符以及词典等等。此次教程中需要指定一个 tokenizer_language 来告诉 torchtext 使用哪个 spaCy 模型。我们使用 en_core_web_sm 模型。<pre class="line-numbers language-none"><code class="language-none">en_core_web_sm模型的下载地址：https://github.com/explosion/spacy-models/releases/tag/en_core_web_sm-3.1.0安装时运行 pip install en_core_web_sm-3.1.0.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li>Dataset :继承自pytorch的Dataset，用于加载数据，提供了TabularDataset可以指点路径，格式，Field信息就可以方便的完成数据加载。同时torchtext还提供预先构建的常用数据集的Dataset对象，可以直接加载使用，splits方法可以同时加载训练集，验证集和测试集。</li><li>Iterator : 主要是数据输出的模型的迭代器，可以支持batch定制</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torchtext<span class="token punctuation">.</span>legacy <span class="token keyword">import</span> data<span class="token comment"># 设置随机种子数，该数可以保证随机数是可重复的</span>SEED <span class="token operator">=</span> <span class="token number">1234</span><span class="token comment"># 设置种子</span>torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span>torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>deterministic <span class="token operator">=</span> <span class="token boolean">True</span><span class="token comment"># 读取数据和标签</span>TEXT <span class="token operator">=</span> data<span class="token punctuation">.</span>Field<span class="token punctuation">(</span>tokenize <span class="token operator">=</span> <span class="token string">'spacy'</span><span class="token punctuation">,</span> tokenizer_language <span class="token operator">=</span> <span class="token string">'en_core_web_sm'</span><span class="token punctuation">)</span>LABEL <span class="token operator">=</span> data<span class="token punctuation">.</span>LabelField<span class="token punctuation">(</span>dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token comment"># 下载 IMDb 数据集</span><span class="token keyword">from</span> torchtext<span class="token punctuation">.</span>legacy <span class="token keyword">import</span> datasetstrain_data<span class="token punctuation">,</span> test_data <span class="token operator">=</span> datasets<span class="token punctuation">.</span>IMDB<span class="token punctuation">.</span>splits<span class="token punctuation">(</span>TEXT<span class="token punctuation">,</span> LABEL<span class="token punctuation">)</span><span class="token comment"># 查看训练集和测试集的大小</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Number of training examples: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>train_data<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Number of testing examples: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>test_data<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span><span class="token comment"># 查看其中一个示例数据</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">vars</span><span class="token punctuation">(</span>train_data<span class="token punctuation">.</span>examples<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-划分数据集"><a href="#2-划分数据集" class="headerlink" title="2.划分数据集"></a>2.划分数据集</h4><p>IMDb 数据集划分了训练集和测试集，这里我们还需要创建一个验证集。 可以使用 .split() 方法来做。（ <strong>注：将之前设置的随机种子SEED传递给 random_state 参数，确保我们每次获得相同的训练集和验证集。</strong> ）</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> randomtrain_data<span class="token punctuation">,</span> valid_data <span class="token operator">=</span> train_data<span class="token punctuation">.</span>split<span class="token punctuation">(</span>split_ratio<span class="token operator">=</span><span class="token number">0.8</span> <span class="token punctuation">,</span> random_state <span class="token operator">=</span> random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 查看训练集，验证集和测试集分别有多少数据</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Number of training examples: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>train_data<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Number of validation examples: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>valid_data<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Number of testing examples: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>test_data<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="3-构建词汇表"><a href="#3-构建词汇表" class="headerlink" title="3.构建词汇表"></a>3.构建词汇表</h4><ul><li>由于模型不能直接对字符串进行操作，只能对数字进行操作，所以需要建立一下查找表，每个单词都有唯一对应的index。</li><li>onehot编码在此不适用（训练集中不同的单词数量巨大，训练的代价大），需要优化，优化方案有：只取前n个出现次数最多的单词作为one-hot的基，另一种是忽略出现次数小于m个的单词。（这里使用前者）</li><li>优化后的onehot会出现很多单词虽然在数据集中，但是无法编码的情况，这里还要引进两个特殊的符号来编码，一个是<unk>用来处理前面所说的问题，另一个是<pad>，用于句子的填充。<pre class="line-numbers language-python" data-language="python"><code class="language-python">MAX_VOCAB_SIZE <span class="token operator">=</span> <span class="token number">25000</span> <span class="token comment"># 词汇表的最大长度</span>TEXT<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> max_size <span class="token operator">=</span> MAX_VOCAB_SIZE<span class="token punctuation">)</span>LABEL<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Unique tokens in TEXT vocabulary: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Unique tokens in LABEL vocabulary: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>LABEL<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></pad></unk></li></ul><h4 id="4-创建迭代器"><a href="#4-创建迭代器" class="headerlink" title="4.创建迭代器"></a>4.创建迭代器</h4><ul><li>准备数据的最后一步是创建迭代器. 需要创建验证集，测试集，以及训练集的迭代器, 每一次的迭代都会返回一个batch的数据。</li><li>本例中使用“BucketIterator”，它将返回一批示例，其中每个样本的长度差不多，从而最小化每个样本的padding数。</li><li>用torch.device，可以将张量放到gpu或者cpu上。<pre class="line-numbers language-python" data-language="python"><code class="language-python">BATCH_SIZE <span class="token operator">=</span> <span class="token number">64</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">'cpu'</span><span class="token punctuation">)</span>train_iterator<span class="token punctuation">,</span> valid_iterator<span class="token punctuation">,</span> test_iterator <span class="token operator">=</span> data<span class="token punctuation">.</span>BucketIterator<span class="token punctuation">.</span>splits<span class="token punctuation">(</span>    <span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> valid_data<span class="token punctuation">,</span> test_data<span class="token punctuation">)</span><span class="token punctuation">,</span>     batch_size <span class="token operator">=</span> BATCH_SIZE<span class="token punctuation">,</span>    device <span class="token operator">=</span> device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h3 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h3><ul><li>首先关于初始化的问题，在某些框架中，使用RNN需要初始化 ℎ0 ，但在pytorch中不用，默认为全0。</li><li>构建一个RNN模型(nn.module子类)<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">class</span> <span class="token class-name">RNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_dim<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span>                self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>RNN<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span>                self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span>            <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment">#text = [sent len, batch size]</span>                embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>text<span class="token punctuation">)</span>                <span class="token comment">#embedded = [sent len, batch size, emb dim]</span>                output<span class="token punctuation">,</span> hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>embedded<span class="token punctuation">)</span>                <span class="token comment">#output = [sent len, batch size, hid dim]</span>        <span class="token comment">#hidden = [1, batch size, hid dim]</span>                <span class="token keyword">assert</span> torch<span class="token punctuation">.</span>equal<span class="token punctuation">(</span>output<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> hidden<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                <span class="token keyword">return</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>hidden<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li>设置超参数<pre class="line-numbers language-python" data-language="python"><code class="language-python">INPUT_DIM <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span>EMBEDDING_DIM <span class="token operator">=</span> <span class="token number">100</span>HIDDEN_DIM <span class="token operator">=</span> <span class="token number">256</span>OUTPUT_DIM <span class="token operator">=</span> <span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><ul><li>设置优化器：SGD</li><li>定义损失函数：BCEWithLogitsLoss一般用来做二分类<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optimoptimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCEWithLogitsLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>model <span class="token operator">=</span> model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>criterion <span class="token operator">=</span> criterion<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li>设置训练函数和评估函数<br>训练函数和评估函数是相似的，评估时不需要再进行梯度计算，所以使用<code>with no_grad()</code>，去掉了<code>optimizer.zero_grad(), loss.backward() , optimizer.step()</code><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> iterator<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span><span class="token punctuation">:</span>        epoch_loss <span class="token operator">=</span> <span class="token number">0</span>    epoch_acc <span class="token operator">=</span> <span class="token number">0</span>        model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> batch <span class="token keyword">in</span> iterator<span class="token punctuation">:</span>                optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>                        predictions <span class="token operator">=</span> model<span class="token punctuation">(</span>batch<span class="token punctuation">.</span>text<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>                loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>                acc <span class="token operator">=</span> binary_accuracy<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>                loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>                optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>                epoch_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        epoch_acc <span class="token operator">+=</span> acc<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">return</span> epoch_loss <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span><span class="token punctuation">,</span> epoch_acc <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">evaluate</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> iterator<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span><span class="token punctuation">:</span>        epoch_loss <span class="token operator">=</span> <span class="token number">0</span>    epoch_acc <span class="token operator">=</span> <span class="token number">0</span>        model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">for</span> batch <span class="token keyword">in</span> iterator<span class="token punctuation">:</span>            predictions <span class="token operator">=</span> model<span class="token punctuation">(</span>batch<span class="token punctuation">.</span>text<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>                        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>                        acc <span class="token operator">=</span> binary_accuracy<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>            epoch_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>            epoch_acc <span class="token operator">+=</span> acc<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">return</span> epoch_loss <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span><span class="token punctuation">,</span> epoch_acc <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li>最后保留在验证集上的损失值最小的模型<pre class="line-numbers language-python" data-language="python"><code class="language-python">N_EPOCHS <span class="token operator">=</span> <span class="token number">5</span>best_valid_loss <span class="token operator">=</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token string">'inf'</span><span class="token punctuation">)</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>N_EPOCHS<span class="token punctuation">)</span><span class="token punctuation">:</span>    start_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>        train_loss<span class="token punctuation">,</span> train_acc <span class="token operator">=</span> train<span class="token punctuation">(</span>model<span class="token punctuation">,</span> train_iterator<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span>    valid_loss<span class="token punctuation">,</span> valid_acc <span class="token operator">=</span> evaluate<span class="token punctuation">(</span>model<span class="token punctuation">,</span> valid_iterator<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span>        end_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>    epoch_mins<span class="token punctuation">,</span> epoch_secs <span class="token operator">=</span> epoch_time<span class="token punctuation">(</span>start_time<span class="token punctuation">,</span> end_time<span class="token punctuation">)</span>        <span class="token keyword">if</span> valid_loss <span class="token operator">&lt;</span> best_valid_loss<span class="token punctuation">:</span>        best_valid_loss <span class="token operator">=</span> valid_loss        torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'tut1-model.pt'</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Epoch: </span><span class="token interpolation"><span class="token punctuation">{</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token format-spec">02</span><span class="token punctuation">}</span></span><span class="token string"> | Epoch Time: </span><span class="token interpolation"><span class="token punctuation">{</span>epoch_mins<span class="token punctuation">}</span></span><span class="token string">m </span><span class="token interpolation"><span class="token punctuation">{</span>epoch_secs<span class="token punctuation">}</span></span><span class="token string">s'</span></span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'\tTrain Loss: </span><span class="token interpolation"><span class="token punctuation">{</span>train_loss<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string"> | Train Acc: </span><span class="token interpolation"><span class="token punctuation">{</span>train_acc<span class="token operator">*</span><span class="token number">100</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string">%'</span></span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'\t Val. Loss: </span><span class="token interpolation"><span class="token punctuation">{</span>valid_loss<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string"> |  Val. Acc: </span><span class="token interpolation"><span class="token punctuation">{</span>valid_acc<span class="token operator">*</span><span class="token number">100</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string">%'</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>贴一张训练结果图</p><p>实验结果的准确率很低，有可能是本身RNN是一个很简单的模型，做不了太复杂的任务，另外word_embedding这里使用的是onehot，也会影响模型准确率~</p><h2 id="Task2：Updated情感分析"><a href="#Task2：Updated情感分析" class="headerlink" title="Task2：Updated情感分析"></a>Task2：Updated情感分析</h2><p>此次任务使用的是双向LSTM网络，并修改了基础模型的词向量，改用Glove</p><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>准备数据阶段与上次任务相同，这里不再赘述</p><ul><li>词向量（Glove）<br>这次任务使用的是 “glove.6B.100d” ，其中，6B表示词向量是在60亿规模的tokens上训练得到的，100d表示词向量是100维的(注意,这个词向量有800多兆)<pre class="line-numbers language-python" data-language="python"><code class="language-python">MAX_VOCAB_SIZE <span class="token operator">=</span> 25_000TEXT<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">,</span>                  max_size <span class="token operator">=</span> MAX_VOCAB_SIZE<span class="token punctuation">,</span>                  vectors <span class="token operator">=</span> <span class="token string">"glove.6B.100d"</span><span class="token punctuation">,</span>                  unk_init <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">.</span>normal_<span class="token punctuation">)</span>LABEL<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li>迭代器与GPU设置也不再赘述</li></ul><h3 id="构建模型-1"><a href="#构建模型-1" class="headerlink" title="构建模型"></a>构建模型</h3><h4 id="有关LSTM"><a href="#有关LSTM" class="headerlink" title="有关LSTM"></a>有关LSTM</h4><p>RNN作为一种处理序列数据的神经网络，序列数据通常会有上下文之间的影响，RNN通过让上一时刻的隐藏层影响当前时刻的隐藏层来解决序列间关系的问题。但是RNN还存在一些漏洞，比如在长序列数据处理中存在梯度消失的问题，长序列处理中，梯度被近距离梯度主导，导致模型难以学到远距离的依赖关系。而LSTM则解决了此问题<br>LSTM的关键是细胞状态（直译：cell state），表示为$C_t$ ，用来保存当前LSTM的状态信息并传递到下一时刻的LSTM中，也就是RNN中那根“自循环”的箭头。当前的LSTM接收来自上一个时刻的细胞状态 $C_{t-1}$ ，并与当前LSTM接收的信号输入 $x_t$ 共同作用产生当前LSTM的细胞状态 $C_t$，具体的作用方式下面将详细介绍。<br><img src="1.png"><br>LSTM主要包括三个不同的门结构：遗忘门、记忆门和输出门。这三个门用来控制LSTM的信息保留和传递，最终反映到细胞状态 $C_t$和输出信$h_t$ 。<br>*遗忘门由一个sigmod神经网络层和一个按位乘操作构成；<br>*记忆门由输入门（input gate）与tanh神经网络层和一个按位乘操作构成；<br>*输出门（output gate）与 tanh函数（注意：这里不是tanh神经网络层）以及按位乘操作共同作用将细胞状态和输入信号传递到输出端。<br>遗忘门<br><img src="2.png"><br>记忆门<br><img src="3.png"><br>更新细胞状态<br><img src="4.png"><br>输出门:<br><img src="5.png"></p><h4 id="模型搭建"><a href="#模型搭建" class="headerlink" title="模型搭建"></a>模型搭建</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">class</span> <span class="token class-name">RNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">,</span> n_layers<span class="token punctuation">,</span>                  bidirectional<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span> pad_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># 词嵌入</span>        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> padding_idx <span class="token operator">=</span> pad_idx<span class="token punctuation">)</span>                <span class="token comment"># 双向LSTM</span>        self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">,</span>  <span class="token comment"># input_size</span>                           hidden_dim<span class="token punctuation">,</span>  <span class="token comment">#output_size</span>                           num_layers<span class="token operator">=</span>n_layers<span class="token punctuation">,</span>  <span class="token comment"># 层数</span>                           bidirectional<span class="token operator">=</span>bidirectional<span class="token punctuation">,</span> <span class="token comment">#是否双向</span>                           dropout<span class="token operator">=</span>dropout<span class="token punctuation">)</span> <span class="token comment">#随机去除神经元</span>        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span> <span class="token comment"># 因为前向传播+后向传播有两个hidden sate,且合并在一起,所以乘以2</span>        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>            <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">,</span> text_lengths<span class="token punctuation">)</span><span class="token punctuation">:</span>        embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">)</span>        packed_embedded <span class="token operator">=</span> nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>pack_padded_sequence<span class="token punctuation">(</span>embedded<span class="token punctuation">,</span> text_lengths<span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">'cpu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        packed_output<span class="token punctuation">,</span> <span class="token punctuation">(</span>hidden<span class="token punctuation">,</span> cell<span class="token punctuation">)</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>packed_embedded<span class="token punctuation">)</span>        hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>hidden<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> hidden<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>hidden<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="实例化模型"><a href="#实例化模型" class="headerlink" title="实例化模型"></a>实例化模型</h4><p>（由于我是GTX1660Ti的显卡，在训练过程中出现问题，所以将教程中的超参数HIDDEN_DIM 改为了128）</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">INPUT_DIM <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span> <span class="token comment"># 250002: 之前设置的只取25000个最频繁的词,加上pad_token和unknown token</span>EMBEDDING_DIM <span class="token operator">=</span> <span class="token number">100</span>HIDDEN_DIM <span class="token operator">=</span> <span class="token number">128</span> <span class="token comment">#原教程为256</span>OUTPUT_DIM <span class="token operator">=</span> <span class="token number">1</span>N_LAYERS <span class="token operator">=</span> <span class="token number">2</span>BIDIRECTIONAL <span class="token operator">=</span> <span class="token boolean">True</span>DROPOUT <span class="token operator">=</span> <span class="token number">0.5</span>PAD_IDX <span class="token operator">=</span> TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>stoi<span class="token punctuation">[</span>TEXT<span class="token punctuation">.</span>pad_token<span class="token punctuation">]</span> <span class="token comment">#指定参数,定义pad_token的index索引值,让模型不管pad token</span>model <span class="token operator">=</span> RNN<span class="token punctuation">(</span>INPUT_DIM<span class="token punctuation">,</span>             EMBEDDING_DIM<span class="token punctuation">,</span>             HIDDEN_DIM<span class="token punctuation">,</span>             OUTPUT_DIM<span class="token punctuation">,</span>             N_LAYERS<span class="token punctuation">,</span>             BIDIRECTIONAL<span class="token punctuation">,</span>             DROPOUT<span class="token punctuation">,</span>             PAD_IDX<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h4><p>这里的优化器更改为Adam，损失函数还是BCEWithLogitsLoss()</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optimoptimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCEWithLogitsLoss<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 损失函数. criterion 在本task中时损失函数的意思</span>model <span class="token operator">=</span> model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>criterion <span class="token operator">=</span> criterion<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><em>准确率</em></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">binary_accuracy</span><span class="token punctuation">(</span>preds<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>    rounded_preds <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">round</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>preds<span class="token punctuation">)</span><span class="token punctuation">)</span>    correct <span class="token operator">=</span> <span class="token punctuation">(</span>rounded_preds <span class="token operator">==</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#convert into float for division </span>    acc <span class="token operator">=</span> correct<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>correct<span class="token punctuation">)</span>    <span class="token keyword">return</span> acc<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><em>训练函数与评估函数</em></li></ul><p>这里的训练函数和评估函数也与之前类似，不再贴代码</p><ul><li><em>验证</em></li></ul><blockquote><p>“predict_sentiment”函数的作用如下：<br>将模型切换为evaluate模式<br>对句子进行分词操作<br>将分词后的每个词，对应着词汇表，转换成对应的index索引，<br>获取句子的长度<br>将indexes，从list转化成tensor<br>通过unsqueezing 添加一个batch维度<br>将length转化成张量tensor<br>用sigmoid函数将预测值压缩到0-1之间<br>用item（）方法，将只有一个值的张量tensor转化成整数</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> spacynlp <span class="token operator">=</span> spacy<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'en_core_web_sm'</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">predict_sentiment</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>    model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    tokenized <span class="token operator">=</span> <span class="token punctuation">[</span>tok<span class="token punctuation">.</span>text <span class="token keyword">for</span> tok <span class="token keyword">in</span> nlp<span class="token punctuation">.</span>tokenizer<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span><span class="token punctuation">]</span>    indexed <span class="token operator">=</span> <span class="token punctuation">[</span>TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>stoi<span class="token punctuation">[</span>t<span class="token punctuation">]</span> <span class="token keyword">for</span> t <span class="token keyword">in</span> tokenized<span class="token punctuation">]</span>    length <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>indexed<span class="token punctuation">)</span><span class="token punctuation">]</span>    tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>indexed<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>    tensor <span class="token operator">=</span> tensor<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    length_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>length<span class="token punctuation">)</span>    prediction <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>model<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> length_tensor<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> prediction<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><p>贴一张训练图片<br><img src="RNN%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C.jpg"></p><h2 id="task3：Faster-情感分析"><a href="#task3：Faster-情感分析" class="headerlink" title="task3：Faster 情感分析"></a>task3：Faster 情感分析</h2><h3 id="有关fastText"><a href="#有关fastText" class="headerlink" title="有关fastText"></a>有关fastText</h3><ul><li>fastText为每个n字符的gram训练一个向量表示，其中包括词、拼错的词、词片段甚至是单个字符。</li><li>fastText 模型架构和 Word2Vec 的 CBOW 模型类似 。和CBOW一样，fastText模型也只有三层：输入层、隐含层、输出层（Hierarchical Softmax），输入都是多个经向量表示的单词，输出都是一个特定的target，隐含层都是对多个词向量的叠加平均。不同的是，CBOW的输入是目标单词的上下文，fastText的输入是多个单词及其n-gram特征，这些特征用来表示单个文档；CBOW的输入单词被onehot编码过，fastText的输入特征是被embedding过；CBOW的输出是目标词汇，fastText的输出是文档对应的类标。fastText在输入时，将单词的字符级别的n-gram向量作为额外的特征；在输出时，fastText采用了分层Softmax，大大降低了模型训练时间。</li></ul><h3 id="数据预处理-1"><a href="#数据预处理-1" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>FastText分类模型与其他文本分类模型最大的不同之处在于其计算了输入句子的n-gram，并将n-gram作为一种附加特征来获取局部词序特征信息添加至标记化列表的末尾。<br>本次task使用bi-grams。即字节大小为2</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">generate_bigrams</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    n_grams <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">[</span>x<span class="token punctuation">[</span>i<span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> n_gram <span class="token keyword">in</span> n_grams<span class="token punctuation">:</span>        x<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>n_gram<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其他像导入包和加载数据构建迭代器等的步骤同上，不再贴代码~</p><h3 id="构建模型-2"><a href="#构建模型-2" class="headerlink" title="构建模型"></a>构建模型</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">class</span> <span class="token class-name">FastText</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">,</span> pad_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> padding_idx<span class="token operator">=</span>pad_idx<span class="token punctuation">)</span>                self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span>            <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>                      embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>text<span class="token punctuation">)</span>               embedded <span class="token operator">=</span> embedded<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        pooled <span class="token operator">=</span> F<span class="token punctuation">.</span>avg_pool2d<span class="token punctuation">(</span>embedded<span class="token punctuation">,</span> <span class="token punctuation">(</span>embedded<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>         <span class="token keyword">return</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>pooled<span class="token punctuation">)</span>INPUT_DIM <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span>EMBEDDING_DIM <span class="token operator">=</span> <span class="token number">100</span>OUTPUT_DIM <span class="token operator">=</span> <span class="token number">1</span>PAD_IDX <span class="token operator">=</span> TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>stoi<span class="token punctuation">[</span>TEXT<span class="token punctuation">.</span>pad_token<span class="token punctuation">]</span>model <span class="token operator">=</span> FastText<span class="token punctuation">(</span>INPUT_DIM<span class="token punctuation">,</span> EMBEDDING_DIM<span class="token punctuation">,</span> OUTPUT_DIM<span class="token punctuation">,</span> PAD_IDX<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><p>基本也是与之前相同，不赘述</p><h3 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h3><p>贴一个结果图,<br><img src="fastText.jpg"></p><p>fastText相对上一次任务的双向LSTM来说，速度快了不少，由于使用了一些特有的技巧，训练效果也得到了提升。</p><h2 id="task4：卷积情感分析"><a href="#task4：卷积情感分析" class="headerlink" title="task4：卷积情感分析"></a>task4：卷积情感分析</h2><p>呕吼~一直以为卷积只是用来处理图片的，没想到还能用来处理文本<br>卷积神经网络能够从局部输入图像块中提取特征，并能将表示模块化，同时可以高效第利用数据;可以用于处理时序数据，时间可以被看作一个空间维度，就像二维图像的高度和宽度<br>至于为什么可以在文本上使用卷积神经网络，原因如下：</p><ul><li>与3x3 filter可以查看图像块的方式相同，1x2 filter 可以查看一段文本中的两个连续单词，即双字符</li><li>本模型将使用多个不同大小的filter，这些filter将查看文本中的bi-grams（a 1x2 filter）、tri-grams（a 1x3 filter）and/or n-grams（a 1xn filter）。</li></ul><h3 id="数据预处理-2"><a href="#数据预处理-2" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>这里与fastText不同，不需要再创建bi-gram</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torchtext<span class="token punctuation">.</span>legacy <span class="token keyword">import</span> data<span class="token keyword">from</span> torchtext<span class="token punctuation">.</span>legacy <span class="token keyword">import</span> datasets<span class="token keyword">import</span> random<span class="token keyword">import</span> numpy <span class="token keyword">as</span> npSEED <span class="token operator">=</span> <span class="token number">1234</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span>torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span>torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>deterministic <span class="token operator">=</span> <span class="token boolean">True</span>TEXT <span class="token operator">=</span> data<span class="token punctuation">.</span>Field<span class="token punctuation">(</span>tokenize <span class="token operator">=</span> <span class="token string">'spacy'</span><span class="token punctuation">,</span>                   tokenizer_language <span class="token operator">=</span> <span class="token string">'en_core_web_sm'</span><span class="token punctuation">,</span>                  batch_first <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>LABEL <span class="token operator">=</span> data<span class="token punctuation">.</span>LabelField<span class="token punctuation">(</span>dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>train_data<span class="token punctuation">,</span> test_data <span class="token operator">=</span> datasets<span class="token punctuation">.</span>IMDB<span class="token punctuation">.</span>splits<span class="token punctuation">(</span>TEXT<span class="token punctuation">,</span> LABEL<span class="token punctuation">)</span>train_data<span class="token punctuation">,</span> valid_data <span class="token operator">=</span> train_data<span class="token punctuation">.</span>split<span class="token punctuation">(</span>random_state <span class="token operator">=</span> random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span><span class="token punctuation">)</span>MAX_VOCAB_SIZE <span class="token operator">=</span> 25_000TEXT<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">,</span>                  max_size <span class="token operator">=</span> MAX_VOCAB_SIZE<span class="token punctuation">,</span>                  vectors <span class="token operator">=</span> <span class="token string">"glove.6B.100d"</span><span class="token punctuation">,</span>                  unk_init <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">.</span>normal_<span class="token punctuation">)</span>LABEL<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="构建模型-3"><a href="#构建模型-3" class="headerlink" title="构建模型"></a>构建模型</h3><h4 id="有关TextCNN"><a href="#有关TextCNN" class="headerlink" title="有关TextCNN"></a>有关TextCNN</h4><blockquote><p>卷积神经网络的核心思想是捕捉局部特征，对于文本来说，局部特征就是由若干单词组成的滑动窗口，类似于N-gram。卷积神经网络的优势在于能够自动地对N-gram特征进行组合和筛选，获得不同抽象层次的语义信息。</p></blockquote><p>具体的笔记再补~参考文献中的那个 <a href="https://zhuanlan.zhihu.com/p/77634533">深入TextCNN（一）详述CNN及TextCNN原理</a> 感觉讲的很好呀</p><h4 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">class</span> <span class="token class-name">CNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> n_filters<span class="token punctuation">,</span> filter_sizes<span class="token punctuation">,</span> output_dim<span class="token punctuation">,</span>                  dropout<span class="token punctuation">,</span> pad_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> padding_idx <span class="token operator">=</span> pad_idx<span class="token punctuation">)</span>                self<span class="token punctuation">.</span>conv_0 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>                                 out_channels <span class="token operator">=</span> n_filters<span class="token punctuation">,</span>                                 kernel_size <span class="token operator">=</span> <span class="token punctuation">(</span>filter_sizes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>conv_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>                                 out_channels <span class="token operator">=</span> n_filters<span class="token punctuation">,</span>                                 kernel_size <span class="token operator">=</span> <span class="token punctuation">(</span>filter_sizes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>conv_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>                                 out_channels <span class="token operator">=</span> n_filters<span class="token punctuation">,</span>                                 kernel_size <span class="token operator">=</span> <span class="token punctuation">(</span>filter_sizes<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>filter_sizes<span class="token punctuation">)</span> <span class="token operator">*</span> n_filters<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span>                self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>            <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>                        <span class="token comment">#text = [batch size, sent len]</span>                embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>text<span class="token punctuation">)</span>                        <span class="token comment">#embedded = [batch size, sent len, emb dim]</span>                embedded <span class="token operator">=</span> embedded<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>                <span class="token comment">#embedded = [batch size, 1, sent len, emb dim]</span>                conved_0 <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv_0<span class="token punctuation">(</span>embedded<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        conved_1 <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv_1<span class="token punctuation">(</span>embedded<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        conved_2 <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv_2<span class="token punctuation">(</span>embedded<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                    <span class="token comment">#conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]</span>                pooled_0 <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool1d<span class="token punctuation">(</span>conved_0<span class="token punctuation">,</span> conved_0<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>        pooled_1 <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool1d<span class="token punctuation">(</span>conved_1<span class="token punctuation">,</span> conved_1<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>        pooled_2 <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool1d<span class="token punctuation">(</span>conved_2<span class="token punctuation">,</span> conved_2<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>                <span class="token comment">#pooled_n = [batch size, n_filters]</span>                cat <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>pooled_0<span class="token punctuation">,</span> pooled_1<span class="token punctuation">,</span> pooled_2<span class="token punctuation">)</span><span class="token punctuation">,</span> dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment">#cat = [batch size, n_filters * len(filter_sizes)]</span>                    <span class="token keyword">return</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>cat<span class="token punctuation">)</span>INPUT_DIM <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span>EMBEDDING_DIM <span class="token operator">=</span> <span class="token number">100</span>N_FILTERS <span class="token operator">=</span> <span class="token number">100</span>FILTER_SIZES <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">]</span>OUTPUT_DIM <span class="token operator">=</span> <span class="token number">1</span>DROPOUT <span class="token operator">=</span> <span class="token number">0.5</span>PAD_IDX <span class="token operator">=</span> TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>stoi<span class="token punctuation">[</span>TEXT<span class="token punctuation">.</span>pad_token<span class="token punctuation">]</span>model <span class="token operator">=</span> CNN<span class="token punctuation">(</span>INPUT_DIM<span class="token punctuation">,</span> EMBEDDING_DIM<span class="token punctuation">,</span> N_FILTERS<span class="token punctuation">,</span> FILTER_SIZES<span class="token punctuation">,</span> OUTPUT_DIM<span class="token punctuation">,</span> DROPOUT<span class="token punctuation">,</span> PAD_IDX<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>后续的任务还是与之前相同，不再赘述。</p><h3 id="小结-3"><a href="#小结-3" class="headerlink" title="小结"></a>小结</h3><h2 id="task5：多类型情感分析"><a href="#task5：多类型情感分析" class="headerlink" title="task5：多类型情感分析"></a>task5：多类型情感分析</h2><p>进行多分类的情感分析任务时，输出不再是正面负面两个标量了，而是一个C维向量。</p><h3 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h3><p>这次换了一个新的数据集，使用的是TREC数据集而不是IMDB数据集：训练集：5452，测试集：500。</p><ul><li>加载数据：</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torchtext <span class="token keyword">import</span> data<span class="token keyword">from</span> torchtext <span class="token keyword">import</span> datasets<span class="token keyword">import</span> randomSEED <span class="token operator">=</span> <span class="token number">1234</span>torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span>torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>deterministic <span class="token operator">=</span> <span class="token boolean">True</span>TEXT <span class="token operator">=</span> data<span class="token punctuation">.</span>Field<span class="token punctuation">(</span>tokenize <span class="token operator">=</span> <span class="token string">'spacy'</span><span class="token punctuation">)</span>LABEL <span class="token operator">=</span> data<span class="token punctuation">.</span>LabelField<span class="token punctuation">(</span><span class="token punctuation">)</span>train_data<span class="token punctuation">,</span> test_data <span class="token operator">=</span> datasets<span class="token punctuation">.</span>TREC<span class="token punctuation">.</span>splits<span class="token punctuation">(</span>TEXT<span class="token punctuation">,</span> LABEL<span class="token punctuation">,</span> fine_grained<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>train_data<span class="token punctuation">,</span> valid_data <span class="token operator">=</span> train_data<span class="token punctuation">.</span>split<span class="token punctuation">(</span>random_state <span class="token operator">=</span> random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">vars</span><span class="token punctuation">(</span>train_data<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>接下来，我们将构建词汇表。 由于这个数据集很小（只有约 3800 个训练样本），它的词汇量也非常小（约 7500 个不同单词，即one-hot向量为7500维），这意味着我们不需要像以前一样在词汇表上设置“max_size”。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">MAX_VOCAB_SIZE <span class="token operator">=</span> 25_000TEXT<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">,</span>                  max_size <span class="token operator">=</span> MAX_VOCAB_SIZE<span class="token punctuation">,</span>                  vectors <span class="token operator">=</span> <span class="token string">"glove.6B.100d"</span><span class="token punctuation">,</span>                  unk_init <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">.</span>normal_<span class="token punctuation">)</span>LABEL<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>检查标签</li></ul><blockquote><p>6 个标签（对于非细粒度情况）对应于数据集中的 6 类问题：<br>HUM：关于人类的问题<br>ENTY：关于实体的问题的<br>DESC：关于要求提供描述的问题<br>NUM：关于答案为数字的问题<br>LOC：关于答案是位置的问题<br>ABBR：关于询问缩写的问题</p></blockquote><h3 id="构建模型-4"><a href="#构建模型-4" class="headerlink" title="构建模型"></a>构建模型</h3><p>这次使用Task04中的CNN模型，将之前模型中的 output_dim 改为 𝐶 维而不是 2 维即可，同时确保将输出维度： OUTPUT_DIM 设置为  𝐶 </p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">class</span> <span class="token class-name">CNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> n_filters<span class="token punctuation">,</span> filter_sizes<span class="token punctuation">,</span> output_dim<span class="token punctuation">,</span>                  dropout<span class="token punctuation">,</span> pad_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span>                self<span class="token punctuation">.</span>convs <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>                                    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>                                               out_channels <span class="token operator">=</span> n_filters<span class="token punctuation">,</span>                                               kernel_size <span class="token operator">=</span> <span class="token punctuation">(</span>fs<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>                                     <span class="token keyword">for</span> fs <span class="token keyword">in</span> filter_sizes                                    <span class="token punctuation">]</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>filter_sizes<span class="token punctuation">)</span> <span class="token operator">*</span> n_filters<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span>                self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>            <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>        text <span class="token operator">=</span> text<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>        embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>text<span class="token punctuation">)</span>        embedded <span class="token operator">=</span> embedded<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>        conved <span class="token operator">=</span> <span class="token punctuation">[</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>conv<span class="token punctuation">(</span>embedded<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span> <span class="token keyword">for</span> conv <span class="token keyword">in</span> self<span class="token punctuation">.</span>convs<span class="token punctuation">]</span>        pooled <span class="token operator">=</span> <span class="token punctuation">[</span>F<span class="token punctuation">.</span>max_pool1d<span class="token punctuation">(</span>conv<span class="token punctuation">,</span> conv<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token keyword">for</span> conv <span class="token keyword">in</span> conved<span class="token punctuation">]</span>        cat <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>pooled<span class="token punctuation">,</span> dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                    <span class="token keyword">return</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>cat<span class="token punctuation">)</span>INPUT_DIM <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span>EMBEDDING_DIM <span class="token operator">=</span> <span class="token number">100</span>N_FILTERS <span class="token operator">=</span> <span class="token number">100</span>FILTER_SIZES <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span>OUTPUT_DIM <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>LABEL<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span>DROPOUT <span class="token operator">=</span> <span class="token number">0.5</span>PAD_IDX <span class="token operator">=</span> TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>stoi<span class="token punctuation">[</span>TEXT<span class="token punctuation">.</span>pad_token<span class="token punctuation">]</span>model <span class="token operator">=</span> CNN<span class="token punctuation">(</span>INPUT_DIM<span class="token punctuation">,</span> EMBEDDING_DIM<span class="token punctuation">,</span> N_FILTERS<span class="token punctuation">,</span> FILTER_SIZES<span class="token punctuation">,</span> OUTPUT_DIM<span class="token punctuation">,</span> DROPOUT<span class="token punctuation">,</span> PAD_IDX<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>与前几个notebook比,损失函数(也就是criterion)是不同的.之前用的是BCEWithLogisLoss,现在使用的是CrossEntropyLoss,它使用的是softrmax函数,来计算cross entropy</p><p>一般来说:</p><ul><li>CrossEntropyLoss :用于多分类问题</li><li>BCEWithLogitsLoss :用于2分类问题,(0,1),也用于多标签分类(multilabel classification) 1vs rest</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optimoptimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>model <span class="token operator">=</span> model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>criterion <span class="token operator">=</span> criterion<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="小结-4"><a href="#小结-4" class="headerlink" title="小结"></a>小结</h3><p>贴一个结果图<br><img src="%E5%A4%9A%E5%88%86%E7%B1%BB.jpg"></p><h2 id="task6：使用Transformer进行情感分析"><a href="#task6：使用Transformer进行情感分析" class="headerlink" title="task6：使用Transformer进行情感分析"></a>task6：使用Transformer进行情感分析</h2><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><p>本次任务使用的是Bert模型，由于模型很大，参数很多，所以将固定（而不训练）transformer，只训练从transformer产生的表示中学习的模型的其余部分。<br>在这种情况下，我们将使用双向GRU继续提取从Bert embedding后的特征。最后在fc层上输出最终的结果。</p><ul><li><em>导入相关库，并设定随机种子</em></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> random<span class="token keyword">import</span> numpy <span class="token keyword">as</span> npSEED <span class="token operator">=</span> <span class="token number">1234</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span>torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span>torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>deterministic <span class="token operator">=</span> <span class="token boolean">True</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>transformers 库为每个提供的transformer 模型都有分词器。 在这种情况下，我们使用忽略大小写的 BERT 模型（即每个单词都会小写）。 我们通过加载预训练的“bert-base-uncased”标记器来实现这一点。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> BertTokenizertokenizer <span class="token operator">=</span> BertTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'bert-base-uncased'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>使用tokenizer.tokenize方法对字符串进行分词，并统一大小写。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">tokens <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span><span class="token string">'Hello WORLD how ARE yoU?'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li><em>定义一个函数处理所有的标记化。</em></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">tokenize_and_cut</span><span class="token punctuation">(</span>sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>    tokens <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span>     tokens <span class="token operator">=</span> tokens<span class="token punctuation">[</span><span class="token punctuation">:</span>max_input_length<span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span>    <span class="token keyword">return</span> tokens<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ul><li><em>定义标签字段</em></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> torchtext<span class="token punctuation">.</span>legacy <span class="token keyword">import</span> dataTEXT <span class="token operator">=</span> data<span class="token punctuation">.</span>Field<span class="token punctuation">(</span>batch_first <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>                  use_vocab <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>                  tokenize <span class="token operator">=</span> tokenize_and_cut<span class="token punctuation">,</span>                  preprocessing <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>convert_tokens_to_ids<span class="token punctuation">,</span>                  init_token <span class="token operator">=</span> init_token_idx<span class="token punctuation">,</span>                  eos_token <span class="token operator">=</span> eos_token_idx<span class="token punctuation">,</span>                  pad_token <span class="token operator">=</span> pad_token_idx<span class="token punctuation">,</span>                  unk_token <span class="token operator">=</span> unk_token_idx<span class="token punctuation">)</span>LABEL <span class="token operator">=</span> data<span class="token punctuation">.</span>LabelField<span class="token punctuation">(</span>dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><em>拆分出训练集和验证集</em></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> torchtext<span class="token punctuation">.</span>legacy <span class="token keyword">import</span> datasetstrain_data<span class="token punctuation">,</span> test_data <span class="token operator">=</span> datasets<span class="token punctuation">.</span>IMDB<span class="token punctuation">.</span>splits<span class="token punctuation">(</span>TEXT<span class="token punctuation">,</span> LABEL<span class="token punctuation">)</span>train_data<span class="token punctuation">,</span> valid_data <span class="token operator">=</span> train_data<span class="token punctuation">.</span>split<span class="token punctuation">(</span>random_state <span class="token operator">=</span> random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><em>创建迭代器</em></li></ul><p>这里依旧是不能用教程里面的参数128，自己训练的时候改为了64</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">BATCH_SIZE <span class="token operator">=</span> <span class="token number">128</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">'cpu'</span><span class="token punctuation">)</span>train_iterator<span class="token punctuation">,</span> valid_iterator<span class="token punctuation">,</span> test_iterator <span class="token operator">=</span> data<span class="token punctuation">.</span>BucketIterator<span class="token punctuation">.</span>splits<span class="token punctuation">(</span>    <span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> valid_data<span class="token punctuation">,</span> test_data<span class="token punctuation">)</span><span class="token punctuation">,</span>     batch_size <span class="token operator">=</span> BATCH_SIZE<span class="token punctuation">,</span>     device <span class="token operator">=</span> device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="构建模型-5"><a href="#构建模型-5" class="headerlink" title="构建模型"></a>构建模型</h3><ul><li><em>导入预训练的bert</em></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> BertTokenizer<span class="token punctuation">,</span> BertModelbert <span class="token operator">=</span> BertModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'bert-base-uncased'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li><em>定义模型</em></li></ul><blockquote><p>*我们将使用预训练的Transformer模型，而不是使用embedding层来获取文本的embedding。然后将这些embedding输入GRU以生成对句子情绪的预测。<br>*通过config属性从transformer中获取嵌入维度大小（称为hidden_size）,其余初始化是标准的<br>*在前向传递的过程中，我们将transformer包装在一个no_grad中，以确保不会再模型的这部分计算梯度。<br>*transformer实际上返回整个序列的embedding以及pooled输入。<br>*前向传递的其余部分是循环模型的标准实现，我们在最后的时间步长中获取隐藏状态，并将其传递给一个线性层以获得我们的预测。</p></blockquote><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">class</span> <span class="token class-name">BERTGRUSentiment</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>                 bert<span class="token punctuation">,</span>                 hidden_dim<span class="token punctuation">,</span>                 output_dim<span class="token punctuation">,</span>                 n_layers<span class="token punctuation">,</span>                 bidirectional<span class="token punctuation">,</span>                 dropout<span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>bert <span class="token operator">=</span> bert                embedding_dim <span class="token operator">=</span> bert<span class="token punctuation">.</span>config<span class="token punctuation">.</span>to_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'hidden_size'</span><span class="token punctuation">]</span>                self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">,</span>                          hidden_dim<span class="token punctuation">,</span>                          num_layers <span class="token operator">=</span> n_layers<span class="token punctuation">,</span>                          bidirectional <span class="token operator">=</span> bidirectional<span class="token punctuation">,</span>                          batch_first <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>                          dropout <span class="token operator">=</span> <span class="token number">0</span> <span class="token keyword">if</span> n_layers <span class="token operator">&lt;</span> <span class="token number">2</span> <span class="token keyword">else</span> dropout<span class="token punctuation">)</span>                self<span class="token punctuation">.</span>out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim <span class="token operator">*</span> <span class="token number">2</span> <span class="token keyword">if</span> bidirectional <span class="token keyword">else</span> hidden_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span>                self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>            <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>                       <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>bert<span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>                        _<span class="token punctuation">,</span> hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>embedded<span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>bidirectional<span class="token punctuation">:</span>            hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>hidden<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> hidden<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>hidden<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>               output <span class="token operator">=</span> self<span class="token punctuation">.</span>out<span class="token punctuation">(</span>hidden<span class="token punctuation">)</span>        <span class="token keyword">return</span> output<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><em>创建实例</em></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">HIDDEN_DIM <span class="token operator">=</span> <span class="token number">256</span>OUTPUT_DIM <span class="token operator">=</span> <span class="token number">1</span>N_LAYERS <span class="token operator">=</span> <span class="token number">2</span>BIDIRECTIONAL <span class="token operator">=</span> <span class="token boolean">True</span>DROPOUT <span class="token operator">=</span> <span class="token number">0.25</span>model <span class="token operator">=</span> BERTGRUSentiment<span class="token punctuation">(</span>bert<span class="token punctuation">,</span>                         HIDDEN_DIM<span class="token punctuation">,</span>                         OUTPUT_DIM<span class="token punctuation">,</span>                         N_LAYERS<span class="token punctuation">,</span>                         BIDIRECTIONAL<span class="token punctuation">,</span>                         DROPOUT<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="训练和评估"><a href="#训练和评估" class="headerlink" title="训练和评估"></a>训练和评估</h3><ul><li><em>定义损失函数</em></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optimoptimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCEWithLogitsLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>model <span class="token operator">=</span> model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>criterion <span class="token operator">=</span> criterion<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><em>构建训练函数</em></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> iterator<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span><span class="token punctuation">:</span>        epoch_loss <span class="token operator">=</span> <span class="token number">0</span>    epoch_acc <span class="token operator">=</span> <span class="token number">0</span>        model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> batch <span class="token keyword">in</span> iterator<span class="token punctuation">:</span>                optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>                predictions <span class="token operator">=</span> model<span class="token punctuation">(</span>batch<span class="token punctuation">.</span>text<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>                loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>                acc <span class="token operator">=</span> binary_accuracy<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>                loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>                optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>                epoch_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        epoch_acc <span class="token operator">+=</span> acc<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">return</span> epoch_loss <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span><span class="token punctuation">,</span> epoch_acc <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><em>评估函数</em></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">evaluate</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> iterator<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span><span class="token punctuation">:</span>        epoch_loss <span class="token operator">=</span> <span class="token number">0</span>    epoch_acc <span class="token operator">=</span> <span class="token number">0</span>        model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">for</span> batch <span class="token keyword">in</span> iterator<span class="token punctuation">:</span>            predictions <span class="token operator">=</span> model<span class="token punctuation">(</span>batch<span class="token punctuation">.</span>text<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>                        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>                        acc <span class="token operator">=</span> binary_accuracy<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>            epoch_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>            epoch_acc <span class="token operator">+=</span> acc<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">return</span> epoch_loss <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span><span class="token punctuation">,</span> epoch_acc <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><em>训练</em></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">N_EPOCHS <span class="token operator">=</span> <span class="token number">5</span>best_valid_loss <span class="token operator">=</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token string">'inf'</span><span class="token punctuation">)</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>N_EPOCHS<span class="token punctuation">)</span><span class="token punctuation">:</span>        start_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>        train_loss<span class="token punctuation">,</span> train_acc <span class="token operator">=</span> train<span class="token punctuation">(</span>model<span class="token punctuation">,</span> train_iterator<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span>    valid_loss<span class="token punctuation">,</span> valid_acc <span class="token operator">=</span> evaluate<span class="token punctuation">(</span>model<span class="token punctuation">,</span> valid_iterator<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span>            end_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>            epoch_mins<span class="token punctuation">,</span> epoch_secs <span class="token operator">=</span> epoch_time<span class="token punctuation">(</span>start_time<span class="token punctuation">,</span> end_time<span class="token punctuation">)</span>            <span class="token keyword">if</span> valid_loss <span class="token operator">&lt;</span> best_valid_loss<span class="token punctuation">:</span>        best_valid_loss <span class="token operator">=</span> valid_loss        torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'tut6-model.pt'</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Epoch: </span><span class="token interpolation"><span class="token punctuation">{</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token format-spec">02</span><span class="token punctuation">}</span></span><span class="token string"> | Epoch Time: </span><span class="token interpolation"><span class="token punctuation">{</span>epoch_mins<span class="token punctuation">}</span></span><span class="token string">m </span><span class="token interpolation"><span class="token punctuation">{</span>epoch_secs<span class="token punctuation">}</span></span><span class="token string">s'</span></span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'\tTrain Loss: </span><span class="token interpolation"><span class="token punctuation">{</span>train_loss<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string"> | Train Acc: </span><span class="token interpolation"><span class="token punctuation">{</span>train_acc<span class="token operator">*</span><span class="token number">100</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string">%'</span></span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'\t Val. Loss: </span><span class="token interpolation"><span class="token punctuation">{</span>valid_loss<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string"> |  Val. Acc: </span><span class="token interpolation"><span class="token punctuation">{</span>valid_acc<span class="token operator">*</span><span class="token number">100</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string">%'</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="小结-5"><a href="#小结-5" class="headerlink" title="小结"></a>小结</h3><p>训练速度很慢，BATCH_SIZE依旧是不能用教程里面的128，改为64之后训练结束~<br>贴一张训练图<br><img src="9.png"><br>\[y = {f_{ {g_1}}}(x)\]<br>参考链接：<a href="https://zhuanlan.zhihu.com/p/31139113">torchtext入门教程，轻松玩转文本数据处理</a><br>        <a href="https://zhuanlan.zhihu.com/p/104475016">[干货]深入浅出LSTM及其Python代码实现</a><br>        <a href="https://zhuanlan.zhihu.com/p/32965521">fastText原理及实践</a><br>        <a href="https://blog.csdn.net/feilong_csdn/article/details/88655927">fastText原理和文本分类实战，看这一篇就够了</a><br>        <a href="https://zhuanlan.zhihu.com/p/77634533">深入TextCNN（一）详述CNN及TextCNN原理</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
