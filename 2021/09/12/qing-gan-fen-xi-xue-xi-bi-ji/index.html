<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="情感分析学习笔记, Coding with yali">
    <meta name="description" content="">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>情感分析学习笔记 | Coding with yali</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>
    <script src="https://sdk.jinrishici.com/v2/browser/jinrishici.js" charset="utf-8"></script>

<meta name="generator" content="Hexo 5.4.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head>






<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Coding with yali</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Coding with yali</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/blinkfox/hexo-theme-matery" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/blinkfox/hexo-theme-matery" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">情感分析学习笔记</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                          <div class="article-tag">
                            <span class="chip bg-color">无标签</span>
                          </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2021-09-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2021-10-01
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    5.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    25 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <p>记录datawhale九月份的学习，地址：<a target="_blank" rel="noopener" href="https://github.com/datawhalechina/team-learning-nlp/tree/master/Emotional_Analysis">情感分析</a></p>
<h2 id="Task0：自然语言处理之PyTorch情感分析简介"><a href="#Task0：自然语言处理之PyTorch情感分析简介" class="headerlink" title="Task0：自然语言处理之PyTorch情感分析简介"></a>Task0：自然语言处理之PyTorch情感分析简介</h2><p>   从来没有接触过情感分析，一切都要从零开始，这次大航海的教程好像都是实战为主的，有点怀疑自己能不能跟得上。先记录一下task0遇到的问题。<br><strong>问题</strong><br>环境配置按照教程的问题不大，进行到以下语句的时候出现了 <em>‘[WinError 10061] 由于目标计算机积极拒绝，无法连接’</em> 的问题</p>
<pre class="line-numbers language-none"><code class="language-none">python -m spacy download zh_core_web_sm
python -m spacy download en_core_web_sm<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p><strong>解决办法</strong><br>搜解决办法好多需要改防火墙设置什么的，而且不怎么奏效，索性直接选择了离线安装的办法。</p>
<ul>
<li>zh_core_web_sm下载地址：<br><a target="_blank" rel="noopener" href="https://github.com/explosion/spacy-models/releases/tag/zh_core_web_sm-3.1.0">https://github.com/explosion/spacy-models/releases/tag/zh_core_web_sm-3.1.0</a></li>
<li>en_core_web_sm下载地址：<br><a target="_blank" rel="noopener" href="https://github.com/explosion/spacy-models/releases/tag/en_core_web_sm-3.1.0">https://github.com/explosion/spacy-models/releases/tag/en_core_web_sm-3.1.0</a></li>
</ul>
<p>下载完成后运行即可</p>
<pre class="line-numbers language-none"><code class="language-none">pip install zh_core_web_sm-3.1.0.tar.gz
pip install en_core_web_sm-3.1.0.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p><strong>插曲</strong><br>记录一下不应该成为问题的问题：中间安装环境的时候，出现了一大片红色（真的好久没有见过这么大片的红），并提示<code>ValueError: check_hostname  requires server_hostname</code> 的错误，关闭科学上网工具后解决。fine~</p>
<h2 id="Task1-情感分析baseline"><a href="#Task1-情感分析baseline" class="headerlink" title="Task1:情感分析baseline"></a>Task1:情感分析baseline</h2><p>任务简介：使用pytorch和torchtext构造一个简单的模型预测情绪（正面或负面）<br>数据集：IMDb数据集</p>
<h3 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h3><h4 id="1-torchtext"><a href="#1-torchtext" class="headerlink" title="1.torchtext"></a>1.torchtext</h4><p>torchtext包含以下组件：</p>
<ul>
<li>Field :主要包含以下数据预处理的配置信息，比如指定分词方法，是否转成小写，起始字符，结束字符，补全字符以及词典等等。此次教程中需要指定一个 tokenizer_language 来告诉 torchtext 使用哪个 spaCy 模型。我们使用 en_core_web_sm 模型。<pre class="line-numbers language-none"><code class="language-none">en_core_web_sm模型的下载地址：https://github.com/explosion/spacy-models/releases/tag/en_core_web_sm-3.1.0
安装时运行 pip install en_core_web_sm-3.1.0.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li>
<li>Dataset :继承自pytorch的Dataset，用于加载数据，提供了TabularDataset可以指点路径，格式，Field信息就可以方便的完成数据加载。同时torchtext还提供预先构建的常用数据集的Dataset对象，可以直接加载使用，splits方法可以同时加载训练集，验证集和测试集。</li>
<li>Iterator : 主要是数据输出的模型的迭代器，可以支持batch定制</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torchtext<span class="token punctuation">.</span>legacy <span class="token keyword">import</span> data

<span class="token comment"># 设置随机种子数，该数可以保证随机数是可重复的</span>
SEED <span class="token operator">=</span> <span class="token number">1234</span>

<span class="token comment"># 设置种子</span>
torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>deterministic <span class="token operator">=</span> <span class="token boolean">True</span>

<span class="token comment"># 读取数据和标签</span>
TEXT <span class="token operator">=</span> data<span class="token punctuation">.</span>Field<span class="token punctuation">(</span>tokenize <span class="token operator">=</span> <span class="token string">'spacy'</span><span class="token punctuation">,</span> tokenizer_language <span class="token operator">=</span> <span class="token string">'en_core_web_sm'</span><span class="token punctuation">)</span>
LABEL <span class="token operator">=</span> data<span class="token punctuation">.</span>LabelField<span class="token punctuation">(</span>dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>

<span class="token comment"># 下载 IMDb 数据集</span>
<span class="token keyword">from</span> torchtext<span class="token punctuation">.</span>legacy <span class="token keyword">import</span> datasets
train_data<span class="token punctuation">,</span> test_data <span class="token operator">=</span> datasets<span class="token punctuation">.</span>IMDB<span class="token punctuation">.</span>splits<span class="token punctuation">(</span>TEXT<span class="token punctuation">,</span> LABEL<span class="token punctuation">)</span>

<span class="token comment"># 查看训练集和测试集的大小</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Number of training examples: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>train_data<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Number of testing examples: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>test_data<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>

<span class="token comment"># 查看其中一个示例数据</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">vars</span><span class="token punctuation">(</span>train_data<span class="token punctuation">.</span>examples<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="2-划分数据集"><a href="#2-划分数据集" class="headerlink" title="2.划分数据集"></a>2.划分数据集</h4><p>IMDb 数据集划分了训练集和测试集，这里我们还需要创建一个验证集。 可以使用 .split() 方法来做。（ <strong>注：将之前设置的随机种子SEED传递给 random_state 参数，确保我们每次获得相同的训练集和验证集。</strong> ）</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> random
train_data<span class="token punctuation">,</span> valid_data <span class="token operator">=</span> train_data<span class="token punctuation">.</span>split<span class="token punctuation">(</span>split_ratio<span class="token operator">=</span><span class="token number">0.8</span> <span class="token punctuation">,</span> random_state <span class="token operator">=</span> random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 查看训练集，验证集和测试集分别有多少数据</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Number of training examples: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>train_data<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Number of validation examples: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>valid_data<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Number of testing examples: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>test_data<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="3-构建词汇表"><a href="#3-构建词汇表" class="headerlink" title="3.构建词汇表"></a>3.构建词汇表</h4><ul>
<li>由于模型不能直接对字符串进行操作，只能对数字进行操作，所以需要建立一下查找表，每个单词都有唯一对应的index。</li>
<li>onehot编码在此不适用（训练集中不同的单词数量巨大，训练的代价大），需要优化，优化方案有：只取前n个出现次数最多的单词作为one-hot的基，另一种是忽略出现次数小于m个的单词。（这里使用前者）</li>
<li>优化后的onehot会出现很多单词虽然在数据集中，但是无法编码的情况，这里还要引进两个特殊的符号来编码，一个是<unk>用来处理前面所说的问题，另一个是<pad>，用于句子的填充。<pre class="line-numbers language-python" data-language="python"><code class="language-python">MAX_VOCAB_SIZE <span class="token operator">=</span> <span class="token number">25000</span> <span class="token comment"># 词汇表的最大长度</span>

TEXT<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> max_size <span class="token operator">=</span> MAX_VOCAB_SIZE<span class="token punctuation">)</span>
LABEL<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Unique tokens in TEXT vocabulary: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Unique tokens in LABEL vocabulary: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>LABEL<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></pad></unk></li>
</ul>
<h4 id="4-创建迭代器"><a href="#4-创建迭代器" class="headerlink" title="4.创建迭代器"></a>4.创建迭代器</h4><ul>
<li>准备数据的最后一步是创建迭代器. 需要创建验证集，测试集，以及训练集的迭代器, 每一次的迭代都会返回一个batch的数据。</li>
<li>本例中使用“BucketIterator”，它将返回一批示例，其中每个样本的长度差不多，从而最小化每个样本的padding数。</li>
<li>用torch.device，可以将张量放到gpu或者cpu上。<pre class="line-numbers language-python" data-language="python"><code class="language-python">BATCH_SIZE <span class="token operator">=</span> <span class="token number">64</span>

device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">'cpu'</span><span class="token punctuation">)</span>

train_iterator<span class="token punctuation">,</span> valid_iterator<span class="token punctuation">,</span> test_iterator <span class="token operator">=</span> data<span class="token punctuation">.</span>BucketIterator<span class="token punctuation">.</span>splits<span class="token punctuation">(</span>
    <span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> valid_data<span class="token punctuation">,</span> test_data<span class="token punctuation">)</span><span class="token punctuation">,</span> 
    batch_size <span class="token operator">=</span> BATCH_SIZE<span class="token punctuation">,</span>
    device <span class="token operator">=</span> device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
</ul>
<h3 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h3><ul>
<li>首先关于初始化的问题，在某些框架中，使用RNN需要初始化 ℎ0 ，但在pytorch中不用，默认为全0。</li>
<li>构建一个RNN模型(nn.module子类)<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token keyword">class</span> <span class="token class-name">RNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_dim<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>
        
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>RNN<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>

        <span class="token comment">#text = [sent len, batch size]</span>
        
        embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
        
        <span class="token comment">#embedded = [sent len, batch size, emb dim]</span>
        
        output<span class="token punctuation">,</span> hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>embedded<span class="token punctuation">)</span>
        
        <span class="token comment">#output = [sent len, batch size, hid dim]</span>
        <span class="token comment">#hidden = [1, batch size, hid dim]</span>
        
        <span class="token keyword">assert</span> torch<span class="token punctuation">.</span>equal<span class="token punctuation">(</span>output<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> hidden<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>hidden<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li>设置超参数<pre class="line-numbers language-python" data-language="python"><code class="language-python">INPUT_DIM <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span>
EMBEDDING_DIM <span class="token operator">=</span> <span class="token number">100</span>
HIDDEN_DIM <span class="token operator">=</span> <span class="token number">256</span>
OUTPUT_DIM <span class="token operator">=</span> <span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li>
</ul>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><ul>
<li>设置优化器：SGD</li>
<li>定义损失函数：BCEWithLogitsLoss一般用来做二分类<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim

optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCEWithLogitsLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
criterion <span class="token operator">=</span> criterion<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li>设置训练函数和评估函数<br>训练函数和评估函数是相似的，评估时不需要再进行梯度计算，所以使用<code>with no_grad()</code>，去掉了<code>optimizer.zero_grad(), loss.backward() , optimizer.step()</code><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> iterator<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span><span class="token punctuation">:</span>
    
    epoch_loss <span class="token operator">=</span> <span class="token number">0</span>
    epoch_acc <span class="token operator">=</span> <span class="token number">0</span>
    
    model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token keyword">for</span> batch <span class="token keyword">in</span> iterator<span class="token punctuation">:</span>
        
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
                
        predictions <span class="token operator">=</span> model<span class="token punctuation">(</span>batch<span class="token punctuation">.</span>text<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        
        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>
        
        acc <span class="token operator">=</span> binary_accuracy<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>
        
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        epoch_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
        epoch_acc <span class="token operator">+=</span> acc<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
    <span class="token keyword">return</span> epoch_loss <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span><span class="token punctuation">,</span> epoch_acc <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">evaluate</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> iterator<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span><span class="token punctuation">:</span>
    
    epoch_loss <span class="token operator">=</span> <span class="token number">0</span>
    epoch_acc <span class="token operator">=</span> <span class="token number">0</span>
    
    model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    
        <span class="token keyword">for</span> batch <span class="token keyword">in</span> iterator<span class="token punctuation">:</span>

            predictions <span class="token operator">=</span> model<span class="token punctuation">(</span>batch<span class="token punctuation">.</span>text<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
            
            loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>
            
            acc <span class="token operator">=</span> binary_accuracy<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>

            epoch_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
            epoch_acc <span class="token operator">+=</span> acc<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
    <span class="token keyword">return</span> epoch_loss <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span><span class="token punctuation">,</span> epoch_acc <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li>最后保留在验证集上的损失值最小的模型<pre class="line-numbers language-python" data-language="python"><code class="language-python">N_EPOCHS <span class="token operator">=</span> <span class="token number">5</span>

best_valid_loss <span class="token operator">=</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token string">'inf'</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>N_EPOCHS<span class="token punctuation">)</span><span class="token punctuation">:</span>

    start_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    train_loss<span class="token punctuation">,</span> train_acc <span class="token operator">=</span> train<span class="token punctuation">(</span>model<span class="token punctuation">,</span> train_iterator<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span>
    valid_loss<span class="token punctuation">,</span> valid_acc <span class="token operator">=</span> evaluate<span class="token punctuation">(</span>model<span class="token punctuation">,</span> valid_iterator<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span>
    
    end_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>

    epoch_mins<span class="token punctuation">,</span> epoch_secs <span class="token operator">=</span> epoch_time<span class="token punctuation">(</span>start_time<span class="token punctuation">,</span> end_time<span class="token punctuation">)</span>
    
    <span class="token keyword">if</span> valid_loss <span class="token operator">&lt;</span> best_valid_loss<span class="token punctuation">:</span>
        best_valid_loss <span class="token operator">=</span> valid_loss
        torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'tut1-model.pt'</span><span class="token punctuation">)</span>
    
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Epoch: </span><span class="token interpolation"><span class="token punctuation">{</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token format-spec">02</span><span class="token punctuation">}</span></span><span class="token string"> | Epoch Time: </span><span class="token interpolation"><span class="token punctuation">{</span>epoch_mins<span class="token punctuation">}</span></span><span class="token string">m </span><span class="token interpolation"><span class="token punctuation">{</span>epoch_secs<span class="token punctuation">}</span></span><span class="token string">s'</span></span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'\tTrain Loss: </span><span class="token interpolation"><span class="token punctuation">{</span>train_loss<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string"> | Train Acc: </span><span class="token interpolation"><span class="token punctuation">{</span>train_acc<span class="token operator">*</span><span class="token number">100</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string">%'</span></span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'\t Val. Loss: </span><span class="token interpolation"><span class="token punctuation">{</span>valid_loss<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string"> |  Val. Acc: </span><span class="token interpolation"><span class="token punctuation">{</span>valid_acc<span class="token operator">*</span><span class="token number">100</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string">%'</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
</ul>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>贴一张训练结果图</p>
<p>实验结果的准确率很低，有可能是本身RNN是一个很简单的模型，做不了太复杂的任务，另外word_embedding这里使用的是onehot，也会影响模型准确率~</p>
<h2 id="Task2：Updated情感分析"><a href="#Task2：Updated情感分析" class="headerlink" title="Task2：Updated情感分析"></a>Task2：Updated情感分析</h2><p>此次任务使用的是双向LSTM网络，并修改了基础模型的词向量，改用Glove</p>
<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>准备数据阶段与上次任务相同，这里不再赘述</p>
<ul>
<li>词向量（Glove）<br>这次任务使用的是 “glove.6B.100d” ，其中，6B表示词向量是在60亿规模的tokens上训练得到的，100d表示词向量是100维的(注意,这个词向量有800多兆)<pre class="line-numbers language-python" data-language="python"><code class="language-python">MAX_VOCAB_SIZE <span class="token operator">=</span> 25_000

TEXT<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> 
                 max_size <span class="token operator">=</span> MAX_VOCAB_SIZE<span class="token punctuation">,</span> 
                 vectors <span class="token operator">=</span> <span class="token string">"glove.6B.100d"</span><span class="token punctuation">,</span> 
                 unk_init <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">.</span>normal_<span class="token punctuation">)</span>

LABEL<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li>迭代器与GPU设置也不再赘述</li>
</ul>
<h3 id="构建模型-1"><a href="#构建模型-1" class="headerlink" title="构建模型"></a>构建模型</h3><h4 id="有关LSTM"><a href="#有关LSTM" class="headerlink" title="有关LSTM"></a>有关LSTM</h4><p>RNN作为一种处理序列数据的神经网络，序列数据通常会有上下文之间的影响，RNN通过让上一时刻的隐藏层影响当前时刻的隐藏层来解决序列间关系的问题。但是RNN还存在一些漏洞，比如在长序列数据处理中存在梯度消失的问题，长序列处理中，梯度被近距离梯度主导，导致模型难以学到远距离的依赖关系。而LSTM则解决了此问题<br>LSTM的关键是细胞状态（直译：cell state），表示为$C_t$ ，用来保存当前LSTM的状态信息并传递到下一时刻的LSTM中，也就是RNN中那根“自循环”的箭头。当前的LSTM接收来自上一个时刻的细胞状态 $C_{t-1}$ ，并与当前LSTM接收的信号输入 $x_t$ 共同作用产生当前LSTM的细胞状态 $C_t$，具体的作用方式下面将详细介绍。<br><img src="1.png"><br>LSTM主要包括三个不同的门结构：遗忘门、记忆门和输出门。这三个门用来控制LSTM的信息保留和传递，最终反映到细胞状态 $C_t$和输出信$h_t$ 。<br>*遗忘门由一个sigmod神经网络层和一个按位乘操作构成；<br>*记忆门由输入门（input gate）与tanh神经网络层和一个按位乘操作构成；<br>*输出门（output gate）与 tanh函数（注意：这里不是tanh神经网络层）以及按位乘操作共同作用将细胞状态和输入信号传递到输出端。<br>遗忘门<br><img src="2.png"><br>记忆门<br><img src="3.png"><br>更新细胞状态<br><img src="4.png"><br>输出门:<br><img src="5.png"></p>
<h4 id="模型搭建"><a href="#模型搭建" class="headerlink" title="模型搭建"></a>模型搭建</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token keyword">class</span> <span class="token class-name">RNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">,</span> n_layers<span class="token punctuation">,</span> 
                 bidirectional<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span> pad_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
        
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 词嵌入</span>
        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> padding_idx <span class="token operator">=</span> pad_idx<span class="token punctuation">)</span>
        
        <span class="token comment"># 双向LSTM</span>
        self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">,</span>  <span class="token comment"># input_size</span>
                           hidden_dim<span class="token punctuation">,</span>  <span class="token comment">#output_size</span>
                           num_layers<span class="token operator">=</span>n_layers<span class="token punctuation">,</span>  <span class="token comment"># 层数</span>
                           bidirectional<span class="token operator">=</span>bidirectional<span class="token punctuation">,</span> <span class="token comment">#是否双向</span>
                           dropout<span class="token operator">=</span>dropout<span class="token punctuation">)</span> <span class="token comment">#随机去除神经元</span>
        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span> <span class="token comment"># 因为前向传播+后向传播有两个hidden sate,且合并在一起,所以乘以2</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">,</span> text_lengths<span class="token punctuation">)</span><span class="token punctuation">:</span>
        embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">)</span>
        packed_embedded <span class="token operator">=</span> nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>pack_padded_sequence<span class="token punctuation">(</span>embedded<span class="token punctuation">,</span> text_lengths<span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">'cpu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        packed_output<span class="token punctuation">,</span> <span class="token punctuation">(</span>hidden<span class="token punctuation">,</span> cell<span class="token punctuation">)</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>packed_embedded<span class="token punctuation">)</span>
        hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>hidden<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> hidden<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>hidden<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="实例化模型"><a href="#实例化模型" class="headerlink" title="实例化模型"></a>实例化模型</h4><p>（由于我是GTX1660Ti的显卡，在训练过程中出现问题，所以将教程中的超参数HIDDEN_DIM 改为了128）</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">INPUT_DIM <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span> <span class="token comment"># 250002: 之前设置的只取25000个最频繁的词,加上pad_token和unknown token</span>
EMBEDDING_DIM <span class="token operator">=</span> <span class="token number">100</span>
HIDDEN_DIM <span class="token operator">=</span> <span class="token number">128</span> <span class="token comment">#原教程为256</span>
OUTPUT_DIM <span class="token operator">=</span> <span class="token number">1</span>
N_LAYERS <span class="token operator">=</span> <span class="token number">2</span>
BIDIRECTIONAL <span class="token operator">=</span> <span class="token boolean">True</span>
DROPOUT <span class="token operator">=</span> <span class="token number">0.5</span>
PAD_IDX <span class="token operator">=</span> TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>stoi<span class="token punctuation">[</span>TEXT<span class="token punctuation">.</span>pad_token<span class="token punctuation">]</span> <span class="token comment">#指定参数,定义pad_token的index索引值,让模型不管pad token</span>

model <span class="token operator">=</span> RNN<span class="token punctuation">(</span>INPUT_DIM<span class="token punctuation">,</span> 
            EMBEDDING_DIM<span class="token punctuation">,</span> 
            HIDDEN_DIM<span class="token punctuation">,</span> 
            OUTPUT_DIM<span class="token punctuation">,</span> 
            N_LAYERS<span class="token punctuation">,</span> 
            BIDIRECTIONAL<span class="token punctuation">,</span> 
            DROPOUT<span class="token punctuation">,</span> 
            PAD_IDX<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h4><p>这里的优化器更改为Adam，损失函数还是BCEWithLogitsLoss()</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCEWithLogitsLoss<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 损失函数. criterion 在本task中时损失函数的意思</span>
model <span class="token operator">=</span> model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
criterion <span class="token operator">=</span> criterion<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li><em>准确率</em></li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">binary_accuracy</span><span class="token punctuation">(</span>preds<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
    rounded_preds <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">round</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>preds<span class="token punctuation">)</span><span class="token punctuation">)</span>
    correct <span class="token operator">=</span> <span class="token punctuation">(</span>rounded_preds <span class="token operator">==</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#convert into float for division </span>
    acc <span class="token operator">=</span> correct<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>correct<span class="token punctuation">)</span>
    <span class="token keyword">return</span> acc<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li><em>训练函数与评估函数</em></li>
</ul>
<p>这里的训练函数和评估函数也与之前类似，不再贴代码</p>
<ul>
<li><em>验证</em></li>
</ul>
<blockquote>
<p>“predict_sentiment”函数的作用如下：<br>将模型切换为evaluate模式<br>对句子进行分词操作<br>将分词后的每个词，对应着词汇表，转换成对应的index索引，<br>获取句子的长度<br>将indexes，从list转化成tensor<br>通过unsqueezing 添加一个batch维度<br>将length转化成张量tensor<br>用sigmoid函数将预测值压缩到0-1之间<br>用item（）方法，将只有一个值的张量tensor转化成整数</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> spacy
nlp <span class="token operator">=</span> spacy<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'en_core_web_sm'</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">predict_sentiment</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>
    model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    tokenized <span class="token operator">=</span> <span class="token punctuation">[</span>tok<span class="token punctuation">.</span>text <span class="token keyword">for</span> tok <span class="token keyword">in</span> nlp<span class="token punctuation">.</span>tokenizer<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span><span class="token punctuation">]</span>
    indexed <span class="token operator">=</span> <span class="token punctuation">[</span>TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>stoi<span class="token punctuation">[</span>t<span class="token punctuation">]</span> <span class="token keyword">for</span> t <span class="token keyword">in</span> tokenized<span class="token punctuation">]</span>
    length <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>indexed<span class="token punctuation">)</span><span class="token punctuation">]</span>
    tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>indexed<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    tensor <span class="token operator">=</span> tensor<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
    length_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>length<span class="token punctuation">)</span>
    prediction <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>model<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> length_tensor<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> prediction<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><p>贴一张训练图片<br><img src="RNN%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C.jpg"></p>
<h2 id="task3：Faster-情感分析"><a href="#task3：Faster-情感分析" class="headerlink" title="task3：Faster 情感分析"></a>task3：Faster 情感分析</h2><h3 id="有关fastText"><a href="#有关fastText" class="headerlink" title="有关fastText"></a>有关fastText</h3><ul>
<li>fastText为每个n字符的gram训练一个向量表示，其中包括词、拼错的词、词片段甚至是单个字符。</li>
<li>fastText 模型架构和 Word2Vec 的 CBOW 模型类似 。和CBOW一样，fastText模型也只有三层：输入层、隐含层、输出层（Hierarchical Softmax），输入都是多个经向量表示的单词，输出都是一个特定的target，隐含层都是对多个词向量的叠加平均。不同的是，CBOW的输入是目标单词的上下文，fastText的输入是多个单词及其n-gram特征，这些特征用来表示单个文档；CBOW的输入单词被onehot编码过，fastText的输入特征是被embedding过；CBOW的输出是目标词汇，fastText的输出是文档对应的类标。fastText在输入时，将单词的字符级别的n-gram向量作为额外的特征；在输出时，fastText采用了分层Softmax，大大降低了模型训练时间。</li>
</ul>
<h3 id="数据预处理-1"><a href="#数据预处理-1" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>FastText分类模型与其他文本分类模型最大的不同之处在于其计算了输入句子的n-gram，并将n-gram作为一种附加特征来获取局部词序特征信息添加至标记化列表的末尾。<br>本次task使用bi-grams。即字节大小为2</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">generate_bigrams</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    n_grams <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">[</span>x<span class="token punctuation">[</span>i<span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> n_gram <span class="token keyword">in</span> n_grams<span class="token punctuation">:</span>
        x<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>n_gram<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>其他像导入包和加载数据构建迭代器等的步骤同上，不再贴代码~</p>
<h3 id="构建模型-2"><a href="#构建模型-2" class="headerlink" title="构建模型"></a>构建模型</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F

<span class="token keyword">class</span> <span class="token class-name">FastText</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">,</span> pad_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        
        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> padding_idx<span class="token operator">=</span>pad_idx<span class="token punctuation">)</span>        
        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>              
        embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>text<span class="token punctuation">)</span>       
        embedded <span class="token operator">=</span> embedded<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        pooled <span class="token operator">=</span> F<span class="token punctuation">.</span>avg_pool2d<span class="token punctuation">(</span>embedded<span class="token punctuation">,</span> <span class="token punctuation">(</span>embedded<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> 
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>pooled<span class="token punctuation">)</span>

INPUT_DIM <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span>
EMBEDDING_DIM <span class="token operator">=</span> <span class="token number">100</span>
OUTPUT_DIM <span class="token operator">=</span> <span class="token number">1</span>
PAD_IDX <span class="token operator">=</span> TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>stoi<span class="token punctuation">[</span>TEXT<span class="token punctuation">.</span>pad_token<span class="token punctuation">]</span>

model <span class="token operator">=</span> FastText<span class="token punctuation">(</span>INPUT_DIM<span class="token punctuation">,</span> EMBEDDING_DIM<span class="token punctuation">,</span> OUTPUT_DIM<span class="token punctuation">,</span> PAD_IDX<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><p>基本也是与之前相同，不赘述</p>
<h3 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h3><p>贴一个结果图,<br><img src="fastText.jpg"></p>
<p>fastText相对上一次任务的双向LSTM来说，速度快了不少，由于使用了一些特有的技巧，训练效果也得到了提升。</p>
<h2 id="task4：卷积情感分析"><a href="#task4：卷积情感分析" class="headerlink" title="task4：卷积情感分析"></a>task4：卷积情感分析</h2><p>呕吼~一直以为卷积只是用来处理图片的，没想到还能用来处理文本<br>卷积神经网络能够从局部输入图像块中提取特征，并能将表示模块化，同时可以高效第利用数据;可以用于处理时序数据，时间可以被看作一个空间维度，就像二维图像的高度和宽度<br>至于为什么可以在文本上使用卷积神经网络，原因如下：</p>
<ul>
<li>与3x3 filter可以查看图像块的方式相同，1x2 filter 可以查看一段文本中的两个连续单词，即双字符</li>
<li>本模型将使用多个不同大小的filter，这些filter将查看文本中的bi-grams（a 1x2 filter）、tri-grams（a 1x3 filter）and/or n-grams（a 1xn filter）。</li>
</ul>
<h3 id="数据预处理-2"><a href="#数据预处理-2" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>这里与fastText不同，不需要再创建bi-gram</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torchtext<span class="token punctuation">.</span>legacy <span class="token keyword">import</span> data
<span class="token keyword">from</span> torchtext<span class="token punctuation">.</span>legacy <span class="token keyword">import</span> datasets
<span class="token keyword">import</span> random
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

SEED <span class="token operator">=</span> <span class="token number">1234</span>

random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span>
np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>deterministic <span class="token operator">=</span> <span class="token boolean">True</span>

TEXT <span class="token operator">=</span> data<span class="token punctuation">.</span>Field<span class="token punctuation">(</span>tokenize <span class="token operator">=</span> <span class="token string">'spacy'</span><span class="token punctuation">,</span> 
                  tokenizer_language <span class="token operator">=</span> <span class="token string">'en_core_web_sm'</span><span class="token punctuation">,</span>
                  batch_first <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
LABEL <span class="token operator">=</span> data<span class="token punctuation">.</span>LabelField<span class="token punctuation">(</span>dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>

train_data<span class="token punctuation">,</span> test_data <span class="token operator">=</span> datasets<span class="token punctuation">.</span>IMDB<span class="token punctuation">.</span>splits<span class="token punctuation">(</span>TEXT<span class="token punctuation">,</span> LABEL<span class="token punctuation">)</span>

train_data<span class="token punctuation">,</span> valid_data <span class="token operator">=</span> train_data<span class="token punctuation">.</span>split<span class="token punctuation">(</span>random_state <span class="token operator">=</span> random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span><span class="token punctuation">)</span>
MAX_VOCAB_SIZE <span class="token operator">=</span> 25_000

TEXT<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> 
                 max_size <span class="token operator">=</span> MAX_VOCAB_SIZE<span class="token punctuation">,</span> 
                 vectors <span class="token operator">=</span> <span class="token string">"glove.6B.100d"</span><span class="token punctuation">,</span> 
                 unk_init <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">.</span>normal_<span class="token punctuation">)</span>

LABEL<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="构建模型-3"><a href="#构建模型-3" class="headerlink" title="构建模型"></a>构建模型</h3><h4 id="有关TextCNN"><a href="#有关TextCNN" class="headerlink" title="有关TextCNN"></a>有关TextCNN</h4><blockquote>
<p>卷积神经网络的核心思想是捕捉局部特征，对于文本来说，局部特征就是由若干单词组成的滑动窗口，类似于N-gram。卷积神经网络的优势在于能够自动地对N-gram特征进行组合和筛选，获得不同抽象层次的语义信息。</p>
</blockquote>
<p>具体的笔记再补~参考文献中的那个 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/77634533">深入TextCNN（一）详述CNN及TextCNN原理</a> 感觉讲的很好呀</p>
<h4 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F

<span class="token keyword">class</span> <span class="token class-name">CNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> n_filters<span class="token punctuation">,</span> filter_sizes<span class="token punctuation">,</span> output_dim<span class="token punctuation">,</span> 
                 dropout<span class="token punctuation">,</span> pad_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
        
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> padding_idx <span class="token operator">=</span> pad_idx<span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>conv_0 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> 
                                out_channels <span class="token operator">=</span> n_filters<span class="token punctuation">,</span> 
                                kernel_size <span class="token operator">=</span> <span class="token punctuation">(</span>filter_sizes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>conv_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> 
                                out_channels <span class="token operator">=</span> n_filters<span class="token punctuation">,</span> 
                                kernel_size <span class="token operator">=</span> <span class="token punctuation">(</span>filter_sizes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>conv_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> 
                                out_channels <span class="token operator">=</span> n_filters<span class="token punctuation">,</span> 
                                kernel_size <span class="token operator">=</span> <span class="token punctuation">(</span>filter_sizes<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>filter_sizes<span class="token punctuation">)</span> <span class="token operator">*</span> n_filters<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>
                
        <span class="token comment">#text = [batch size, sent len]</span>
        
        embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
                
        <span class="token comment">#embedded = [batch size, sent len, emb dim]</span>
        
        embedded <span class="token operator">=</span> embedded<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        
        <span class="token comment">#embedded = [batch size, 1, sent len, emb dim]</span>
        
        conved_0 <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv_0<span class="token punctuation">(</span>embedded<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        conved_1 <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv_1<span class="token punctuation">(</span>embedded<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        conved_2 <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv_2<span class="token punctuation">(</span>embedded<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            
        <span class="token comment">#conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]</span>
        
        pooled_0 <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool1d<span class="token punctuation">(</span>conved_0<span class="token punctuation">,</span> conved_0<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>
        pooled_1 <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool1d<span class="token punctuation">(</span>conved_1<span class="token punctuation">,</span> conved_1<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>
        pooled_2 <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool1d<span class="token punctuation">(</span>conved_2<span class="token punctuation">,</span> conved_2<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>
        
        <span class="token comment">#pooled_n = [batch size, n_filters]</span>
        
        cat <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>pooled_0<span class="token punctuation">,</span> pooled_1<span class="token punctuation">,</span> pooled_2<span class="token punctuation">)</span><span class="token punctuation">,</span> dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token comment">#cat = [batch size, n_filters * len(filter_sizes)]</span>
            
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>cat<span class="token punctuation">)</span>


INPUT_DIM <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span>
EMBEDDING_DIM <span class="token operator">=</span> <span class="token number">100</span>
N_FILTERS <span class="token operator">=</span> <span class="token number">100</span>
FILTER_SIZES <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">]</span>
OUTPUT_DIM <span class="token operator">=</span> <span class="token number">1</span>
DROPOUT <span class="token operator">=</span> <span class="token number">0.5</span>
PAD_IDX <span class="token operator">=</span> TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>stoi<span class="token punctuation">[</span>TEXT<span class="token punctuation">.</span>pad_token<span class="token punctuation">]</span>

model <span class="token operator">=</span> CNN<span class="token punctuation">(</span>INPUT_DIM<span class="token punctuation">,</span> EMBEDDING_DIM<span class="token punctuation">,</span> N_FILTERS<span class="token punctuation">,</span> FILTER_SIZES<span class="token punctuation">,</span> OUTPUT_DIM<span class="token punctuation">,</span> DROPOUT<span class="token punctuation">,</span> PAD_IDX<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<p>后续的任务还是与之前相同，不再赘述。</p>
<h3 id="小结-3"><a href="#小结-3" class="headerlink" title="小结"></a>小结</h3><h2 id="task5：多类型情感分析"><a href="#task5：多类型情感分析" class="headerlink" title="task5：多类型情感分析"></a>task5：多类型情感分析</h2><p>进行多分类的情感分析任务时，输出不再是正面负面两个标量了，而是一个C维向量。</p>
<h3 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h3><p>这次换了一个新的数据集，使用的是TREC数据集而不是IMDB数据集：训练集：5452，测试集：500。</p>
<ul>
<li>加载数据：</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torchtext <span class="token keyword">import</span> data
<span class="token keyword">from</span> torchtext <span class="token keyword">import</span> datasets
<span class="token keyword">import</span> random

SEED <span class="token operator">=</span> <span class="token number">1234</span>

torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>deterministic <span class="token operator">=</span> <span class="token boolean">True</span>

TEXT <span class="token operator">=</span> data<span class="token punctuation">.</span>Field<span class="token punctuation">(</span>tokenize <span class="token operator">=</span> <span class="token string">'spacy'</span><span class="token punctuation">)</span>
LABEL <span class="token operator">=</span> data<span class="token punctuation">.</span>LabelField<span class="token punctuation">(</span><span class="token punctuation">)</span>

train_data<span class="token punctuation">,</span> test_data <span class="token operator">=</span> datasets<span class="token punctuation">.</span>TREC<span class="token punctuation">.</span>splits<span class="token punctuation">(</span>TEXT<span class="token punctuation">,</span> LABEL<span class="token punctuation">,</span> fine_grained<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

train_data<span class="token punctuation">,</span> valid_data <span class="token operator">=</span> train_data<span class="token punctuation">.</span>split<span class="token punctuation">(</span>random_state <span class="token operator">=</span> random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">vars</span><span class="token punctuation">(</span>train_data<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li>接下来，我们将构建词汇表。 由于这个数据集很小（只有约 3800 个训练样本），它的词汇量也非常小（约 7500 个不同单词，即one-hot向量为7500维），这意味着我们不需要像以前一样在词汇表上设置“max_size”。</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">MAX_VOCAB_SIZE <span class="token operator">=</span> 25_000

TEXT<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> 
                 max_size <span class="token operator">=</span> MAX_VOCAB_SIZE<span class="token punctuation">,</span> 
                 vectors <span class="token operator">=</span> <span class="token string">"glove.6B.100d"</span><span class="token punctuation">,</span> 
                 unk_init <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">.</span>normal_<span class="token punctuation">)</span>

LABEL<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li>检查标签</li>
</ul>
<blockquote>
<p>6 个标签（对于非细粒度情况）对应于数据集中的 6 类问题：<br>HUM：关于人类的问题<br>ENTY：关于实体的问题的<br>DESC：关于要求提供描述的问题<br>NUM：关于答案为数字的问题<br>LOC：关于答案是位置的问题<br>ABBR：关于询问缩写的问题</p>
</blockquote>
<h3 id="构建模型-4"><a href="#构建模型-4" class="headerlink" title="构建模型"></a>构建模型</h3><p>这次使用Task04中的CNN模型，将之前模型中的 output_dim 改为 𝐶 维而不是 2 维即可，同时确保将输出维度： OUTPUT_DIM 设置为  𝐶 </p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F

<span class="token keyword">class</span> <span class="token class-name">CNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> n_filters<span class="token punctuation">,</span> filter_sizes<span class="token punctuation">,</span> output_dim<span class="token punctuation">,</span> 
                 dropout<span class="token punctuation">,</span> pad_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
        
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>convs <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>
                                    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> 
                                              out_channels <span class="token operator">=</span> n_filters<span class="token punctuation">,</span> 
                                              kernel_size <span class="token operator">=</span> <span class="token punctuation">(</span>fs<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span><span class="token punctuation">)</span> 
                                    <span class="token keyword">for</span> fs <span class="token keyword">in</span> filter_sizes
                                    <span class="token punctuation">]</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>filter_sizes<span class="token punctuation">)</span> <span class="token operator">*</span> n_filters<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>
        text <span class="token operator">=</span> text<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
        embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
        embedded <span class="token operator">=</span> embedded<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        conved <span class="token operator">=</span> <span class="token punctuation">[</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>conv<span class="token punctuation">(</span>embedded<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span> <span class="token keyword">for</span> conv <span class="token keyword">in</span> self<span class="token punctuation">.</span>convs<span class="token punctuation">]</span>
        pooled <span class="token operator">=</span> <span class="token punctuation">[</span>F<span class="token punctuation">.</span>max_pool1d<span class="token punctuation">(</span>conv<span class="token punctuation">,</span> conv<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token keyword">for</span> conv <span class="token keyword">in</span> conved<span class="token punctuation">]</span>
        cat <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>pooled<span class="token punctuation">,</span> dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>cat<span class="token punctuation">)</span>

INPUT_DIM <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span>
EMBEDDING_DIM <span class="token operator">=</span> <span class="token number">100</span>
N_FILTERS <span class="token operator">=</span> <span class="token number">100</span>
FILTER_SIZES <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span>
OUTPUT_DIM <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>LABEL<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span>
DROPOUT <span class="token operator">=</span> <span class="token number">0.5</span>
PAD_IDX <span class="token operator">=</span> TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>stoi<span class="token punctuation">[</span>TEXT<span class="token punctuation">.</span>pad_token<span class="token punctuation">]</span>

model <span class="token operator">=</span> CNN<span class="token punctuation">(</span>INPUT_DIM<span class="token punctuation">,</span> EMBEDDING_DIM<span class="token punctuation">,</span> N_FILTERS<span class="token punctuation">,</span> FILTER_SIZES<span class="token punctuation">,</span> OUTPUT_DIM<span class="token punctuation">,</span> DROPOUT<span class="token punctuation">,</span> PAD_IDX<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>与前几个notebook比,损失函数(也就是criterion)是不同的.之前用的是BCEWithLogisLoss,现在使用的是CrossEntropyLoss,它使用的是softrmax函数,来计算cross entropy</p>
<p>一般来说:</p>
<ul>
<li>CrossEntropyLoss :用于多分类问题</li>
<li>BCEWithLogitsLoss :用于2分类问题,(0,1),也用于多标签分类(multilabel classification) 1vs rest</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
criterion <span class="token operator">=</span> criterion<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="小结-4"><a href="#小结-4" class="headerlink" title="小结"></a>小结</h3><p>贴一个结果图<br><img src="%E5%A4%9A%E5%88%86%E7%B1%BB.jpg"></p>
<h2 id="task6：使用Transformer进行情感分析"><a href="#task6：使用Transformer进行情感分析" class="headerlink" title="task6：使用Transformer进行情感分析"></a>task6：使用Transformer进行情感分析</h2><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><p>本次任务使用的是Bert模型，由于模型很大，参数很多，所以将固定（而不训练）transformer，只训练从transformer产生的表示中学习的模型的其余部分。<br>在这种情况下，我们将使用双向GRU继续提取从Bert embedding后的特征。最后在fc层上输出最终的结果。</p>
<ul>
<li><em>导入相关库，并设定随机种子</em></li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch

<span class="token keyword">import</span> random
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

SEED <span class="token operator">=</span> <span class="token number">1234</span>

random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span>
np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>deterministic <span class="token operator">=</span> <span class="token boolean">True</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>transformers 库为每个提供的transformer 模型都有分词器。 在这种情况下，我们使用忽略大小写的 BERT 模型（即每个单词都会小写）。 我们通过加载预训练的“bert-base-uncased”标记器来实现这一点。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> BertTokenizer
tokenizer <span class="token operator">=</span> BertTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'bert-base-uncased'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>使用tokenizer.tokenize方法对字符串进行分词，并统一大小写。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">tokens <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span><span class="token string">'Hello WORLD how ARE yoU?'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<ul>
<li><em>定义一个函数处理所有的标记化。</em></li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">tokenize_and_cut</span><span class="token punctuation">(</span>sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>
    tokens <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span> 
    tokens <span class="token operator">=</span> tokens<span class="token punctuation">[</span><span class="token punctuation">:</span>max_input_length<span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span>
    <span class="token keyword">return</span> tokens<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li><em>定义标签字段</em></li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> torchtext<span class="token punctuation">.</span>legacy <span class="token keyword">import</span> data

TEXT <span class="token operator">=</span> data<span class="token punctuation">.</span>Field<span class="token punctuation">(</span>batch_first <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>
                  use_vocab <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
                  tokenize <span class="token operator">=</span> tokenize_and_cut<span class="token punctuation">,</span>
                  preprocessing <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>convert_tokens_to_ids<span class="token punctuation">,</span>
                  init_token <span class="token operator">=</span> init_token_idx<span class="token punctuation">,</span>
                  eos_token <span class="token operator">=</span> eos_token_idx<span class="token punctuation">,</span>
                  pad_token <span class="token operator">=</span> pad_token_idx<span class="token punctuation">,</span>
                  unk_token <span class="token operator">=</span> unk_token_idx<span class="token punctuation">)</span>

LABEL <span class="token operator">=</span> data<span class="token punctuation">.</span>LabelField<span class="token punctuation">(</span>dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li><em>拆分出训练集和验证集</em></li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> torchtext<span class="token punctuation">.</span>legacy <span class="token keyword">import</span> datasets

train_data<span class="token punctuation">,</span> test_data <span class="token operator">=</span> datasets<span class="token punctuation">.</span>IMDB<span class="token punctuation">.</span>splits<span class="token punctuation">(</span>TEXT<span class="token punctuation">,</span> LABEL<span class="token punctuation">)</span>

train_data<span class="token punctuation">,</span> valid_data <span class="token operator">=</span> train_data<span class="token punctuation">.</span>split<span class="token punctuation">(</span>random_state <span class="token operator">=</span> random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li><em>创建迭代器</em></li>
</ul>
<p>这里依旧是不能用教程里面的参数128，自己训练的时候改为了64</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">BATCH_SIZE <span class="token operator">=</span> <span class="token number">128</span>
device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">'cpu'</span><span class="token punctuation">)</span>
train_iterator<span class="token punctuation">,</span> valid_iterator<span class="token punctuation">,</span> test_iterator <span class="token operator">=</span> data<span class="token punctuation">.</span>BucketIterator<span class="token punctuation">.</span>splits<span class="token punctuation">(</span>
    <span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> valid_data<span class="token punctuation">,</span> test_data<span class="token punctuation">)</span><span class="token punctuation">,</span> 
    batch_size <span class="token operator">=</span> BATCH_SIZE<span class="token punctuation">,</span> 
    device <span class="token operator">=</span> device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="构建模型-5"><a href="#构建模型-5" class="headerlink" title="构建模型"></a>构建模型</h3><ul>
<li><em>导入预训练的bert</em></li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> BertTokenizer<span class="token punctuation">,</span> BertModel
bert <span class="token operator">=</span> BertModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'bert-base-uncased'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<ul>
<li><em>定义模型</em></li>
</ul>
<blockquote>
<p>*我们将使用预训练的Transformer模型，而不是使用embedding层来获取文本的embedding。然后将这些embedding输入GRU以生成对句子情绪的预测。<br>*通过config属性从transformer中获取嵌入维度大小（称为hidden_size）,其余初始化是标准的<br>*在前向传递的过程中，我们将transformer包装在一个no_grad中，以确保不会再模型的这部分计算梯度。<br>*transformer实际上返回整个序列的embedding以及pooled输入。<br>*前向传递的其余部分是循环模型的标准实现，我们在最后的时间步长中获取隐藏状态，并将其传递给一个线性层以获得我们的预测。</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token keyword">class</span> <span class="token class-name">BERTGRUSentiment</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>
                 bert<span class="token punctuation">,</span>
                 hidden_dim<span class="token punctuation">,</span>
                 output_dim<span class="token punctuation">,</span>
                 n_layers<span class="token punctuation">,</span>
                 bidirectional<span class="token punctuation">,</span>
                 dropout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>bert <span class="token operator">=</span> bert
        
        embedding_dim <span class="token operator">=</span> bert<span class="token punctuation">.</span>config<span class="token punctuation">.</span>to_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'hidden_size'</span><span class="token punctuation">]</span>
        
        self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">,</span>
                          hidden_dim<span class="token punctuation">,</span>
                          num_layers <span class="token operator">=</span> n_layers<span class="token punctuation">,</span>
                          bidirectional <span class="token operator">=</span> bidirectional<span class="token punctuation">,</span>
                          batch_first <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>
                          dropout <span class="token operator">=</span> <span class="token number">0</span> <span class="token keyword">if</span> n_layers <span class="token operator">&lt;</span> <span class="token number">2</span> <span class="token keyword">else</span> dropout<span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim <span class="token operator">*</span> <span class="token number">2</span> <span class="token keyword">if</span> bidirectional <span class="token keyword">else</span> hidden_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>
               
        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>bert<span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
                
        _<span class="token punctuation">,</span> hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>embedded<span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>bidirectional<span class="token punctuation">:</span>
            hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>hidden<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> hidden<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>hidden<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
       
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>out<span class="token punctuation">(</span>hidden<span class="token punctuation">)</span>
        <span class="token keyword">return</span> output<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li><em>创建实例</em></li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">HIDDEN_DIM <span class="token operator">=</span> <span class="token number">256</span>
OUTPUT_DIM <span class="token operator">=</span> <span class="token number">1</span>
N_LAYERS <span class="token operator">=</span> <span class="token number">2</span>
BIDIRECTIONAL <span class="token operator">=</span> <span class="token boolean">True</span>
DROPOUT <span class="token operator">=</span> <span class="token number">0.25</span>

model <span class="token operator">=</span> BERTGRUSentiment<span class="token punctuation">(</span>bert<span class="token punctuation">,</span>
                         HIDDEN_DIM<span class="token punctuation">,</span>
                         OUTPUT_DIM<span class="token punctuation">,</span>
                         N_LAYERS<span class="token punctuation">,</span>
                         BIDIRECTIONAL<span class="token punctuation">,</span>
                         DROPOUT<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="训练和评估"><a href="#训练和评估" class="headerlink" title="训练和评估"></a>训练和评估</h3><ul>
<li><em>定义损失函数</em></li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim

optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCEWithLogitsLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
criterion <span class="token operator">=</span> criterion<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li><em>构建训练函数</em></li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> iterator<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span><span class="token punctuation">:</span>
    
    epoch_loss <span class="token operator">=</span> <span class="token number">0</span>
    epoch_acc <span class="token operator">=</span> <span class="token number">0</span>
    
    model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token keyword">for</span> batch <span class="token keyword">in</span> iterator<span class="token punctuation">:</span>
        
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        predictions <span class="token operator">=</span> model<span class="token punctuation">(</span>batch<span class="token punctuation">.</span>text<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        
        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>
        
        acc <span class="token operator">=</span> binary_accuracy<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>
        
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        epoch_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
        epoch_acc <span class="token operator">+=</span> acc<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
    <span class="token keyword">return</span> epoch_loss <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span><span class="token punctuation">,</span> epoch_acc <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li><em>评估函数</em></li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">evaluate</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> iterator<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span><span class="token punctuation">:</span>
    
    epoch_loss <span class="token operator">=</span> <span class="token number">0</span>
    epoch_acc <span class="token operator">=</span> <span class="token number">0</span>
    
    model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    
        <span class="token keyword">for</span> batch <span class="token keyword">in</span> iterator<span class="token punctuation">:</span>

            predictions <span class="token operator">=</span> model<span class="token punctuation">(</span>batch<span class="token punctuation">.</span>text<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
            
            loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>
            
            acc <span class="token operator">=</span> binary_accuracy<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>

            epoch_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
            epoch_acc <span class="token operator">+=</span> acc<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
    <span class="token keyword">return</span> epoch_loss <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span><span class="token punctuation">,</span> epoch_acc <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li><em>训练</em></li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">N_EPOCHS <span class="token operator">=</span> <span class="token number">5</span>

best_valid_loss <span class="token operator">=</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token string">'inf'</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>N_EPOCHS<span class="token punctuation">)</span><span class="token punctuation">:</span>
    
    start_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    train_loss<span class="token punctuation">,</span> train_acc <span class="token operator">=</span> train<span class="token punctuation">(</span>model<span class="token punctuation">,</span> train_iterator<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span>
    valid_loss<span class="token punctuation">,</span> valid_acc <span class="token operator">=</span> evaluate<span class="token punctuation">(</span>model<span class="token punctuation">,</span> valid_iterator<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span>
        
    end_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
    epoch_mins<span class="token punctuation">,</span> epoch_secs <span class="token operator">=</span> epoch_time<span class="token punctuation">(</span>start_time<span class="token punctuation">,</span> end_time<span class="token punctuation">)</span>
        
    <span class="token keyword">if</span> valid_loss <span class="token operator">&lt;</span> best_valid_loss<span class="token punctuation">:</span>
        best_valid_loss <span class="token operator">=</span> valid_loss
        torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'tut6-model.pt'</span><span class="token punctuation">)</span>
    
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Epoch: </span><span class="token interpolation"><span class="token punctuation">{</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token format-spec">02</span><span class="token punctuation">}</span></span><span class="token string"> | Epoch Time: </span><span class="token interpolation"><span class="token punctuation">{</span>epoch_mins<span class="token punctuation">}</span></span><span class="token string">m </span><span class="token interpolation"><span class="token punctuation">{</span>epoch_secs<span class="token punctuation">}</span></span><span class="token string">s'</span></span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'\tTrain Loss: </span><span class="token interpolation"><span class="token punctuation">{</span>train_loss<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string"> | Train Acc: </span><span class="token interpolation"><span class="token punctuation">{</span>train_acc<span class="token operator">*</span><span class="token number">100</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string">%'</span></span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'\t Val. Loss: </span><span class="token interpolation"><span class="token punctuation">{</span>valid_loss<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string"> |  Val. Acc: </span><span class="token interpolation"><span class="token punctuation">{</span>valid_acc<span class="token operator">*</span><span class="token number">100</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string">%'</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="小结-5"><a href="#小结-5" class="headerlink" title="小结"></a>小结</h3><p>训练速度很慢，BATCH_SIZE依旧是不能用教程里面的128，改为64之后训练结束~<br>贴一张训练图<br><img src="9.png"><br>\[y = {f_{ {g_1}}}(x)\]<br>参考链接：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/31139113">torchtext入门教程，轻松玩转文本数据处理</a><br>        <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/104475016">[干货]深入浅出LSTM及其Python代码实现</a><br>        <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32965521">fastText原理及实践</a><br>        <a target="_blank" rel="noopener" href="https://blog.csdn.net/feilong_csdn/article/details/88655927">fastText原理和文本分类实战，看这一篇就够了</a><br>        <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/77634533">深入TextCNN（一）详述CNN及TextCNN原理</a></p>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">yali</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://jiyali.github.io/2021/09/12/qing-gan-fen-xi-xue-xi-bi-ji/">https://jiyali.github.io/2021/09/12/qing-gan-fen-xi-xue-xi-bi-ji/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">yali</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            <span class="chip bg-color">无标签</span>
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2021/10/10/shu-mo-xing-yu-ji-cheng-xue-xi-noxue-xi-bi-ji/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/11.jpg" class="responsive-img" alt="树模型与集成学习の学习笔记">
                        
                        <span class="card-title">树模型与集成学习の学习笔记</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2021-10-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            yali
                            
                        </span>
                    </div>
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                本篇&nbsp;<i class="far fa-dot-circle"></i>
            </div>
            <div class="card">
                <a href="/2021/09/12/qing-gan-fen-xi-xue-xi-bi-ji/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/6.jpg" class="responsive-img" alt="情感分析学习笔记">
                        
                        <span class="card-title">情感分析学习笔记</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2021-09-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            yali
                            
                        </span>
                    </div>
                </div>

                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\(', '\)']]}
    });
</script>



    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="105311555"
                   fixed='true'
                   autoplay='true'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            &copy; 2021 yali. 版权所有;<span id="sitetime"></span>
<!--              -->
<!--                 <span id="year">2019-2021</span> -->
<!--              -->
<!--             <span id="year">2019</span> -->
            <a href="/about" target="_blank">yali</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/jiyali" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:762115542@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=762115542" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 762115542" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>

<script language=javascript>
    function siteTime() {
        window.setTimeout("siteTime()", 1000);
        var seconds = 1000;
        var minutes = seconds * 60;
        var hours = minutes * 60;
        var days = hours * 24;
        var years = days * 365;
        var today = new Date();
        var todayYear = today.getFullYear();
        var todayMonth = today.getMonth() + 1;
        var todayDate = today.getDate();
        var todayHour = today.getHours();
        var todayMinute = today.getMinutes();
        var todaySecond = today.getSeconds();
        /* Date.UTC() -- 返回date对象距世界标准时间(UTC)1970年1月1日午夜之间的毫秒数(时间戳)
        year - 作为date对象的年份，为4位年份值
        month - 0-11之间的整数，做为date对象的月份
        day - 1-31之间的整数，做为date对象的天数
        hours - 0(午夜24点)-23之间的整数，做为date对象的小时数
        minutes - 0-59之间的整数，做为date对象的分钟数
        seconds - 0-59之间的整数，做为date对象的秒数
        microseconds - 0-999之间的整数，做为date对象的毫秒数 */
        var t1 = Date.UTC(2020, 09, 13, 00, 00, 00); //北京时间2018-2-13 00:00:00
        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
        var diff = t2 - t1;
        var diffYears = Math.floor(diff / years);
        var diffDays = Math.floor((diff / days) - diffYears * 365);
        var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
        var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) / minutes);
        var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours - diffMinutes * minutes) / seconds);
        //document.getElementById("sitetime").innerHTML = "本站已运行 " + diffYears + " 年 " + diffDays + " 天 " + diffHours + " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
    }/*因为建站时间还没有一年，就将之注释掉了。需要的可以取消*/
    siteTime();
</script>

    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

	
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

    <script type="text/javascript">
    //只在桌面版网页启用特效
    var windowWidth = $(window).width();
    if (windowWidth > 768) {
        document.write('<script type="text/javascript" src="/js/sakura.js"><\/script>');
    }
    </script>

    <script type="text/javascript">
        var OriginTitile=document.title,st;
	    document.addEventListener("visibilitychange",function(){
            document.hidden?(document.title="ヽ(●-`Д´-)ノ崩溃啦崩溃啦！",clearTimeout(st)):(document.title="(Ő∀Ő3)ノ欧呦好啦！",st=setTimeout(function(){document.title=OriginTitile},3e3))
        })
    </script>

</body>

</html>
